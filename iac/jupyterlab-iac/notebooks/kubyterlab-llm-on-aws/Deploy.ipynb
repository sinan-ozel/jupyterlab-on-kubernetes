{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d02ca0b4-1c4a-4b8e-a7a9-8fe3c9ba40b1",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38534870-6679-40ae-bcf1-f778377e5998",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from time import sleep\n",
    "from time import time as unixtime\n",
    "from typing import Callable, List\n",
    "import random\n",
    "from math import ceil\n",
    "import yaml\n",
    "import string\n",
    "import json\n",
    "from urllib.parse import urlparse\n",
    "from time import sleep\n",
    "import re\n",
    "from operator import itemgetter\n",
    "import subprocess\n",
    "from tempfile import NamedTemporaryFile\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "import boto3\n",
    "import kubernetes\n",
    "from kubernetes.client.rest import ApiException\n",
    "\n",
    "from pyhelm3 import Client as HelmClient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a98f1d3-e2b6-4c0a-ad3d-c07f5416d684",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv('/.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08774840-c227-48d4-844c-165c6f5dc8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import wait_until\n",
    "from helper.ec2 import (get_vpcs_ids, get_internet_gateway_ids_attached_to_vpc, \n",
    "                        get_route_table_ids_for_vpc, route_to_gateway_exists, \n",
    "                        get_subnet_ids_in_vpc, get_security_group_ids)\n",
    "from helper.k8s import (get_one_running_pod, get_jupyter_token_from_pod)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1800b74-d019-445d-942e-7223e770f332",
   "metadata": {},
   "source": [
    "# Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3b9f759-4820-40ad-8916-e935d6e57232",
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = 'ca-central-1'\n",
    "CLUSTER_NAME = 'kubyterlab-llm'\n",
    "EBS_VOLUME_SIZE = 500 # GiB\n",
    "TAGS = {'cluster': CLUSTER_NAME, 'purpose': 'llm'}  # Do not change the keys, they are hardcoded throughout.\n",
    "CLUSTER_TAGS = {'cluster': CLUSTER_NAME}\n",
    "CLUSTER_FILTERS = [{'Name': f'tag:{k}', 'Values': [CLUSTER_TAGS[k]]} for k in CLUSTER_TAGS]\n",
    "K8S_VERSION = os.environ['K8S_VERSION']  # '1.30'\n",
    "K8S_VERSION = '.'.join(K8S_VERSION.split('.')[:2]) if len(K8S_VERSION.split('.')) > 2 else K8S_VERSION\n",
    "INSTANCE_TYPES = {'gpu': ['g4dn.2xlarge'], 'default': ['t3.medium']}\n",
    "ALLOWED_PORTS = [8888]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5f820ba-9cbe-4a56-b952-015f4e88a5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLUSTER_FILTERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d859c1f-c31c-43b5-9a3a-fa9ed0ae7006",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_all_vpcs_available(response: dict) -> True:\n",
    "    if not 'Vpcs' in response:\n",
    "        raise ValueError\n",
    "    return all([vpc.get('State', '') == 'available' for vpc in response['Vpcs']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70ff5d92-c3be-4255-9d40-afea872ff121",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_cluster_active(response: dict) -> bool:\n",
    "    status = response['cluster']['status']\n",
    "    clear_output(wait=True)\n",
    "    display(status)\n",
    "    return status == 'ACTIVE'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8983294-0e11-457c-abb6-2ec996243d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_node_group_active(response: dict) -> bool:\n",
    "    status = response['nodegroup']['status']\n",
    "    clear_output(wait=True)\n",
    "    display(status)\n",
    "    return status in ['ACTIVE', 'CREATE_FAILED']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9938e8cf-fc3a-42f8-819b-7748181fffc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_snapshot_completed(response: dict) -> bool:\n",
    "    state = response['Snapshots'][0]['State']\n",
    "    clear_output(wait=True)\n",
    "    display(state)\n",
    "    return state.lower() == 'completed'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f79a307-fbfa-4d1e-9d75-9f77405a7dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_volume_available(response: dict) -> bool:\n",
    "    state = response['Volumes'][0]['State']\n",
    "    clear_output(wait=True)\n",
    "    display(state)\n",
    "    return state.lower() == 'available'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73ff831-37b6-4fbf-9aad-1068cfb8efe7",
   "metadata": {},
   "source": [
    "# Instantiate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97c98c25-b95b-40da-be00-f65f5952d5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = boto3.Session(region_name=REGION)\n",
    "eks_client = session.client('eks')\n",
    "ec2_client = session.client('ec2')\n",
    "iam_client = session.client('iam')\n",
    "\n",
    "aws_account_id = boto3.client('sts').get_caller_identity().get('Account')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbf7c2f-2e86-48c9-b369-1047bd8fea6c",
   "metadata": {},
   "source": [
    "# Create or Restore Volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "025247fd-7e39-46c3-ba9c-8bf58a3dcc19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    response = ec2_client.describe_volumes(Filters=CLUSTER_FILTERS)\n",
    "    volumes = response.get('Volumes', [])\n",
    "except RuntimeError:\n",
    "    volumes = []\n",
    "volume_ids = [volume['VolumeId'] for volume in volumes]\n",
    "volume_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "728edb7d-b012-4778-8f98-34f00f520042",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(volume_ids) <= 1  # TODO: Get the latest one if more than one.\n",
    "if volume_ids:\n",
    "    volume_id = volume_ids[0]\n",
    "    availability_zone = volumes[0]['AvailabilityZone']\n",
    "else:\n",
    "    volume_id = None\n",
    "volume_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf65cde1-f0ad-4b9a-9c91-49c20d09b2f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "snap-0f39349a1322cc9d3\n"
     ]
    }
   ],
   "source": [
    "if not volume_id:\n",
    "    response = ec2_client.describe_snapshots(Filters=CLUSTER_FILTERS)\n",
    "    snapshots = response.get('Snapshots', [])\n",
    "    if snapshots:\n",
    "        sorted_snapshots = sorted(snapshots, key=itemgetter('StartTime'), reverse=True)\n",
    "        snapshot_id = sorted_snapshots[0]['SnapshotId']\n",
    "        print(snapshot_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "17ac7e4d-ea1e-4a05-9b51-05d98dc3d937",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert volume_id or snapshot_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e5d85302-63d3-4ef2-a07e-bd59dc564768",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'available'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('vol-0111a7543a373b3b9', 'ca-central-1a')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not volume_id:\n",
    "    response = ec2_client.describe_availability_zones()\n",
    "    availability_zones = response['AvailabilityZones']\n",
    "    availability_zone = availability_zones[0]['ZoneName']\n",
    "    # availability_zone = f'{REGION}a'\n",
    "    if snapshots:\n",
    "        # TODO: Change this to the latest snapshot!!\n",
    "        response = ec2_client.create_volume(\n",
    "            SnapshotId=snapshot_id,\n",
    "            Size=EBS_VOLUME_SIZE,\n",
    "            AvailabilityZone=availability_zone,\n",
    "            VolumeType='gp3',\n",
    "            TagSpecifications=[\n",
    "                {\n",
    "                    'ResourceType': 'volume',\n",
    "                    'Tags': [{'Key': k, 'Value': TAGS[k]} for k in TAGS]\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "    else:\n",
    "        response = ec2_client.create_volume(\n",
    "            Size=EBS_VOLUME_SIZE,\n",
    "            AvailabilityZone=availability_zones[0]['ZoneName'],\n",
    "            VolumeType='gp3',\n",
    "            TagSpecifications=[\n",
    "                {\n",
    "                    'ResourceType': 'volume',\n",
    "                    'Tags': [{'Key': k, 'Value': TAGS[k]} for k in TAGS]\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "    wait_until(ec2_client.describe_volumes, {'VolumeIds': [response['VolumeId']]}, is_volume_available)\n",
    "    volume_id = response['VolumeId']\n",
    "volume_id, availability_zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e815d5e-1d1e-4f94-ac53-9fa53edbdd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Change the parts below to use Terraform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbda3f9d-a108-48f4-8411-f7dd2e0e1f43",
   "metadata": {},
   "source": [
    "# Create VPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "429b8e60-e14a-460b-a448-45bc392e6c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating VPC...\n",
      "Creating Internet Gateway...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('vpc-0cf9b38d0fcd171c1', 'igw-0efb8fc45d440dc00', 'rtb-0c9abc976bd4d8b81')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vpc_ids = get_vpcs_ids(ec2_client, TAGS)\n",
    "vpc_exists = len(vpc_ids) > 0\n",
    "\n",
    "if len(vpc_ids) > 1:\n",
    "    raise RuntimeError\n",
    "elif vpc_exists:\n",
    "    vpc_id = vpc_ids[0]\n",
    "\n",
    "# Create VPC\n",
    "if not vpc_exists:\n",
    "    print('Creating VPC...')\n",
    "    vpc_response = ec2_client.create_vpc(\n",
    "        CidrBlock='10.0.0.0/16',\n",
    "        TagSpecifications=[\n",
    "            {\n",
    "                'ResourceType': 'vpc',\n",
    "                'Tags': [{'Key': tag, 'Value': TAGS[tag] }for tag in TAGS]\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    vpc_id = vpc_response['Vpc']['VpcId']\n",
    "wait_until(ec2_client.describe_vpcs, {'VpcIds': [vpc_id]}, check_all_vpcs_available)\n",
    "\n",
    "# Create Internet Gateway\n",
    "igw_ids = get_internet_gateway_ids_attached_to_vpc(ec2_client, vpc_id)\n",
    "igw_exists = len(igw_ids) > 0\n",
    "\n",
    "if len(igw_ids) > 1:\n",
    "    raise RuntimeError\n",
    "elif igw_exists:\n",
    "    igw_id = igw_ids[0]\n",
    "\n",
    "\n",
    "if not igw_exists:\n",
    "    print('Creating Internet Gateway...')\n",
    "    # Create an Internet Gateway\n",
    "    igw_response = ec2_client.create_internet_gateway()\n",
    "    igw_id = igw_response['InternetGateway']['InternetGatewayId']\n",
    "    \n",
    "    # Attach Internet Gateway to VPC\n",
    "    ec2_client.attach_internet_gateway(\n",
    "        InternetGatewayId=igw_id,\n",
    "        VpcId=vpc_id\n",
    "    )\n",
    "\n",
    "# Create a route table\n",
    "route_table_ids = get_route_table_ids_for_vpc(ec2_client, vpc_id)\n",
    "route_table_exists = len(route_table_ids) > 0\n",
    "\n",
    "if not route_table_exists:\n",
    "    print('Creating a route table...')\n",
    "    # Create a route table\n",
    "    route_table_response = ec2_client.create_route_table(VpcId=vpc_id)\n",
    "    route_table_id = route_table_response['RouteTable']['RouteTableId']\n",
    "\n",
    "is_route_created = False\n",
    "for route_table_id in route_table_ids:\n",
    "    if route_to_gateway_exists(ec2_client, route_table_id, igw_id):\n",
    "        is_route_created = True\n",
    "        break\n",
    "\n",
    "if not is_route_created:\n",
    "    # Create a route to the Internet Gateway\n",
    "    ec2_client.create_route(\n",
    "        RouteTableId=route_table_id,\n",
    "        DestinationCidrBlock='0.0.0.0/0',\n",
    "        GatewayId=igw_id\n",
    "    )\n",
    "\n",
    "vpc_id, igw_id, route_table_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbfdb24-1435-47bb-8293-b87b9d4075b7",
   "metadata": {},
   "source": [
    "# Create Subnets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4b32166a-ce50-45f6-aac3-3f809836f103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating subnet #1...\n",
      "Creating subnet #2...\n"
     ]
    }
   ],
   "source": [
    "subnet_ids = get_subnet_ids_in_vpc(ec2_client, vpc_id)\n",
    "min_subnet_count = 2\n",
    "current_subnet_count = len(subnet_ids)\n",
    "\n",
    "# if current_subnet_count < min_subnet_count:\n",
    "    # response = ec2_client.describe_availability_zones()\n",
    "    \n",
    "    # availability_zones = [az['ZoneName'] for az in response['AvailabilityZones']]\n",
    "    # random.shuffle(availability_zones)\n",
    "    # TODO: Following logic is still necessary for situations with larger subnets. Change to something reasonable.\n",
    "    # availability_zones *= ceil(min_subnet_count / len(availability_zones))\n",
    "    \n",
    "\n",
    "while current_subnet_count < min_subnet_count:\n",
    "    i = current_subnet_count + 1\n",
    "    print(f'Creating subnet #{i}...')\n",
    "    subnet_response = ec2_client.create_subnet(\n",
    "        VpcId=vpc_id,\n",
    "        CidrBlock=f'10.0.{i}.0/24',\n",
    "        # AvailabilityZone=availability_zone,\n",
    "        AvailabilityZone=availability_zones[current_subnet_count]['ZoneName'],\n",
    "        TagSpecifications=[\n",
    "            {\n",
    "                'ResourceType': 'subnet',\n",
    "                'Tags': [{'Key': tag, 'Value': TAGS[tag]} for tag in TAGS]\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    subnet_id = subnet_response['Subnet']['SubnetId']\n",
    "    subnet_ids.append(subnet_id)\n",
    "    current_subnet_count = len(subnet_ids)\n",
    "\n",
    "# check if any subnet is public, add the table to first subnet if not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c837c746-05e1-432f-8619-e3c0581650f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "subnets_response = ec2_client.describe_subnets(SubnetIds=subnet_ids)\n",
    "assert len(subnets_response) == len(subnet_ids) == min_subnet_count\n",
    "is_any_subnet_open_to_public = False\n",
    "for subnet in subnets_response['Subnets']:\n",
    "    if subnets_response.get('MapPublicIpOnLaunch', False):\n",
    "        # I could also look at the route tables, and see if they are assigned to any subnet. ec2_client.describe_route_tables(RouteTableIds=[route_table_id]) RouteTables.SubnetId (optional parameter)\n",
    "        public_subnet_id = subnet['SubnetId']\n",
    "        is_any_subnet_open_to_public = True\n",
    "\n",
    "if not is_any_subnet_open_to_public:\n",
    "    for subnet in subnets_response['Subnets']:\n",
    "        if subnet['CidrBlock'] == '10.0.1.0/24':\n",
    "            public_subnet_id = subnet['SubnetId']\n",
    "\n",
    "            # Associate the public subnet with the route table\n",
    "            ec2_client.associate_route_table(\n",
    "                RouteTableId=route_table_id,\n",
    "                SubnetId=public_subnet_id\n",
    "            )\n",
    "            \n",
    "            # Modify the public subnet to auto-assign public IPs\n",
    "            ec2_client.modify_subnet_attribute(\n",
    "                SubnetId=public_subnet_id,\n",
    "                MapPublicIpOnLaunch={\"Value\": True}\n",
    "            )\n",
    "            break\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95af95c-f3a4-46d3-b5f8-b7d953eda3a9",
   "metadata": {},
   "source": [
    "# Create Security Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bf11870e-3dee-4564-9c82-c78a0262ce77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Security Group...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'sg-0b6c0fb8f72fe6aa2'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "security_group_ids = get_security_group_ids(ec2_client, TAGS)\n",
    "\n",
    "security_group_exists = len(security_group_ids) > 0\n",
    "\n",
    "if len(security_group_ids) > 1:\n",
    "    raise RuntimeError\n",
    "elif security_group_exists:\n",
    "    security_group_id = security_group_ids[0]\n",
    "\n",
    "# The security group gets torn down when deleted, so there is no need to check the rules and rewrite all of them.\n",
    "\n",
    "if not security_group_exists:\n",
    "    print('Creating Security Group...')\n",
    "    response = ec2_client.create_security_group(\n",
    "        GroupName=f'eks-cluster-sg-{CLUSTER_NAME}',\n",
    "        TagSpecifications=[\n",
    "            {\n",
    "                'ResourceType': 'security-group', \n",
    "                'Tags': [{'Key': tag, 'Value': TAGS[tag]} for tag in TAGS]\n",
    "            }\n",
    "        ],\n",
    "        Description=f'Security group for EKS cluster: {CLUSTER_NAME}',\n",
    "        VpcId=vpc_id\n",
    "    )\n",
    "    security_group_id = response['GroupId']\n",
    "\n",
    "    # TODO: Can I make the CidrIp more restrictive for the next deployment? Load Balancer needs to have a static IP, probably through the Kubernetes YAML?\n",
    "    for port in ALLOWED_PORTS:\n",
    "        ec2_client.authorize_security_group_ingress(\n",
    "            GroupId=security_group_id,\n",
    "            IpPermissions=[\n",
    "                {\n",
    "                    'IpProtocol': 'tcp',\n",
    "                    'FromPort': port,\n",
    "                    'ToPort': port,\n",
    "                    'IpRanges': [{'CidrIp': '0.0.0.0/0'}]\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "    \n",
    "security_group_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6a6e50f1-8540-4dcf-9aa6-46c0fd668aea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('vol-0111a7543a373b3b9',\n",
       " 'vpc-0cf9b38d0fcd171c1',\n",
       " 'igw-0efb8fc45d440dc00',\n",
       " 'rtb-0c9abc976bd4d8b81',\n",
       " 'subnet-038f12546d6f50a3c',\n",
       " ['subnet-038f12546d6f50a3c', 'subnet-02d016cf23e121bbb'],\n",
       " 'sg-0b6c0fb8f72fe6aa2')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "volume_id, vpc_id, igw_id, route_table_id, public_subnet_id, subnet_ids, security_group_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5e3e6a-1650-4a59-91b7-bff33b937a52",
   "metadata": {},
   "source": [
    "# Create Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e5e5fec8-eea5-4ef7-806c-4bdea1f61845",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ACTIVE'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tear_down_and_rebuild = False\n",
    "response = eks_client.list_clusters()\n",
    "clusters = response['clusters']\n",
    "if CLUSTER_NAME not in clusters:\n",
    "    eks_client.create_cluster(name=CLUSTER_NAME, \n",
    "                              version=K8S_VERSION, \n",
    "                              roleArn=f'arn:aws:iam::{aws_account_id}:role/EKS_Cluster_Role', \n",
    "                              resourcesVpcConfig={'subnetIds': subnet_ids,\n",
    "                                                  'securityGroupIds': [security_group_id],\n",
    "                                                  'endpointPublicAccess': True,\n",
    "                                                  'endpointPrivateAccess': False\n",
    "                              },\n",
    "                              tags=TAGS,\n",
    "                             )\n",
    "else:\n",
    "    if tear_down_and_rebuild:\n",
    "        pass\n",
    "        # TODO: Create a new cluster with a temp name.\n",
    "        # Wait until the cluster has finished forming\n",
    "        # Delete the old cluster\n",
    "        # Wait until deletion is complete\n",
    "        # Rename the new cluster\n",
    "        \n",
    "wait_until(eks_client.describe_cluster, {'name': CLUSTER_NAME}, is_cluster_active, timeout=7 * 60)\n",
    "response = eks_client.describe_cluster(name=CLUSTER_NAME)\n",
    "assert response['cluster']['status'] == 'ACTIVE'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7a812b-4a62-4b3b-a88e-90b0db82a194",
   "metadata": {},
   "source": [
    "# Create IAM Role for Node Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "49bcf925-9e7a-47af-9d1a-d160959148cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Role EKS_Cluster_Role already exists. Arn: arn:aws:iam::275678099358:role/EKS_Cluster_Role.\n",
      "Attached policy AmazonEKSWorkerNodePolicy to role EKS_Cluster_Role.\n",
      "Attached policy AmazonEC2ContainerRegistryReadOnly to role EKS_Cluster_Role.\n",
      "Attached policy AmazonEKS_CNI_Policy to role EKS_Cluster_Role.\n",
      "Attached policy AmazonEKSClusterPolicy to role EKS_Cluster_Role.\n"
     ]
    }
   ],
   "source": [
    "# Define the role name\n",
    "role_name = 'EKS_Cluster_Role'  # WARNING: I get a weird error if I call this role anything else. The call looks for this particular name, and I do not know how to override it.\n",
    "\n",
    "# Create the trust policy for the role\n",
    "trust_policy = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\n",
    "                \"Service\": [\n",
    "                    \"eks.amazonaws.com\",\n",
    "                    \"ec2.amazonaws.com\"\n",
    "                ]\n",
    "            },\n",
    "            \"Action\": \"sts:AssumeRole\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create the IAM role\n",
    "try:\n",
    "    response = iam_client.create_role(\n",
    "        RoleName=role_name,\n",
    "        AssumeRolePolicyDocument=json.dumps(trust_policy),\n",
    "        Description='Role for EKS Node Group'\n",
    "    )\n",
    "    node_role_arn = response['Role']['Arn']\n",
    "    print(f\"Created role: {node_role_arn}\")\n",
    "except iam_client.exceptions.EntityAlreadyExistsException:\n",
    "    response = iam_client.get_role(RoleName=role_name)\n",
    "    node_role_arn = response['Role']['Arn']\n",
    "    print(f\"Role {role_name} already exists. Arn: {node_role_arn}.\")\n",
    "    # TODO: Check if trust_policy is correct.\n",
    "\n",
    "# Attach necessary policies\n",
    "policies = [\n",
    "    'AmazonEKSWorkerNodePolicy',\n",
    "    'AmazonEC2ContainerRegistryReadOnly',\n",
    "    'AmazonEKS_CNI_Policy',\n",
    "    'AmazonEKSClusterPolicy',\n",
    "    # 'AmazonSSMManagedInstanceCore',\n",
    "]\n",
    "# Policy AmazonSSMManagedInstanceCore is not necessary, I used it for debugging, to connect to the node and run commands. \n",
    "\n",
    "for policy in policies:\n",
    "    try:\n",
    "        iam_client.attach_role_policy(\n",
    "            RoleName=role_name,\n",
    "            PolicyArn=f'arn:aws:iam::aws:policy/{policy}'\n",
    "        )\n",
    "        print(f\"Attached policy {policy} to role {role_name}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error attaching policy {policy}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8c8bd2-f80d-4a36-b648-4895429988ab",
   "metadata": {},
   "source": [
    "# Add Node Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "11c771d0-9929-4c01-8ae6-e7c0845760d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: This part may also need a wait."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "75cfa5fb-e965-4e51-bee6-8414c32a7689",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CREATING'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_groups = eks_client.list_nodegroups(clusterName=CLUSTER_NAME)\n",
    "# TODO: Make this static and put to the top, with different `diskSize` and `scalingConfig`\n",
    "node_group_name = 'gpu'\n",
    "ami_type = 'AL2_x86_64_GPU'\n",
    "if node_group_name in node_groups['nodegroups']:\n",
    "    response = eks_client.describe_nodegroup(clusterName=CLUSTER_NAME, nodegroupName=node_group_name)\n",
    "    status = response['nodegroup']['status']\n",
    "    if status == 'CREATE_FAILED':\n",
    "        print('Node group exists, but failed to create. Deleting...')\n",
    "        failed_node_group_info = response\n",
    "\n",
    "        eks_client.delete_nodegroup(clusterName=CLUSTER_NAME, nodegroupName=node_group_name)\n",
    "    \n",
    "        wait_until(eks_client.list_nodegroups, {'clusterName': CLUSTER_NAME}, lambda x: node_group_name not in x['nodegroups'])\n",
    "        node_groups = eks_client.list_nodegroups(clusterName=CLUSTER_NAME)\n",
    "    else:\n",
    "        print('Node group exists.')\n",
    "\n",
    "\n",
    "\n",
    "if not node_groups['nodegroups']:\n",
    "    print('Creating node group...')\n",
    "    response = eks_client.create_nodegroup(\n",
    "        clusterName=CLUSTER_NAME,\n",
    "        nodegroupName=node_group_name,\n",
    "        scalingConfig={\n",
    "            'desiredSize': 1,\n",
    "            'minSize': 1,\n",
    "            'maxSize': 1\n",
    "        },\n",
    "        diskSize=50,  # Size in GiB\n",
    "        subnets=[public_subnet_id],\n",
    "        nodeRole=node_role_arn,\n",
    "        amiType=ami_type,\n",
    "        instanceTypes=INSTANCE_TYPES[node_group_name],\n",
    "        labels={\n",
    "            'gpu-memory': 'true'\n",
    "        },\n",
    "        taints=[\n",
    "            {\n",
    "                'key': 'nvidia.com/gpu',\n",
    "                'value': 'true',\n",
    "                'effect': 'NO_SCHEDULE'\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "wait_until(eks_client.describe_nodegroup, {'clusterName': CLUSTER_NAME, 'nodegroupName': node_group_name}, is_node_group_active)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a69858b4-e5f2-4951-aa3d-69cd404c0e27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CREATING'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_groups = eks_client.list_nodegroups(clusterName=CLUSTER_NAME)\n",
    "# TODO: Make this static and put to the top, with different `diskSize` and `scalingConfig`\n",
    "node_group_name = 'default'\n",
    "ami_type = 'AL2_x86_64'\n",
    "if node_group_name in node_groups['nodegroups']:\n",
    "    response = eks_client.describe_nodegroup(clusterName=CLUSTER_NAME, nodegroupName=node_group_name)\n",
    "    status = response['nodegroup']['status']\n",
    "    if status == 'CREATE_FAILED':\n",
    "        print('Node group exists, but failed to create. Deleting...')\n",
    "        failed_node_group_info = response\n",
    "\n",
    "        eks_client.delete_nodegroup(clusterName=CLUSTER_NAME, nodegroupName=node_group_name)\n",
    "    \n",
    "        wait_until(eks_client.list_nodegroups, {'clusterName': CLUSTER_NAME}, lambda x: node_group_name not in x['nodegroups'])\n",
    "        node_groups = eks_client.list_nodegroups(clusterName=CLUSTER_NAME)\n",
    "    else:\n",
    "        print('Node group exists.')\n",
    "\n",
    "\n",
    "# TODO: See if lower disksize will work.\n",
    "if node_group_name not in node_groups['nodegroups']:\n",
    "    print('Creating node group...')\n",
    "    response = eks_client.create_nodegroup(\n",
    "        clusterName=CLUSTER_NAME,\n",
    "        nodegroupName=node_group_name,\n",
    "        scalingConfig={\n",
    "            'desiredSize': 2,\n",
    "            'minSize': 2,\n",
    "            'maxSize': 2\n",
    "        },\n",
    "        diskSize=1000,  # Size in GiB\n",
    "        subnets=[public_subnet_id],\n",
    "        nodeRole=node_role_arn,\n",
    "        amiType=ami_type,\n",
    "        instanceTypes=INSTANCE_TYPES[node_group_name],\n",
    "    )\n",
    "wait_until(eks_client.describe_nodegroup, {'clusterName': CLUSTER_NAME, 'nodegroupName': node_group_name}, is_node_group_active)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9df50a-ede3-4b60-af87-f7295796c94c",
   "metadata": {},
   "source": [
    "# Initialize Kubernetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "77edf8ec-0ec3-42d2-a8a6-f8dbf606364f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated context arn:aws:eks:ca-central-1:275678099358:cluster/kubyterlab-llm in /root/.kube/config\n"
     ]
    }
   ],
   "source": [
    "!aws eks update-kubeconfig --name $CLUSTER_NAME --region $REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8ac935-b3bd-491f-9a4c-ec60bd3f3271",
   "metadata": {},
   "source": [
    "# Initialize Helm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9ec6ffe2-7458-4d1b-90dd-0b6fbbb0e921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"aws-ebs-csi-driver\" already exists with the same configuration, skipping\n"
     ]
    }
   ],
   "source": [
    "!helm repo add aws-ebs-csi-driver https://kubernetes-sigs.github.io/aws-ebs-csi-driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "79fbbaa4-e44b-4a43-bfd1-04d3a9dcf4fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hang tight while we grab the latest from your chart repositories...\n",
      "...Successfully got an update from the \"aws-ebs-csi-driver\" chart repository\n",
      "Update Complete. ⎈Happy Helming!⎈\n"
     ]
    }
   ],
   "source": [
    "!helm repo update"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1566d62-4c89-4f87-84a1-5d465d8d89d4",
   "metadata": {},
   "source": [
    "# Create a Role to Install the CSI Driver and Give Permissions Using the OIDC Provider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "baafae37-f87f-47bb-a642-fc1c1c6cb5a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://oidc.eks.ca-central-1.amazonaws.com/id/3607A6B25018F663123A7078ED5F5B9E'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_response = eks_client.describe_cluster(name=CLUSTER_NAME)\n",
    "oidc_id = cluster_response['cluster']['identity']['oidc']['issuer'].split('/')[-1]\n",
    "oidc_url = f'https://oidc.eks.{REGION}.amazonaws.com/id/{oidc_id}'\n",
    "oidc_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "47f875e6-9a22-4615-98a9-eb43c520fd94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m2025-02-04 22:28:47 [ℹ]  will create IAM Open ID Connect provider for cluster \"kubyterlab-llm\" in \"ca-central-1\"\n",
      "\u001b[36m2025-02-04 22:28:47 [✔]  created IAM Open ID Connect provider for cluster \"kubyterlab-llm\" in \"ca-central-1\"\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!eksctl utils associate-iam-oidc-provider --cluster $CLUSTER_NAME --approve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "023b4e7f-f182-4474-a035-4027c0c75351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arn:aws:iam::275678099358:policy/AmazonEKS_EBS_CSI_Driver_Policy'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazon_eks_ebs_csi_driver_policy_name = 'AmazonEKS_EBS_CSI_Driver_Policy'\n",
    "amazon_eks_ebs_csi_driver_policy = {\n",
    "  \"Version\": \"2012-10-17\",\n",
    "  \"Statement\": [\n",
    "    {\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Action\": [\n",
    "        \"ec2:CreateSnapshot\",\n",
    "        \"ec2:AttachVolume\",\n",
    "        \"ec2:DetachVolume\",\n",
    "        \"ec2:ModifyVolume\",\n",
    "        \"ec2:DescribeAvailabilityZones\",\n",
    "        \"ec2:DescribeInstances\",\n",
    "        \"ec2:DescribeSnapshots\",\n",
    "        \"ec2:DescribeTags\",\n",
    "        \"ec2:DescribeVolumes\",\n",
    "        \"ec2:DescribeVolumesModifications\"\n",
    "      ],\n",
    "      \"Resource\": \"*\"\n",
    "    },\n",
    "    {\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Action\": [\n",
    "        \"ec2:CreateTags\"\n",
    "      ],\n",
    "      \"Resource\": [\n",
    "        \"arn:aws:ec2:*:*:volume/*\",\n",
    "        \"arn:aws:ec2:*:*:snapshot/*\"\n",
    "      ],\n",
    "      \"Condition\": {\n",
    "        \"StringEquals\": {\n",
    "          \"ec2:CreateAction\": [\n",
    "            \"CreateVolume\",\n",
    "            \"CreateSnapshot\"\n",
    "          ]\n",
    "        }\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Action\": [\n",
    "        \"ec2:DeleteTags\"\n",
    "      ],\n",
    "      \"Resource\": [\n",
    "        \"arn:aws:ec2:*:*:volume/*\",\n",
    "        \"arn:aws:ec2:*:*:snapshot/*\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Action\": [\n",
    "        \"ec2:CreateVolume\"\n",
    "      ],\n",
    "      \"Resource\": \"*\",\n",
    "      \"Condition\": {\n",
    "        \"StringLike\": {\n",
    "          \"aws:RequestTag/ebs.csi.aws.com/cluster\": \"true\"\n",
    "        }\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Action\": [\n",
    "        \"ec2:CreateVolume\"\n",
    "      ],\n",
    "      \"Resource\": \"*\",\n",
    "      \"Condition\": {\n",
    "        \"StringLike\": {\n",
    "          \"aws:RequestTag/CSIVolumeName\": \"*\"\n",
    "        }\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Action\": [\n",
    "        \"ec2:DeleteVolume\"\n",
    "      ],\n",
    "      \"Resource\": \"*\",\n",
    "      \"Condition\": {\n",
    "        \"StringLike\": {\n",
    "          \"ec2:ResourceTag/CSIVolumeName\": \"*\"\n",
    "        }\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Action\": [\n",
    "        \"ec2:DeleteVolume\"\n",
    "      ],\n",
    "      \"Resource\": \"*\",\n",
    "      \"Condition\": {\n",
    "        \"StringLike\": {\n",
    "          \"ec2:ResourceTag/ebs.csi.aws.com/cluster\": \"true\"\n",
    "        }\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Action\": [\n",
    "        \"ec2:DeleteSnapshot\"\n",
    "      ],\n",
    "      \"Resource\": \"*\",\n",
    "      \"Condition\": {\n",
    "        \"StringLike\": {\n",
    "          \"ec2:ResourceTag/CSIVolumeSnapshotName\": \"*\"\n",
    "        }\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Action\": [\n",
    "        \"ec2:DeleteSnapshot\"\n",
    "      ],\n",
    "      \"Resource\": \"*\",\n",
    "      \"Condition\": {\n",
    "        \"StringLike\": {\n",
    "          \"ec2:ResourceTag/ebs.csi.aws.com/cluster\": \"true\"\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "try:\n",
    "    response = iam_client.create_policy(\n",
    "        PolicyName=amazon_eks_ebs_csi_driver_policy_name,\n",
    "        PolicyDocument=json.dumps(amazon_eks_ebs_csi_driver_policy)\n",
    "    )\n",
    "    amazon_eks_ebs_csi_driver_policy_arn = response['Policy']['Arn']\n",
    "except iam_client.exceptions.EntityAlreadyExistsException:\n",
    "    amazon_eks_ebs_csi_driver_policy_arn = f'arn:aws:iam::{aws_account_id}:policy/{amazon_eks_ebs_csi_driver_policy_name}'\n",
    "    response = iam_client.get_policy(PolicyArn=amazon_eks_ebs_csi_driver_policy_arn)\n",
    "\n",
    "    # TODO: If policy body is not the same, update. Code to update follows.\n",
    "    # iam_client.create_policy_version(\n",
    "    #     PolicyArn=policy_arn,\n",
    "    #     PolicyDocument=policy_document_json,\n",
    "    #     SetAsDefault=True\n",
    "    # )\n",
    "\n",
    "amazon_eks_ebs_csi_driver_policy_arn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e4e0ebf6-a8ef-42f7-bc4d-ba852964f0da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Role already exists. Updating the trust policy.\n"
     ]
    }
   ],
   "source": [
    "trust_policy = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\n",
    "                \"Federated\": f\"arn:aws:iam::{aws_account_id}:oidc-provider/oidc.eks.{REGION}.amazonaws.com/id/{oidc_id}\"\n",
    "            },\n",
    "            \"Action\": \"sts:AssumeRoleWithWebIdentity\",\n",
    "            \"Condition\": {\n",
    "                \"StringEquals\": {\n",
    "                    f\"oidc.eks.{REGION}.amazonaws.com/id/{oidc_id}:aud\": \"sts.amazonaws.com\",\n",
    "                    f\"oidc.eks.{REGION}.amazonaws.com/id/{oidc_id}:sub\": \"system:serviceaccount:kube-system:ebs-csi-controller-sa\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "try:\n",
    "    create_role_response = iam_client.create_role(\n",
    "        RoleName='AmazonEKS_EBS_CSI_DriverRole',\n",
    "        \n",
    "        AssumeRolePolicyDocument=json.dumps(trust_policy),\n",
    "        Description='Role for Amazon EKS EBS CSI Driver'\n",
    "    )\n",
    "    print(f\"Role created successfully: {create_role_response['Role']['Arn']}\")\n",
    "except iam_client.exceptions.EntityAlreadyExistsException:\n",
    "    print(\"Role already exists. Updating the trust policy.\")\n",
    "    iam_client.update_assume_role_policy(\n",
    "        RoleName='AmazonEKS_EBS_CSI_DriverRole',\n",
    "        PolicyDocument=json.dumps(trust_policy)\n",
    "    )\n",
    "\n",
    "attach_policy_response = iam_client.attach_role_policy(\n",
    "    RoleName='AmazonEKS_EBS_CSI_DriverRole',\n",
    "    PolicyArn='arn:aws:iam::aws:policy/service-role/AmazonEBSCSIDriverPolicy'\n",
    ")\n",
    "\n",
    "attach_policy_response = iam_client.attach_role_policy(\n",
    "    RoleName='AmazonEKS_EBS_CSI_DriverRole',\n",
    "    PolicyArn=amazon_eks_ebs_csi_driver_policy_arn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa839d9c-00b3-41f3-ba6b-f70575889176",
   "metadata": {},
   "source": [
    "# Install CSI Driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e2451fff-9fa9-4f93-b6c1-f3b26924ac3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "serviceaccount/ebs-csi-controller-sa created\n"
     ]
    }
   ],
   "source": [
    "!kubectl create sa ebs-csi-controller-sa -n kube-system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fc410c14-878b-4af9-b960-40503d451a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "serviceaccount/ebs-csi-controller-sa annotated\n"
     ]
    }
   ],
   "source": [
    "!kubectl annotate sa ebs-csi-controller-sa -n kube-system eks.amazonaws.com/role-arn=$ebs_csi_driver_role_arn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c15647b1-29f9-445e-a115-68db3fbec3d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "serviceaccount/ebs-csi-controller-sa annotated\n"
     ]
    }
   ],
   "source": [
    "!kubectl annotate sa ebs-csi-controller-sa -n kube-system meta.helm.sh/release-namespace=kube-system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "feef5937-e243-4ae6-84ee-5be572ad1c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "serviceaccount/ebs-csi-controller-sa annotated\n"
     ]
    }
   ],
   "source": [
    "!kubectl annotate sa ebs-csi-controller-sa -n kube-system meta.helm.sh/release-name=aws-ebs-csi-driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b0b85917-e62c-42aa-8f71-7a0001265efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "serviceaccount/ebs-csi-controller-sa labeled\n"
     ]
    }
   ],
   "source": [
    "!kubectl label sa ebs-csi-controller-sa -n kube-system app.kubernetes.io/managed-by=Helm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "65c7ca0d-34d7-4b6f-93cb-f39379235bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Release \"aws-ebs-csi-driver\" does not exist. Installing it now.\n",
      "NAME: aws-ebs-csi-driver\n",
      "LAST DEPLOYED: Tue Feb  4 22:28:54 2025\n",
      "NAMESPACE: kube-system\n",
      "STATUS: deployed\n",
      "REVISION: 1\n",
      "NOTES:\n",
      "To verify that aws-ebs-csi-driver has started, run:\n",
      "\n",
      "    kubectl get pod -n kube-system -l \"app.kubernetes.io/name=aws-ebs-csi-driver,app.kubernetes.io/instance=aws-ebs-csi-driver\"\n",
      "\n",
      "[ACTION REQUIRED] Update to the EBS CSI Driver IAM Policy\n",
      "\n",
      "Due to an upcoming change in handling of IAM polices for the CreateVolume API when creating a volume from an EBS snapshot, a change to your EBS CSI Driver policy may be needed. For more information and remediation steps, see GitHub issue #2190 (https://github.com/kubernetes-sigs/aws-ebs-csi-driver/issues/2190). This change affects all versions of the EBS CSI Driver and action may be required even on clusters where the driver is not upgraded.\n"
     ]
    }
   ],
   "source": [
    "!helm upgrade --install aws-ebs-csi-driver \\\n",
    "    --namespace kube-system \\\n",
    "    aws-ebs-csi-driver/aws-ebs-csi-driver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f599762-1e35-459c-a195-464eb9bd7f1c",
   "metadata": {},
   "source": [
    "### == End of Code Specific to AWS =="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bcb2b6-3c4b-4d3e-9d6c-2daf8baadfee",
   "metadata": {},
   "source": [
    "# Install Nvidia Device Plugin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2715339c-3b4c-4203-92e0-15b51e2fd75d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "daemonset.apps/nvidia-device-plugin-daemonset created\n"
     ]
    }
   ],
   "source": [
    "!kubectl apply -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/refs/heads/main/deployments/static/nvidia-device-plugin.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5a0d46-7a29-4c21-80d0-1ba0f5f9b5fd",
   "metadata": {},
   "source": [
    "# Apply Kubernetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e803a8bf-61c4-422f-8ed9-8c9f04a46916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Change this to Helm Charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c455d623-b4a9-48ef-a176-d2c91f677e62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('persistentvolume/pv-llm created\\n', '')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(f'/k8s/{CLUSTER_NAME}/{CLUSTER_NAME}-pv.yaml') as f:\n",
    "    pv = yaml.safe_load(f)\n",
    "    pv['spec']['awsElasticBlockStore']['volumeID'] = volume_id\n",
    "    with NamedTemporaryFile('w', suffix='.json', delete=False) as temp_file:\n",
    "        json.dump(pv, temp_file)\n",
    "        file_path = temp_file.name\n",
    "    result = subprocess.run(['kubectl', 'apply', '-f', file_path], capture_output=True, text=True)\n",
    "result.stdout, result.stderr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "906e9f90-cf29-458c-9c97-0fc49b0b5f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "persistentvolumeclaim/pvc-llm created\n"
     ]
    }
   ],
   "source": [
    "!kubectl apply -f /k8s/$CLUSTER_NAME/$CLUSTER_NAME-pvc.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ecd2f570-f9ce-45a3-b67f-8eca7e6bbd6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('deployment.apps/kubyterlab-llm-pod created\\n', '')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(f'/k8s/{CLUSTER_NAME}/{CLUSTER_NAME}-pod.yaml') as f:\n",
    "    pod = yaml.safe_load(f)\n",
    "    for i, container in enumerate(pod['spec']['template']['spec']['containers']):\n",
    "        pod['spec']['template']['spec']['containers'][i]['image'] = container['image']\\\n",
    "\t\t\t.replace('<AWS_ACCOUNT_ID>', aws_account_id)\\\n",
    "\t\t\t.replace('<REGION>', REGION)\n",
    "    with NamedTemporaryFile('w', suffix='.json', delete=False) as temp_file:\n",
    "        json.dump(pod, temp_file)\n",
    "        file_path = temp_file.name\n",
    "    result = subprocess.run(['kubectl', 'apply', '-f', file_path], capture_output=True, text=True)\n",
    "result.stdout, result.stderr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4de8b5d6-1a01-4b09-8352-13100477f174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "service/kubyterlab-llm-service created\n"
     ]
    }
   ],
   "source": [
    "!kubectl apply -f /k8s/$CLUSTER_NAME/kubyterlab-llm-service.yaml\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97f5d85-1064-4d39-b205-71fddb658689",
   "metadata": {},
   "source": [
    "# Get URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6ec37f9a-6ffd-4e4e-b9a2-f0955ae0423a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kubyterlab-llm-pod-85d468557b-cdvcf'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wait_until(get_one_running_pod, {}, lambda x: x is not None, timeout=8 * 60)\n",
    "pod_name = get_one_running_pod()\n",
    "assert pod_name is not None\n",
    "pod_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3cee78c8-55c3-43d9-bd54-258451b2520d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('a55cfe529cb114c519933d9d0e83a5c4-1462697016.ca-central-1.elb.amazonaws.com',\n",
       " '8888')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = !kubectl get service kubyterlab-llm-service --no-headers\n",
    "fields = re.split(r'\\s+', output[0])\n",
    "external_url = fields[3]\n",
    "port = fields[4].split('/')[0].split(':')[0]\n",
    "external_url, port"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "53321db9-5b60-4c9f-8895-9dc6be504761",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c77a61fd61c59473542e7cf6800dbdb7aa099f50727b3e4b'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wait_until(get_jupyter_token_from_pod, {\"pod_name\": pod_name}, lambda x: x is not None)\n",
    "token = get_jupyter_token_from_pod(pod_name)\n",
    "assert token is not None\n",
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8bf520ca-59d0-427f-8251-bc940cad8e46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://a55cfe529cb114c519933d9d0e83a5c4-1462697016.ca-central-1.elb.amazonaws.com:8888/lab?token=c77a61fd61c59473542e7cf6800dbdb7aa099f50727b3e4b'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'http://{external_url}:{port}/lab?token={token}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c7f019-64b1-4b2d-a343-b536fca16633",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "438b3f0b-0cc8-4aad-b796-5e7bb32fd110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulted container \"kubyterlab-llm\" out of: kubyterlab-llm, init-container (init)\n",
      "True\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Defaulted container \"kubyterlab-llm\" out of: kubyterlab-llm, init-container (init)\n",
    "# True\n",
    "# 1\n",
    "!kubectl exec $pod_name bash -- python3 -c 'import torch; print(torch.cuda.is_available()); print(torch.cuda.device_count())'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6bfde574-66a3-4a21-b26d-0cab8eac0893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulted container \"kubyterlab-llm\" out of: kubyterlab-llm, init-container (init)\n",
      "memory.total [MiB], memory.used [MiB], memory.free [MiB]\n",
      "15360 MiB, 1 MiB, 15095 MiB\n"
     ]
    }
   ],
   "source": [
    "!kubectl exec $pod_name bash -- bash -c 'nvidia-smi --query-gpu=memory.total,memory.used,memory.free --format=csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4ae1ff-9361-4709-8343-2b2310999403",
   "metadata": {},
   "source": [
    "# == End of Procedure =="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54d53ff-124d-409d-95f3-6985f7382654",
   "metadata": {},
   "source": [
    "## == Do Not Continue: rest of the code is only for reference for troubleshooting =="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bd4f0a-fa1d-4578-89d6-84b9c30c2179",
   "metadata": {},
   "source": [
    "# Troubleshoot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f10cb1-1b0f-4212-a689-052928a4924d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that for the following to work, you need to attach the policy AmazonSSMManagedInstanceCore, in the relevant cell above, during the deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "44b4b8fb-d7fb-4c9c-87ba-777bf78f1f7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'9528f9bd-ba96-49a8-b6f9-45df5433a905'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = !aws ssm send-command \\\n",
    "    --document-name \"AWS-RunShellScript\" \\\n",
    "    --targets \"Key=instanceIds,Values=i-02f3c1f8a118eb352\" \\\n",
    "    --parameters 'commands=[\"nvidia-smi --query-gpu=memory.total,memory.used,memory.free --format=csv\"]' \\\n",
    "    --region $REGION\n",
    "command_id = json.loads(''.join(output))['Command']['CommandId']\n",
    "command_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a45b62bf-644f-41ce-a4bf-f745311de116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory.total [MiB], memory.used [MiB], memory.free [MiB]\n",
      "15360 MiB, 1 MiB, 15095 MiB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output = !aws ssm list-command-invocations \\\n",
    "    --command-id $command_id \\\n",
    "    --details\n",
    "output = json.loads(''.join(output))['CommandInvocations'][0]['CommandPlugins'][0]['Output']\n",
    "for line in output.split('\\n'):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0480a8c3-9aad-4a76-9f62-20de23589501",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39f876b-ea67-4dd5-a9ee-92af8f218ce9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a8992595-dd14-4067-8607-3596e7fa746c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                 READY   STATUS    RESTARTS   AGE\n",
      "ebs-csi-controller-59b6797bf-mdw4c   5/5     Running   0          157m\n",
      "ebs-csi-controller-59b6797bf-psqv5   5/5     Running   0          157m\n",
      "ebs-csi-node-mxkzh                   3/3     Running   0          157m\n",
      "ebs-csi-node-nkgzs                   3/3     Running   0          157m\n",
      "ebs-csi-node-vkj5h                   3/3     Running   0          157m\n"
     ]
    }
   ],
   "source": [
    "!kubectl get pod -n kube-system -l \"app.kubernetes.io/name=aws-ebs-csi-driver,app.kubernetes.io/instance=aws-ebs-csi-driver\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3b9460f8-e4d4-4247-908f-22706f2d876d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                          GPU\n",
      "ip-10-0-1-121.ca-central-1.compute.internal   1\n",
      "ip-10-0-1-224.ca-central-1.compute.internal   <none>\n",
      "ip-10-0-1-70.ca-central-1.compute.internal    <none>\n"
     ]
    }
   ],
   "source": [
    "# NAME                                          GPU\n",
    "# ip-10-0-1-129.ca-central-1.compute.internal   <none>\n",
    "# ip-10-0-1-223.ca-central-1.compute.internal   <none>\n",
    "# ip-10-0-1-8.ca-central-1.compute.internal     1z\n",
    "!kubectl get nodes \"-o=custom-columns=NAME:.metadata.name,GPU:.status.allocatable.nvidia\\.com/gpu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5d4e2db7-eec8-484d-af50-ff23a339c8f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                          TAINTS\n",
      "ip-10-0-1-103.ca-central-1.compute.internal   <none>\n",
      "ip-10-0-1-167.ca-central-1.compute.internal   [map[effect:NoSchedule key:nvidia.com/gpu value:true]]\n",
      "ip-10-0-1-251.ca-central-1.compute.internal   <none>\n"
     ]
    }
   ],
   "source": [
    "# NAME                                          TAINTS\n",
    "# ip-10-0-1-107.ca-central-1.compute.internal   <none>\n",
    "# ip-10-0-1-175.ca-central-1.compute.internal   [map[effect:NoSchedule key:nvidia.com/gpu value:true]]\n",
    "# ip-10-0-1-90.ca-central-1.compute.internal    <none>\n",
    "!kubectl get nodes -o custom-columns=NAME:.metadata.name,TAINTS:.spec.taints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "35f0a916-0089-4030-97aa-a074fe340403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                     TYPE           CLUSTER-IP      EXTERNAL-IP                                                                  PORT(S)          AGE\n",
      "kubernetes               ClusterIP      172.20.0.1      <none>                                                                       443/TCP          10m\n",
      "kubyterlab-llm-service   LoadBalancer   172.20.31.136   ad6bb4a45883c4a1399cb117fa0c1814-1996712325.ca-central-1.elb.amazonaws.com   8888:31969/TCP   5m24s\n"
     ]
    }
   ],
   "source": [
    "# NAME                     TYPE           CLUSTER-IP     EXTERNAL-IP                                                                  PORT(S)          AGE\n",
    "# kubernetes               ClusterIP      172.20.0.1     <none>                                                                       443/TCP          37m\n",
    "# kubyterlab-llm-service   LoadBalancer   172.20.129.8   a4740e3e56bfe40ac81121bd46071903-1377611187.ca-central-1.elb.amazonaws.com   8888:31434/TCP   13s\n",
    "!kubectl get service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "fde9f7f4-b4cc-4834-9ccc-e69f70ffbbad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                 READY   UP-TO-DATE   AVAILABLE   AGE\n",
      "kubyterlab-llm-pod   0/1     1            0           54m\n"
     ]
    }
   ],
   "source": [
    "# NAME               READY   UP-TO-DATE   AVAILABLE   AGE\n",
    "# kubyterlab-llm-pod   1/1     1            1           31m\n",
    "!kubectl get deployments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "9c927c44-b74d-4fa4-839b-775a50c1c8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kubectl describe deployment kubyterlab-llm-pod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3d2ac1bd-3d25-4fa8-b2dd-fb7fa10e42d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                  READY   STATUS    RESTARTS       AGE\n",
      "kubyterlab-llm-pod-5b75b56d56-rpqj9   1/1     Running   2 (116s ago)   79m\n"
     ]
    }
   ],
   "source": [
    "!kubectl get pods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "0a2da721-c219-436c-8ba5-11d905c507d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME      STATUS   VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   VOLUMEATTRIBUTESCLASS   AGE\n",
      "pvc-llm   Bound    pv-llm   500Gi      RWO            manual         <unset>                 26m\n"
     ]
    }
   ],
   "source": [
    "!kubectl get pvc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6fbb74-4125-4ad3-96b4-f7117b82e1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl exec $pod_name bash -- nvidia-smi --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e4308b32-7f70-4861-81f6-7bc0702cbc87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulted container \"kubyterlab-llm\" out of: kubyterlab-llm, init-container (init)\n",
      "Mon Jan 13 22:15:38 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       On  |   00000000:00:1E.0 Off |                    0 |\n",
      "| N/A   25C    P8             11W /   70W |       1MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!kubectl exec $pod_name bash -- nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5a68c2d5-dc22-4529-9e8f-a7935f360340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulted container \"kubyterlab-llm\" out of: kubyterlab-llm, init-container (init)\n",
      "GPU 0: Tesla T4 (UUID: GPU-fe1acad6-41f9-c441-44ed-d4ead6db6dc2)\n"
     ]
    }
   ],
   "source": [
    "!kubectl exec $pod_name bash -- nvidia-smi -L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8d229310-97eb-4788-bf0e-22203dffe078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulted container \"kubyterlab-llm\" out of: kubyterlab-llm, init-container (init)\n",
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2024 NVIDIA Corporation\n",
      "Built on Wed_Aug_14_10:10:22_PDT_2024\n",
      "Cuda compilation tools, release 12.6, V12.6.68\n",
      "Build cuda_12.6.r12.6/compiler.34714021_0\n"
     ]
    }
   ],
   "source": [
    "!kubectl exec $pod_name bash -- nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722a4aa7-a0c3-462e-9e37-60230261fcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl describe pod $pod_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "116fd849-a569-472e-93d6-23624e93c7e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                   READY   STATUS    RESTARTS   AGE\n",
      "aws-node-cvzp2                         2/2     Running   0          4m59s\n",
      "aws-node-kmnjj                         2/2     Running   0          4m57s\n",
      "aws-node-zlmn8                         2/2     Running   0          6m35s\n",
      "coredns-749d5dbdd9-cdd8v               1/1     Running   0          8m29s\n",
      "coredns-749d5dbdd9-xtrc8               1/1     Running   0          8m29s\n",
      "ebs-csi-controller-59b6797bf-fhcph     5/5     Running   0          4m6s\n",
      "ebs-csi-controller-59b6797bf-w9mns     5/5     Running   0          4m6s\n",
      "ebs-csi-node-mp9sr                     3/3     Running   0          4m6s\n",
      "ebs-csi-node-mqt55                     3/3     Running   0          4m6s\n",
      "ebs-csi-node-q8dfl                     3/3     Running   0          4m6s\n",
      "kube-proxy-ctjcd                       1/1     Running   0          6m35s\n",
      "kube-proxy-nv8tx                       1/1     Running   0          4m57s\n",
      "kube-proxy-qmwmh                       1/1     Running   0          4m59s\n",
      "nvidia-device-plugin-daemonset-4b76p   1/1     Running   0          4m4s\n",
      "nvidia-device-plugin-daemonset-ltcdg   1/1     Running   0          4m4s\n",
      "nvidia-device-plugin-daemonset-wv2rd   1/1     Running   0          4m4s\n"
     ]
    }
   ],
   "source": [
    "!kubectl get pod -n kube-system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "90495808-55bb-4d5b-84e8-c00ca25250b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:          pvc-llm\n",
      "Namespace:     default\n",
      "StorageClass:  manual\n",
      "Status:        Bound\n",
      "Volume:        pv-llm\n",
      "Labels:        <none>\n",
      "Annotations:   pv.kubernetes.io/bind-completed: yes\n",
      "               pv.kubernetes.io/bound-by-controller: yes\n",
      "Finalizers:    [kubernetes.io/pvc-protection]\n",
      "Capacity:      10Gi\n",
      "Access Modes:  RWO\n",
      "VolumeMode:    Filesystem\n",
      "Used By:       kubyterlab-llm-pod-67f5cf95dc-s2mrn\n",
      "Events:        <none>\n"
     ]
    }
   ],
   "source": [
    "!kubectl describe pvc pvc-llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "da047557-f797-4492-b7f6-1cdcb16a3283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:            pv-llm\n",
      "Labels:          <none>\n",
      "Annotations:     pv.kubernetes.io/bound-by-controller: yes\n",
      "Finalizers:      [kubernetes.io/pv-protection]\n",
      "StorageClass:    manual\n",
      "Status:          Bound\n",
      "Claim:           default/pvc-llm\n",
      "Reclaim Policy:  Retain\n",
      "Access Modes:    RWO\n",
      "VolumeMode:      Filesystem\n",
      "Capacity:        10Gi\n",
      "Node Affinity:   <none>\n",
      "Message:         \n",
      "Source:\n",
      "    Type:       AWSElasticBlockStore (a Persistent Disk resource in AWS)\n",
      "    VolumeID:   vol-04ce08a2c05b8e1da\n",
      "    FSType:     ext4\n",
      "    Partition:  0\n",
      "    ReadOnly:   false\n",
      "Events:         <none>\n"
     ]
    }
   ],
   "source": [
    "!kubectl describe pv pv-llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "80827487-9836-450d-806d-5f56b00ab821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:                   kubyterlab-llm-pod\n",
      "Namespace:              default\n",
      "CreationTimestamp:      Mon, 02 Dec 2024 20:05:56 +0000\n",
      "Labels:                 <none>\n",
      "Annotations:            deployment.kubernetes.io/revision: 1\n",
      "Selector:               app=kubyterlab-llm\n",
      "Replicas:               1 desired | 1 updated | 1 total | 0 available | 1 unavailable\n",
      "StrategyType:           RollingUpdate\n",
      "MinReadySeconds:        0\n",
      "RollingUpdateStrategy:  25% max unavailable, 25% max surge\n",
      "Pod Template:\n",
      "  Labels:  app=kubyterlab-llm\n",
      "  Containers:\n",
      "   kubyterlab-llm:\n",
      "    Image:      kubyterlab:24.10\n",
      "    Port:       8888/TCP\n",
      "    Host Port:  0/TCP\n",
      "    Limits:\n",
      "      cpu:             1\n",
      "      memory:          8Gi\n",
      "      nvidia.com/gpu:  1\n",
      "    Requests:\n",
      "      cpu:             1\n",
      "      memory:          8Gi\n",
      "      nvidia.com/gpu:  1\n",
      "    Environment:\n",
      "      JUPYTERLAB_SETTINGS_DIR:  /jupyterlab/config\n",
      "      MISTRAL_MODEL:            /models/mistral\n",
      "    Mounts:\n",
      "      /corpus from pv-llm (rw)\n",
      "      /jupyterlab from pv-llm (rw)\n",
      "      /models from pv-llm (rw)\n",
      "  Volumes:\n",
      "   pv-llm:\n",
      "    Type:          PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)\n",
      "    ClaimName:     pvc-llm\n",
      "    ReadOnly:      false\n",
      "  Node-Selectors:  <none>\n",
      "  Tolerations:     nvidia.com/gpu:NoSchedule op=Exists\n",
      "Conditions:\n",
      "  Type           Status  Reason\n",
      "  ----           ------  ------\n",
      "  Available      False   MinimumReplicasUnavailable\n",
      "  Progressing    True    ReplicaSetUpdated\n",
      "OldReplicaSets:  <none>\n",
      "NewReplicaSet:   kubyterlab-llm-pod-67f5cf95dc (1/1 replicas created)\n",
      "Events:\n",
      "  Type    Reason             Age    From                   Message\n",
      "  ----    ------             ----   ----                   -------\n",
      "  Normal  ScalingReplicaSet  5m46s  deployment-controller  Scaled up replica set kubyterlab-llm-pod-67f5cf95dc to 1\n"
     ]
    }
   ],
   "source": [
    "!kubectl describe deployment kubyterlab-llm-pod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ce402031-be81-4578-8877-15bcd40ad8e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                          STATUS   ROLES    AGE   VERSION\n",
      "ip-10-0-1-100.ca-central-1.compute.internal   Ready    <none>   47m   v1.30.7-eks-59bf375\n",
      "ip-10-0-1-25.ca-central-1.compute.internal    Ready    <none>   47m   v1.30.7-eks-59bf375\n",
      "ip-10-0-1-50.ca-central-1.compute.internal    Ready    <none>   48m   v1.30.7-eks-59bf375\n"
     ]
    }
   ],
   "source": [
    "!kubectl get nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d0bb4314-1002-45c7-b0c2-4d706b5d533f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME               DATA   AGE\n",
      "kube-root-ca.crt   1      50m\n"
     ]
    }
   ],
   "source": [
    "!kubectl get configmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6d65a766-6f17-413c-a7f5-9f8e07fd0a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                     READY   STATUS    RESTARTS   AGE\n",
      "pod/ebs-csi-controller-59b6797bf-vfvs2   5/5     Running   0          47m\n",
      "pod/ebs-csi-controller-59b6797bf-xt7fv   5/5     Running   0          47m\n",
      "pod/ebs-csi-node-dwzz7                   3/3     Running   0          47m\n",
      "pod/ebs-csi-node-gplqp                   3/3     Running   0          47m\n",
      "pod/ebs-csi-node-l5nh6                   3/3     Running   0          47m\n",
      "\n",
      "NAME                          DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE\n",
      "daemonset.apps/ebs-csi-node   3         3         3       3            3           kubernetes.io/os=linux   47m\n",
      "\n",
      "NAME                                 READY   UP-TO-DATE   AVAILABLE   AGE\n",
      "deployment.apps/ebs-csi-controller   2/2     2            2           47m\n",
      "\n",
      "NAME                                           DESIRED   CURRENT   READY   AGE\n",
      "replicaset.apps/ebs-csi-controller-59b6797bf   2         2         2       47m\n"
     ]
    }
   ],
   "source": [
    "!kubectl get all -l app.kubernetes.io/name=aws-ebs-csi-driver -n kube-system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3bf4b6bf-4a67-48e4-ba16-8207e90b7fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"OpenIDConnectProviderList\": [\n",
      "        {\n",
      "            \"Arn\": \"arn:aws:iam::275678099358:oidc-provider/oidc.eks.ca-central-1.amazonaws.com/id/8D7619520212428BD59C46B20BF19338\"\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!aws iam list-open-id-connect-providers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "65c00bf0-9326-417c-964f-530ab62c85da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 pods, using pod/ebs-csi-controller-59b6797bf-vfvs2\n",
      "I0113 21:28:06.936951       1 main.go:153] \"Initializing metadata\"\n",
      "I0113 21:28:06.943481       1 metadata.go:48] \"Retrieved metadata from IMDS\"\n",
      "I0113 21:28:06.944585       1 driver.go:69] \"Driver Information\" Driver=\"ebs.csi.aws.com\" Version=\"v1.38.1\"\n",
      "I0113 22:01:16.236299       1 controller.go:410] \"ControllerPublishVolume: attaching\" volumeID=\"vol-0ed5d3cc8cb5a989e\" nodeID=\"i-0fd8ede93d6ac59a4\"\n",
      "I0113 22:01:17.882498       1 controller.go:419] \"ControllerPublishVolume: attached\" volumeID=\"vol-0ed5d3cc8cb5a989e\" nodeID=\"i-0fd8ede93d6ac59a4\" devicePath=\"/dev/xvdaa\"\n"
     ]
    }
   ],
   "source": [
    "!kubectl logs deployment/ebs-csi-controller -n kube-system -c ebs-plugin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8ab7f84b-398b-41fb-9c35-30f40c179f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 pods, using pod/ebs-csi-node-l5nh6\n",
      "I0113 21:27:55.828513       1 main.go:153] \"Initializing metadata\"\n",
      "I0113 21:27:55.832956       1 metadata.go:48] \"Retrieved metadata from IMDS\"\n",
      "I0113 21:27:55.833450       1 driver.go:69] \"Driver Information\" Driver=\"ebs.csi.aws.com\" Version=\"v1.38.1\"\n",
      "E0113 21:27:56.896438       1 node.go:856] \"Unexpected failure when attempting to remove node taint(s)\" err=\"isAllocatableSet: driver not found on node ip-10-0-1-50.ca-central-1.compute.internal\"\n",
      "I0113 21:27:57.410789       1 node.go:936] \"CSINode Allocatable value is set\" nodeName=\"ip-10-0-1-50.ca-central-1.compute.internal\" count=24\n",
      "I0113 22:01:18.817732       1 node.go:204] \"NodeStageVolume: invalid partition config, will ignore.\" partition=\"0\"\n",
      "I0113 22:01:18.980008       1 mount_linux.go:295] Detected OS without systemd\n"
     ]
    }
   ],
   "source": [
    "!kubectl logs daemonset/ebs-csi-node -n kube-system -c ebs-plugin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f2fd1fcf-f18f-4b19-abeb-1c207ddd58de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 pods, using pod/ebs-csi-controller-59b6797bf-vfvs2\n",
      "W0113 21:28:09.566495       1 feature_gate.go:354] Setting GA feature gate Topology=true. It will be removed in a future release.\n",
      "I0113 21:28:09.566789       1 feature_gate.go:387] feature gates: {map[Topology:true]}\n",
      "I0113 21:28:09.566840       1 csi-provisioner.go:154] Version: v5.1.0\n",
      "I0113 21:28:09.566851       1 csi-provisioner.go:177] Building kube configs for running in cluster...\n",
      "I0113 21:28:09.568886       1 common.go:143] \"Probing CSI driver for readiness\"\n",
      "I0113 21:28:09.598446       1 csi-provisioner.go:230] Detected CSI driver ebs.csi.aws.com\n",
      "I0113 21:28:09.598619       1 csi-provisioner.go:240] Supports migration from in-tree plugin: kubernetes.io/aws-ebs\n",
      "I0113 21:28:09.600672       1 common.go:143] \"Probing CSI driver for readiness\"\n",
      "I0113 21:28:09.603385       1 csi-provisioner.go:299] CSI driver supports PUBLISH_UNPUBLISH_VOLUME, watching VolumeAttachments\n",
      "I0113 21:28:09.604750       1 controller.go:744] \"Using saving PVs to API server in background\"\n",
      "I0113 21:28:09.605162       1 leaderelection.go:254] attempting to acquire leader lease kube-system/ebs-csi-aws-com...\n",
      "I0113 21:28:09.627768       1 leaderelection.go:268] successfully acquired lease kube-system/ebs-csi-aws-com\n",
      "I0113 21:28:09.629736       1 leader_election.go:184] \"became leader, starting\"\n",
      "I0113 21:28:09.629845       1 envvar.go:172] \"Feature gate default state\" feature=\"WatchListClient\" enabled=false\n",
      "I0113 21:28:09.629863       1 envvar.go:172] \"Feature gate default state\" feature=\"InformerResourceVersion\" enabled=false\n",
      "I0113 21:28:09.640053       1 reflector.go:368] Caches populated for *v1.StorageClass from k8s.io/client-go@v0.31.0/tools/cache/reflector.go:243\n",
      "I0113 21:28:09.651560       1 reflector.go:368] Caches populated for *v1.PersistentVolumeClaim from k8s.io/client-go@v0.31.0/tools/cache/reflector.go:243\n",
      "I0113 21:28:09.656122       1 reflector.go:368] Caches populated for *v1.Node from k8s.io/client-go@v0.31.0/tools/cache/reflector.go:243\n",
      "I0113 21:28:09.656502       1 reflector.go:368] Caches populated for *v1.VolumeAttachment from k8s.io/client-go@v0.31.0/tools/cache/reflector.go:243\n",
      "I0113 21:28:09.657442       1 reflector.go:368] Caches populated for *v1.CSINode from k8s.io/client-go@v0.31.0/tools/cache/reflector.go:243\n",
      "I0113 21:28:09.729875       1 controller.go:824] \"Starting provisioner controller\" component=\"ebs.csi.aws.com_ebs-csi-controller-59b6797bf-vfvs2_1c292ac0-a9d9-48ff-8621-9ba1f75e01b8\"\n",
      "I0113 21:28:09.729957       1 volume_store.go:98] \"Starting save volume queue\"\n",
      "I0113 21:28:09.734566       1 reflector.go:368] Caches populated for *v1.StorageClass from k8s.io/client-go@v0.31.0/tools/cache/reflector.go:243\n",
      "I0113 21:28:09.734567       1 reflector.go:368] Caches populated for *v1.PersistentVolume from k8s.io/client-go@v0.31.0/tools/cache/reflector.go:243\n",
      "I0113 21:28:09.831005       1 controller.go:873] \"Started provisioner controller\" component=\"ebs.csi.aws.com_ebs-csi-controller-59b6797bf-vfvs2_1c292ac0-a9d9-48ff-8621-9ba1f75e01b8\"\n"
     ]
    }
   ],
   "source": [
    "!kubectl logs deployment/ebs-csi-controller -n kube-system -c csi-provisioner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "49134a19-5818-4030-8a7a-6c9e04d6157c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 pods, using pod/ebs-csi-controller-59b6797bf-vfvs2\n",
      "I0113 21:28:12.211572       1 main.go:109] \"Version\" version=\"v4.7.0\"\n",
      "I0113 21:28:12.215865       1 common.go:143] \"Probing CSI driver for readiness\"\n",
      "I0113 21:28:12.238251       1 main.go:169] \"CSI driver name\" driver=\"ebs.csi.aws.com\"\n",
      "I0113 21:28:12.240448       1 common.go:143] \"Probing CSI driver for readiness\"\n",
      "I0113 21:28:12.242734       1 main.go:249] \"CSI driver supports ControllerPublishUnpublish, using real CSI handler\" driver=\"ebs.csi.aws.com\"\n",
      "I0113 21:28:12.243624       1 leaderelection.go:254] attempting to acquire leader lease kube-system/external-attacher-leader-ebs-csi-aws-com...\n",
      "I0113 21:28:12.265555       1 leaderelection.go:268] successfully acquired lease kube-system/external-attacher-leader-ebs-csi-aws-com\n",
      "I0113 21:28:12.266787       1 leader_election.go:184] \"became leader, starting\"\n",
      "I0113 21:28:12.266811       1 controller.go:129] \"Starting CSI attacher\"\n",
      "I0113 21:28:12.267257       1 envvar.go:172] \"Feature gate default state\" feature=\"InformerResourceVersion\" enabled=false\n",
      "I0113 21:28:12.267536       1 envvar.go:172] \"Feature gate default state\" feature=\"WatchListClient\" enabled=false\n",
      "I0113 21:28:12.271734       1 reflector.go:368] Caches populated for *v1.PersistentVolume from k8s.io/client-go@v0.31.0/tools/cache/reflector.go:243\n",
      "I0113 21:28:12.272191       1 reflector.go:368] Caches populated for *v1.CSINode from k8s.io/client-go@v0.31.0/tools/cache/reflector.go:243\n",
      "I0113 21:28:12.272546       1 reflector.go:368] Caches populated for *v1.VolumeAttachment from k8s.io/client-go@v0.31.0/tools/cache/reflector.go:243\n",
      "I0113 22:01:16.209140       1 csi_handler.go:261] \"Attaching\" VolumeAttachment=\"csi-ed0e5d922afcde1b934ffafc0f0d3a9543d0760d7dfa4ce99585f3fc2c2922b0\"\n",
      "I0113 22:01:17.882946       1 csi_handler.go:273] \"Attached\" VolumeAttachment=\"csi-ed0e5d922afcde1b934ffafc0f0d3a9543d0760d7dfa4ce99585f3fc2c2922b0\"\n"
     ]
    }
   ],
   "source": [
    "!kubectl logs deployment/ebs-csi-controller -n kube-system -c csi-attacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6fdb0f08-aba7-4d63-8e01-2b25d1b06b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"eks.amazonaws.com/role-arn\":\"\",\"meta.helm.sh/release-name\":\"aws-ebs-csi-driver\",\"meta.helm.sh/release-namespace\":\"kube-system\"}"
     ]
    }
   ],
   "source": [
    "!kubectl get sa ebs-csi-controller-sa -n kube-system -o jsonpath='{.metadata.annotations}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a85d88b-946c-40e7-8b59-0e69a4567d5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
