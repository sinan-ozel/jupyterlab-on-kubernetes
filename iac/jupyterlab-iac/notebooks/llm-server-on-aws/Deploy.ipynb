{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d02ca0b4-1c4a-4b8e-a7a9-8fe3c9ba40b1",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38534870-6679-40ae-bcf1-f778377e5998",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from time import sleep\n",
    "from time import time as unixtime\n",
    "from typing import Callable, List\n",
    "import random\n",
    "from math import ceil\n",
    "import yaml\n",
    "import string\n",
    "import json\n",
    "from urllib.parse import urlparse\n",
    "from time import sleep\n",
    "import re\n",
    "from operator import itemgetter\n",
    "import subprocess\n",
    "from tempfile import NamedTemporaryFile\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "import boto3\n",
    "import kubernetes\n",
    "from kubernetes.client.rest import ApiException\n",
    "\n",
    "from pyhelm3 import Client as HelmClient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a98f1d3-e2b6-4c0a-ad3d-c07f5416d684",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv('/.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08774840-c227-48d4-844c-165c6f5dc8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import wait_until\n",
    "from helper.ec2 import (get_vpcs_ids, get_internet_gateway_ids_attached_to_vpc, \n",
    "                        get_route_table_ids_for_vpc, route_to_gateway_exists, \n",
    "                        get_subnet_ids_in_vpc, get_security_group_ids)\n",
    "from helper.k8s import (get_one_running_pod, get_jupyter_token_from_pod)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1800b74-d019-445d-942e-7223e770f332",
   "metadata": {},
   "source": [
    "# Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3b9f759-4820-40ad-8916-e935d6e57232",
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = 'ca-central-1'\n",
    "CLUSTER_NAME = 'kubyterlab-llm'\n",
    "EBS_VOLUME_SIZE = 500 # GiB\n",
    "TAGS = {'cluster': CLUSTER_NAME, 'purpose': 'llm'}  # Do not change the keys, they are hardcoded throughout.\n",
    "CLUSTER_TAGS = {'cluster': CLUSTER_NAME}\n",
    "VOLUME_FILTERS = [\n",
    "    {'Name': f'tag:purpose', \n",
    "     'Values': ['kubyterlab-llm', 'llm']}]\n",
    "K8S_VERSION = os.environ['K8S_VERSION']  # '1.30'\n",
    "K8S_VERSION = '.'.join(K8S_VERSION.split('.')[:2]) if len(K8S_VERSION.split('.')) > 2 else K8S_VERSION\n",
    "INSTANCE_TYPES = {'gpu': ['g4dn.2xlarge'], 'default': ['t3.medium']}\n",
    "ALLOWED_PORTS = [80, 443, 22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d859c1f-c31c-43b5-9a3a-fa9ed0ae7006",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_all_vpcs_available(response: dict) -> True:\n",
    "    if not 'Vpcs' in response:\n",
    "        raise ValueError\n",
    "    return all([vpc.get('State', '') == 'available' for vpc in response['Vpcs']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70ff5d92-c3be-4255-9d40-afea872ff121",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_cluster_active(response: dict) -> bool:\n",
    "    status = response['cluster']['status']\n",
    "    clear_output(wait=True)\n",
    "    display(status)\n",
    "    return status == 'ACTIVE'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8983294-0e11-457c-abb6-2ec996243d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_node_group_active(response: dict) -> bool:\n",
    "    status = response['nodegroup']['status']\n",
    "    clear_output(wait=True)\n",
    "    display(status)\n",
    "    return status in ['ACTIVE', 'CREATE_FAILED']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9938e8cf-fc3a-42f8-819b-7748181fffc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_snapshot_completed(response: dict) -> bool:\n",
    "    state = response['Snapshots'][0]['State']\n",
    "    clear_output(wait=True)\n",
    "    display(state)\n",
    "    return state.lower() == 'completed'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f79a307-fbfa-4d1e-9d75-9f77405a7dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_volume_available(response: dict) -> bool:\n",
    "    state = response['Volumes'][0]['State']\n",
    "    clear_output(wait=True)\n",
    "    display(state)\n",
    "    return state.lower() == 'available'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73ff831-37b6-4fbf-9aad-1068cfb8efe7",
   "metadata": {},
   "source": [
    "# Instantiate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97c98c25-b95b-40da-be00-f65f5952d5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = boto3.Session(region_name=REGION)\n",
    "eks_client = session.client('eks')\n",
    "ec2_client = session.client('ec2')\n",
    "iam_client = session.client('iam')\n",
    "\n",
    "aws_account_id = boto3.client('sts').get_caller_identity().get('Account')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbf7c2f-2e86-48c9-b369-1047bd8fea6c",
   "metadata": {},
   "source": [
    "# Create or Restore Volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "025247fd-7e39-46c3-ba9c-8bf58a3dcc19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    response = ec2_client.describe_volumes(Filters=VOLUME_FILTERS)\n",
    "    volumes = response.get('Volumes', [])\n",
    "except RuntimeError:\n",
    "    volumes = []\n",
    "volume_ids = [volume['VolumeId'] for volume in volumes]\n",
    "volume_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "728edb7d-b012-4778-8f98-34f00f520042",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(volume_ids) <= 1  # TODO: Get the latest one if more than one.\n",
    "if volume_ids:\n",
    "    volume_id = volume_ids[0]\n",
    "    availability_zone = volumes[0]['AvailabilityZone']\n",
    "else:\n",
    "    volume_id = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf65cde1-f0ad-4b9a-9c91-49c20d09b2f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "snap-0a8e03d57c305cff3\n"
     ]
    }
   ],
   "source": [
    "if not volume_id:\n",
    "    response = ec2_client.describe_snapshots(Filters=VOLUME_FILTERS)\n",
    "    snapshots = response.get('Snapshots', [])\n",
    "    if snapshots:\n",
    "        sorted_snapshots = sorted(snapshots, key=itemgetter('StartTime'), reverse=True)\n",
    "        snapshot_id = sorted_snapshots[0]['SnapshotId']\n",
    "        print(snapshot_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5d85302-63d3-4ef2-a07e-bd59dc564768",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'available'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('vol-0848b8a532803d785', 'ca-central-1a')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not volume_id:\n",
    "    response = ec2_client.describe_availability_zones()\n",
    "    availability_zones = response['AvailabilityZones']\n",
    "    availability_zone = availability_zones[0]['ZoneName']\n",
    "    # availability_zone = f'{REGION}a'\n",
    "    if snapshots:\n",
    "        # TODO: Change this to the latest snapshot!!\n",
    "        response = ec2_client.create_volume(\n",
    "            SnapshotId=snapshot_id,\n",
    "            Size=EBS_VOLUME_SIZE,\n",
    "            AvailabilityZone=availability_zone,\n",
    "            VolumeType='gp3',\n",
    "            TagSpecifications=[\n",
    "                {\n",
    "                    'ResourceType': 'volume',\n",
    "                    'Tags': [{'Key': 'purpose', 'Value': 'llm'}]\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "    else:\n",
    "        response = ec2_client.create_volume(\n",
    "            Size=EBS_VOLUME_SIZE,\n",
    "            AvailabilityZone=availability_zones[0]['ZoneName'],\n",
    "            VolumeType='gp3',\n",
    "            TagSpecifications=[\n",
    "                {\n",
    "                    'ResourceType': 'volume',\n",
    "                    'Tags': [{'Key': 'purpose', 'Value': 'llm'}]\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "    wait_until(ec2_client.describe_volumes, {'VolumeIds': [response['VolumeId']]}, is_volume_available)\n",
    "    volume_id = response['VolumeId']\n",
    "volume_id, availability_zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e815d5e-1d1e-4f94-ac53-9fa53edbdd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Change the parts below to use Terraform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbda3f9d-a108-48f4-8411-f7dd2e0e1f43",
   "metadata": {},
   "source": [
    "# Create VPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "429b8e60-e14a-460b-a448-45bc392e6c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating VPC...\n",
      "Creating Internet Gateway...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('vpc-012915162f7ff797f', 'igw-05ff3ff9b42e78077', 'rtb-0e66400b1c8805af4')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vpc_ids = get_vpcs_ids(ec2_client, TAGS)\n",
    "vpc_exists = len(vpc_ids) > 0\n",
    "\n",
    "if len(vpc_ids) > 1:\n",
    "    raise RuntimeError\n",
    "elif vpc_exists:\n",
    "    vpc_id = vpc_ids[0]\n",
    "\n",
    "# Create VPC\n",
    "if not vpc_exists:\n",
    "    print('Creating VPC...')\n",
    "    vpc_response = ec2_client.create_vpc(\n",
    "        CidrBlock='10.0.0.0/16',\n",
    "        TagSpecifications=[\n",
    "            {\n",
    "                'ResourceType': 'vpc',\n",
    "                'Tags': [{'Key': tag, 'Value': TAGS[tag] }for tag in TAGS]\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    vpc_id = vpc_response['Vpc']['VpcId']\n",
    "wait_until(ec2_client.describe_vpcs, {'VpcIds': [vpc_id]}, check_all_vpcs_available)\n",
    "\n",
    "# Create Internet Gateway\n",
    "igw_ids = get_internet_gateway_ids_attached_to_vpc(ec2_client, vpc_id)\n",
    "igw_exists = len(igw_ids) > 0\n",
    "\n",
    "if len(igw_ids) > 1:\n",
    "    raise RuntimeError\n",
    "elif igw_exists:\n",
    "    igw_id = igw_ids[0]\n",
    "\n",
    "\n",
    "if not igw_exists:\n",
    "    print('Creating Internet Gateway...')\n",
    "    # Create an Internet Gateway\n",
    "    igw_response = ec2_client.create_internet_gateway()\n",
    "    igw_id = igw_response['InternetGateway']['InternetGatewayId']\n",
    "    \n",
    "    # Attach Internet Gateway to VPC\n",
    "    ec2_client.attach_internet_gateway(\n",
    "        InternetGatewayId=igw_id,\n",
    "        VpcId=vpc_id\n",
    "    )\n",
    "\n",
    "# Create a route table\n",
    "route_table_ids = get_route_table_ids_for_vpc(ec2_client, vpc_id)\n",
    "route_table_exists = len(route_table_ids) > 0\n",
    "\n",
    "if not route_table_exists:\n",
    "    print('Creating a route table...')\n",
    "    # Create a route table\n",
    "    route_table_response = ec2_client.create_route_table(VpcId=vpc_id)\n",
    "    route_table_id = route_table_response['RouteTable']['RouteTableId']\n",
    "\n",
    "is_route_created = False\n",
    "for route_table_id in route_table_ids:\n",
    "    if route_to_gateway_exists(ec2_client, route_table_id, igw_id):\n",
    "        is_route_created = True\n",
    "        break\n",
    "\n",
    "if not is_route_created:\n",
    "    # Create a route to the Internet Gateway\n",
    "    ec2_client.create_route(\n",
    "        RouteTableId=route_table_id,\n",
    "        DestinationCidrBlock='0.0.0.0/0',\n",
    "        GatewayId=igw_id\n",
    "    )\n",
    "\n",
    "vpc_id, igw_id, route_table_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbfdb24-1435-47bb-8293-b87b9d4075b7",
   "metadata": {},
   "source": [
    "# Create Subnets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b32166a-ce50-45f6-aac3-3f809836f103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating subnet #1...\n",
      "Creating subnet #2...\n"
     ]
    }
   ],
   "source": [
    "subnet_ids = get_subnet_ids_in_vpc(ec2_client, vpc_id)\n",
    "min_subnet_count = 2\n",
    "current_subnet_count = len(subnet_ids)\n",
    "\n",
    "# if current_subnet_count < min_subnet_count:\n",
    "    # response = ec2_client.describe_availability_zones()\n",
    "    \n",
    "    # availability_zones = [az['ZoneName'] for az in response['AvailabilityZones']]\n",
    "    # random.shuffle(availability_zones)\n",
    "    # TODO: Following logic is still necessary for situations with larger subnets. Change to something reasonable.\n",
    "    # availability_zones *= ceil(min_subnet_count / len(availability_zones))\n",
    "    \n",
    "\n",
    "while current_subnet_count < min_subnet_count:\n",
    "    i = current_subnet_count + 1\n",
    "    print(f'Creating subnet #{i}...')\n",
    "    subnet_response = ec2_client.create_subnet(\n",
    "        VpcId=vpc_id,\n",
    "        CidrBlock=f'10.0.{i}.0/24',\n",
    "        # AvailabilityZone=availability_zone,\n",
    "        AvailabilityZone=availability_zones[current_subnet_count]['ZoneName'],\n",
    "        TagSpecifications=[\n",
    "            {\n",
    "                'ResourceType': 'subnet',\n",
    "                'Tags': [{'Key': tag, 'Value': TAGS[tag]} for tag in TAGS]\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    subnet_id = subnet_response['Subnet']['SubnetId']\n",
    "    subnet_ids.append(subnet_id)\n",
    "    current_subnet_count = len(subnet_ids)\n",
    "\n",
    "# check if any subnet is public, add the table to first subnet if not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c837c746-05e1-432f-8619-e3c0581650f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "subnets_response = ec2_client.describe_subnets(SubnetIds=subnet_ids)\n",
    "assert len(subnets_response) == len(subnet_ids) == min_subnet_count\n",
    "is_any_subnet_open_to_public = False\n",
    "for subnet in subnets_response['Subnets']:\n",
    "    if subnets_response.get('MapPublicIpOnLaunch', False):\n",
    "        # I could also look at the route tables, and see if they are assigned to any subnet. ec2_client.describe_route_tables(RouteTableIds=[route_table_id]) RouteTables.SubnetId (optional parameter)\n",
    "        public_subnet_id = subnet['SubnetId']\n",
    "        is_any_subnet_open_to_public = True\n",
    "\n",
    "if not is_any_subnet_open_to_public:\n",
    "    for subnet in subnets_response['Subnets']:\n",
    "        if subnet['CidrBlock'] == '10.0.1.0/24':\n",
    "            public_subnet_id = subnet['SubnetId']\n",
    "\n",
    "            # Associate the public subnet with the route table\n",
    "            ec2_client.associate_route_table(\n",
    "                RouteTableId=route_table_id,\n",
    "                SubnetId=public_subnet_id\n",
    "            )\n",
    "            \n",
    "            # Modify the public subnet to auto-assign public IPs\n",
    "            ec2_client.modify_subnet_attribute(\n",
    "                SubnetId=public_subnet_id,\n",
    "                MapPublicIpOnLaunch={\"Value\": True}\n",
    "            )\n",
    "            break\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95af95c-f3a4-46d3-b5f8-b7d953eda3a9",
   "metadata": {},
   "source": [
    "# Create Security Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf11870e-3dee-4564-9c82-c78a0262ce77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Security Group...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'sg-04c79273796e5a522'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "security_group_ids = get_security_group_ids(ec2_client, TAGS)\n",
    "\n",
    "security_group_exists = len(security_group_ids) > 0\n",
    "\n",
    "if len(security_group_ids) > 1:\n",
    "    raise RuntimeError\n",
    "elif security_group_exists:\n",
    "    security_group_id = security_group_ids[0]\n",
    "\n",
    "# The security group gets torn down when deleted, so there is no need to check the rules and rewrite all of them.\n",
    "\n",
    "if not security_group_exists:\n",
    "    print('Creating Security Group...')\n",
    "    response = ec2_client.create_security_group(\n",
    "        GroupName=f'eks-cluster-sg-{CLUSTER_NAME}',\n",
    "        TagSpecifications=[\n",
    "            {\n",
    "                'ResourceType': 'security-group', \n",
    "                'Tags': [{'Key': tag, 'Value': TAGS[tag]} for tag in TAGS]\n",
    "            }\n",
    "        ],\n",
    "        Description=f'Security group for EKS cluster: {CLUSTER_NAME}',\n",
    "        VpcId=vpc_id\n",
    "    )\n",
    "    security_group_id = response['GroupId']\n",
    "\n",
    "    # TODO: Can I make the CidrIp more restrictive for the next deployment? Load Balancer needs to have a static IP, probably through the Kubernetes YAML?\n",
    "    for port in ALLOWED_PORTS:\n",
    "        ec2_client.authorize_security_group_ingress(\n",
    "            GroupId=security_group_id,\n",
    "            IpPermissions=[\n",
    "                {\n",
    "                    'IpProtocol': 'tcp',\n",
    "                    'FromPort': port,\n",
    "                    'ToPort': port,\n",
    "                    'IpRanges': [{'CidrIp': '0.0.0.0/0'}]\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "    \n",
    "security_group_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6a6e50f1-8540-4dcf-9aa6-46c0fd668aea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('vol-0848b8a532803d785',\n",
       " 'vpc-012915162f7ff797f',\n",
       " 'igw-05ff3ff9b42e78077',\n",
       " 'rtb-0e66400b1c8805af4',\n",
       " 'subnet-0d65238d6cf1eee0c',\n",
       " ['subnet-0d65238d6cf1eee0c', 'subnet-0b4a19e1b8d35b740'],\n",
       " 'sg-04c79273796e5a522')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "volume_id, vpc_id, igw_id, route_table_id, public_subnet_id, subnet_ids, security_group_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5e3e6a-1650-4a59-91b7-bff33b937a52",
   "metadata": {},
   "source": [
    "# Create Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e5e5fec8-eea5-4ef7-806c-4bdea1f61845",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ACTIVE'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tear_down_and_rebuild = False\n",
    "response = eks_client.list_clusters()\n",
    "clusters = response['clusters']\n",
    "if CLUSTER_NAME not in clusters:\n",
    "    eks_client.create_cluster(name=CLUSTER_NAME, \n",
    "                              version=K8S_VERSION, \n",
    "                              roleArn=f'arn:aws:iam::{aws_account_id}:role/EKS_Cluster_Role', \n",
    "                              resourcesVpcConfig={'subnetIds': subnet_ids,\n",
    "                                                  'securityGroupIds': [security_group_id],\n",
    "                                                  'endpointPublicAccess': True,\n",
    "                                                  'endpointPrivateAccess': False\n",
    "                              },\n",
    "                              tags=TAGS,\n",
    "                             )\n",
    "else:\n",
    "    if tear_down_and_rebuild:\n",
    "        pass\n",
    "        # TODO: Create a new cluster with a temp name.\n",
    "        # Wait until the cluster has finished forming\n",
    "        # Delete the old cluster\n",
    "        # Wait until deletion is complete\n",
    "        # Rename the new cluster\n",
    "        \n",
    "wait_until(eks_client.describe_cluster, {'name': CLUSTER_NAME}, is_cluster_active, timeout=7 * 60)\n",
    "response = eks_client.describe_cluster(name=CLUSTER_NAME)\n",
    "assert response['cluster']['status'] == 'ACTIVE'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7a812b-4a62-4b3b-a88e-90b0db82a194",
   "metadata": {},
   "source": [
    "# Create IAM Role for Node Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "49bcf925-9e7a-47af-9d1a-d160959148cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Role EKS_Cluster_Role already exists. Arn: arn:aws:iam::275678099358:role/EKS_Cluster_Role.\n",
      "Attached policy AmazonEKSWorkerNodePolicy to role EKS_Cluster_Role.\n",
      "Attached policy AmazonEC2ContainerRegistryReadOnly to role EKS_Cluster_Role.\n",
      "Attached policy AmazonEKS_CNI_Policy to role EKS_Cluster_Role.\n",
      "Attached policy AmazonEKSClusterPolicy to role EKS_Cluster_Role.\n"
     ]
    }
   ],
   "source": [
    "# Define the role name\n",
    "role_name = 'EKS_Cluster_Role'  # WARNING: I get a weird error if I call this role anything else. The call looks for this particular name, and I do not know how to override it.\n",
    "\n",
    "# Create the trust policy for the role\n",
    "trust_policy = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\n",
    "                \"Service\": [\n",
    "                    \"eks.amazonaws.com\",\n",
    "                    \"ec2.amazonaws.com\"\n",
    "                ]\n",
    "            },\n",
    "            \"Action\": \"sts:AssumeRole\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create the IAM role\n",
    "try:\n",
    "    response = iam_client.create_role(\n",
    "        RoleName=role_name,\n",
    "        AssumeRolePolicyDocument=json.dumps(trust_policy),\n",
    "        Description='Role for EKS Node Group'\n",
    "    )\n",
    "    node_role_arn = response['Role']['Arn']\n",
    "    print(f\"Created role: {node_role_arn}\")\n",
    "except iam_client.exceptions.EntityAlreadyExistsException:\n",
    "    response = iam_client.get_role(RoleName=role_name)\n",
    "    node_role_arn = response['Role']['Arn']\n",
    "    print(f\"Role {role_name} already exists. Arn: {node_role_arn}.\")\n",
    "    # TODO: Check if trust_policy is correct.\n",
    "\n",
    "# Attach necessary policies\n",
    "policies = [\n",
    "    'AmazonEKSWorkerNodePolicy',\n",
    "    'AmazonEC2ContainerRegistryReadOnly',\n",
    "    'AmazonEKS_CNI_Policy',\n",
    "    'AmazonEKSClusterPolicy',\n",
    "    # 'AmazonSSMManagedInstanceCore',\n",
    "]\n",
    "# Policy AmazonSSMManagedInstanceCore is not necessary, I used it for debugging, to connect to the node and run commands. \n",
    "\n",
    "for policy in policies:\n",
    "    try:\n",
    "        iam_client.attach_role_policy(\n",
    "            RoleName=role_name,\n",
    "            PolicyArn=f'arn:aws:iam::aws:policy/{policy}'\n",
    "        )\n",
    "        print(f\"Attached policy {policy} to role {role_name}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error attaching policy {policy}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8c8bd2-f80d-4a36-b648-4895429988ab",
   "metadata": {},
   "source": [
    "# Add Node Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "11c771d0-9929-4c01-8ae6-e7c0845760d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: This part may also need a wait."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "75cfa5fb-e965-4e51-bee6-8414c32a7689",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CREATING'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_groups = eks_client.list_nodegroups(clusterName=CLUSTER_NAME)\n",
    "# TODO: Make this static and put to the top, with different `diskSize` and `scalingConfig`\n",
    "node_group_name = 'gpu'\n",
    "ami_type = 'AL2_x86_64_GPU'\n",
    "if node_group_name in node_groups['nodegroups']:\n",
    "    response = eks_client.describe_nodegroup(clusterName=CLUSTER_NAME, nodegroupName=node_group_name)\n",
    "    status = response['nodegroup']['status']\n",
    "    if status == 'CREATE_FAILED':\n",
    "        print('Node group exists, but failed to create. Deleting...')\n",
    "        failed_node_group_info = response\n",
    "\n",
    "        eks_client.delete_nodegroup(clusterName=CLUSTER_NAME, nodegroupName=node_group_name)\n",
    "    \n",
    "        wait_until(eks_client.list_nodegroups, {'clusterName': CLUSTER_NAME}, lambda x: node_group_name not in x['nodegroups'])\n",
    "        node_groups = eks_client.list_nodegroups(clusterName=CLUSTER_NAME)\n",
    "    else:\n",
    "        print('Node group exists.')\n",
    "\n",
    "\n",
    "\n",
    "if not node_groups['nodegroups']:\n",
    "    print('Creating node group...')\n",
    "    response = eks_client.create_nodegroup(\n",
    "        clusterName=CLUSTER_NAME,\n",
    "        nodegroupName=node_group_name,\n",
    "        scalingConfig={\n",
    "            'desiredSize': 1,\n",
    "            'minSize': 1,\n",
    "            'maxSize': 1\n",
    "        },\n",
    "        diskSize=50,  # Size in GiB\n",
    "        subnets=[public_subnet_id],\n",
    "        nodeRole=node_role_arn,\n",
    "        amiType=ami_type,\n",
    "        instanceTypes=INSTANCE_TYPES[node_group_name],\n",
    "        labels={\n",
    "            'gpu-memory': 'true'\n",
    "        },\n",
    "        taints=[\n",
    "            {\n",
    "                'key': 'nvidia.com/gpu',\n",
    "                'value': 'true',\n",
    "                'effect': 'NO_SCHEDULE'\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "wait_until(eks_client.describe_nodegroup, {'clusterName': CLUSTER_NAME, 'nodegroupName': node_group_name}, is_node_group_active)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a69858b4-e5f2-4951-aa3d-69cd404c0e27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CREATING'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_groups = eks_client.list_nodegroups(clusterName=CLUSTER_NAME)\n",
    "# TODO: Make this static and put to the top, with different `diskSize` and `scalingConfig`\n",
    "node_group_name = 'default'\n",
    "ami_type = 'AL2_x86_64'\n",
    "if node_group_name in node_groups['nodegroups']:\n",
    "    response = eks_client.describe_nodegroup(clusterName=CLUSTER_NAME, nodegroupName=node_group_name)\n",
    "    status = response['nodegroup']['status']\n",
    "    if status == 'CREATE_FAILED':\n",
    "        print('Node group exists, but failed to create. Deleting...')\n",
    "        failed_node_group_info = response\n",
    "\n",
    "        eks_client.delete_nodegroup(clusterName=CLUSTER_NAME, nodegroupName=node_group_name)\n",
    "    \n",
    "        wait_until(eks_client.list_nodegroups, {'clusterName': CLUSTER_NAME}, lambda x: node_group_name not in x['nodegroups'])\n",
    "        node_groups = eks_client.list_nodegroups(clusterName=CLUSTER_NAME)\n",
    "    else:\n",
    "        print('Node group exists.')\n",
    "\n",
    "\n",
    "# TODO: See if lower disksize will work.\n",
    "if node_group_name not in node_groups['nodegroups']:\n",
    "    print('Creating node group...')\n",
    "    response = eks_client.create_nodegroup(\n",
    "        clusterName=CLUSTER_NAME,\n",
    "        nodegroupName=node_group_name,\n",
    "        scalingConfig={\n",
    "            'desiredSize': 2,\n",
    "            'minSize': 2,\n",
    "            'maxSize': 2\n",
    "        },\n",
    "        diskSize=1000,  # Size in GiB\n",
    "        subnets=[public_subnet_id],\n",
    "        nodeRole=node_role_arn,\n",
    "        amiType=ami_type,\n",
    "        instanceTypes=INSTANCE_TYPES[node_group_name],\n",
    "    )\n",
    "wait_until(eks_client.describe_nodegroup, {'clusterName': CLUSTER_NAME, 'nodegroupName': node_group_name}, is_node_group_active)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9df50a-ede3-4b60-af87-f7295796c94c",
   "metadata": {},
   "source": [
    "# Initialize Kubernetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "77edf8ec-0ec3-42d2-a8a6-f8dbf606364f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added new context arn:aws:eks:ca-central-1:275678099358:cluster/kubyterlab-llm to /root/.kube/config\n"
     ]
    }
   ],
   "source": [
    "!aws eks update-kubeconfig --name $CLUSTER_NAME --region $REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8ac935-b3bd-491f-9a4c-ec60bd3f3271",
   "metadata": {},
   "source": [
    "# Initialize Helm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9ec6ffe2-7458-4d1b-90dd-0b6fbbb0e921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"aws-ebs-csi-driver\" has been added to your repositories\n"
     ]
    }
   ],
   "source": [
    "!helm repo add aws-ebs-csi-driver https://kubernetes-sigs.github.io/aws-ebs-csi-driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1d3707f7-5505-4195-8666-748cda44c02a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"eks\" has been added to your repositories\n"
     ]
    }
   ],
   "source": [
    "!helm repo add eks https://aws.github.io/eks-charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "79fbbaa4-e44b-4a43-bfd1-04d3a9dcf4fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hang tight while we grab the latest from your chart repositories...\n",
      "...Successfully got an update from the \"aws-ebs-csi-driver\" chart repository\n",
      "...Successfully got an update from the \"eks\" chart repository\n",
      "Update Complete. ⎈Happy Helming!⎈\n"
     ]
    }
   ],
   "source": [
    "!helm repo update"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1566d62-4c89-4f87-84a1-5d465d8d89d4",
   "metadata": {},
   "source": [
    "# Create a Role to Install the CSI Driver and Give Permissions Using the OIDC Provider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "baafae37-f87f-47bb-a642-fc1c1c6cb5a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://oidc.eks.ca-central-1.amazonaws.com/id/EBF08E356C5BBA07E521E67C1931CB0A'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_response = eks_client.describe_cluster(name=CLUSTER_NAME)\n",
    "oidc_id = cluster_response['cluster']['identity']['oidc']['issuer'].split('/')[-1]\n",
    "oidc_url = f'https://oidc.eks.{REGION}.amazonaws.com/id/{oidc_id}'\n",
    "oidc_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "47f875e6-9a22-4615-98a9-eb43c520fd94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m2025-02-09 15:09:43 [ℹ]  will create IAM Open ID Connect provider for cluster \"kubyterlab-llm\" in \"ca-central-1\"\n",
      "\u001b[36m2025-02-09 15:09:43 [✔]  created IAM Open ID Connect provider for cluster \"kubyterlab-llm\" in \"ca-central-1\"\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!eksctl utils associate-iam-oidc-provider --cluster $CLUSTER_NAME --approve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "023b4e7f-f182-4474-a035-4027c0c75351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arn:aws:iam::275678099358:policy/AmazonEKS_EBS_CSI_Driver_Policy'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazon_eks_ebs_csi_driver_policy_name = 'AmazonEKS_EBS_CSI_Driver_Policy'\n",
    "amazon_eks_ebs_csi_driver_policy = {\n",
    "  \"Version\": \"2012-10-17\",\n",
    "  \"Statement\": [\n",
    "    {\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Action\": [\n",
    "        \"ec2:CreateSnapshot\",\n",
    "        \"ec2:AttachVolume\",\n",
    "        \"ec2:DetachVolume\",\n",
    "        \"ec2:ModifyVolume\",\n",
    "        \"ec2:DescribeAvailabilityZones\",\n",
    "        \"ec2:DescribeInstances\",\n",
    "        \"ec2:DescribeSnapshots\",\n",
    "        \"ec2:DescribeTags\",\n",
    "        \"ec2:DescribeVolumes\",\n",
    "        \"ec2:DescribeVolumesModifications\"\n",
    "      ],\n",
    "      \"Resource\": \"*\"\n",
    "    },\n",
    "    {\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Action\": [\n",
    "        \"ec2:CreateTags\"\n",
    "      ],\n",
    "      \"Resource\": [\n",
    "        \"arn:aws:ec2:*:*:volume/*\",\n",
    "        \"arn:aws:ec2:*:*:snapshot/*\"\n",
    "      ],\n",
    "      \"Condition\": {\n",
    "        \"StringEquals\": {\n",
    "          \"ec2:CreateAction\": [\n",
    "            \"CreateVolume\",\n",
    "            \"CreateSnapshot\"\n",
    "          ]\n",
    "        }\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Action\": [\n",
    "        \"ec2:DeleteTags\"\n",
    "      ],\n",
    "      \"Resource\": [\n",
    "        \"arn:aws:ec2:*:*:volume/*\",\n",
    "        \"arn:aws:ec2:*:*:snapshot/*\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Action\": [\n",
    "        \"ec2:CreateVolume\"\n",
    "      ],\n",
    "      \"Resource\": \"*\",\n",
    "      \"Condition\": {\n",
    "        \"StringLike\": {\n",
    "          \"aws:RequestTag/ebs.csi.aws.com/cluster\": \"true\"\n",
    "        }\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Action\": [\n",
    "        \"ec2:CreateVolume\"\n",
    "      ],\n",
    "      \"Resource\": \"*\",\n",
    "      \"Condition\": {\n",
    "        \"StringLike\": {\n",
    "          \"aws:RequestTag/CSIVolumeName\": \"*\"\n",
    "        }\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Action\": [\n",
    "        \"ec2:DeleteVolume\"\n",
    "      ],\n",
    "      \"Resource\": \"*\",\n",
    "      \"Condition\": {\n",
    "        \"StringLike\": {\n",
    "          \"ec2:ResourceTag/CSIVolumeName\": \"*\"\n",
    "        }\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Action\": [\n",
    "        \"ec2:DeleteVolume\"\n",
    "      ],\n",
    "      \"Resource\": \"*\",\n",
    "      \"Condition\": {\n",
    "        \"StringLike\": {\n",
    "          \"ec2:ResourceTag/ebs.csi.aws.com/cluster\": \"true\"\n",
    "        }\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Action\": [\n",
    "        \"ec2:DeleteSnapshot\"\n",
    "      ],\n",
    "      \"Resource\": \"*\",\n",
    "      \"Condition\": {\n",
    "        \"StringLike\": {\n",
    "          \"ec2:ResourceTag/CSIVolumeSnapshotName\": \"*\"\n",
    "        }\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Action\": [\n",
    "        \"ec2:DeleteSnapshot\"\n",
    "      ],\n",
    "      \"Resource\": \"*\",\n",
    "      \"Condition\": {\n",
    "        \"StringLike\": {\n",
    "          \"ec2:ResourceTag/ebs.csi.aws.com/cluster\": \"true\"\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "try:\n",
    "    response = iam_client.create_policy(\n",
    "        PolicyName=amazon_eks_ebs_csi_driver_policy_name,\n",
    "        PolicyDocument=json.dumps(amazon_eks_ebs_csi_driver_policy)\n",
    "    )\n",
    "    amazon_eks_ebs_csi_driver_policy_arn = response['Policy']['Arn']\n",
    "except iam_client.exceptions.EntityAlreadyExistsException:\n",
    "    amazon_eks_ebs_csi_driver_policy_arn = f'arn:aws:iam::{aws_account_id}:policy/{amazon_eks_ebs_csi_driver_policy_name}'\n",
    "    response = iam_client.get_policy(PolicyArn=amazon_eks_ebs_csi_driver_policy_arn)\n",
    "\n",
    "    # TODO: If policy body is not the same, update. Code to update follows.\n",
    "    # iam_client.create_policy_version(\n",
    "    #     PolicyArn=policy_arn,\n",
    "    #     PolicyDocument=policy_document_json,\n",
    "    #     SetAsDefault=True\n",
    "    # )\n",
    "\n",
    "amazon_eks_ebs_csi_driver_policy_arn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e4e0ebf6-a8ef-42f7-bc4d-ba852964f0da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Role already exists. Updating the trust policy.\n"
     ]
    }
   ],
   "source": [
    "trust_policy = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\n",
    "                \"Federated\": f\"arn:aws:iam::{aws_account_id}:oidc-provider/oidc.eks.{REGION}.amazonaws.com/id/{oidc_id}\"\n",
    "            },\n",
    "            \"Action\": \"sts:AssumeRoleWithWebIdentity\",\n",
    "            \"Condition\": {\n",
    "                \"StringEquals\": {\n",
    "                    f\"oidc.eks.{REGION}.amazonaws.com/id/{oidc_id}:aud\": \"sts.amazonaws.com\",\n",
    "                    f\"oidc.eks.{REGION}.amazonaws.com/id/{oidc_id}:sub\": \"system:serviceaccount:kube-system:ebs-csi-controller-sa\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "try:\n",
    "    create_role_response = iam_client.create_role(\n",
    "        RoleName='AmazonEKS_EBS_CSI_DriverRole',\n",
    "        \n",
    "        AssumeRolePolicyDocument=json.dumps(trust_policy),\n",
    "        Description='Role for Amazon EKS EBS CSI Driver'\n",
    "    )\n",
    "    print(f\"Role created successfully: {create_role_response['Role']['Arn']}\")\n",
    "except iam_client.exceptions.EntityAlreadyExistsException:\n",
    "    print(\"Role already exists. Updating the trust policy.\")\n",
    "    iam_client.update_assume_role_policy(\n",
    "        RoleName='AmazonEKS_EBS_CSI_DriverRole',\n",
    "        PolicyDocument=json.dumps(trust_policy)\n",
    "    )\n",
    "\n",
    "attach_policy_response = iam_client.attach_role_policy(\n",
    "    RoleName='AmazonEKS_EBS_CSI_DriverRole',\n",
    "    PolicyArn='arn:aws:iam::aws:policy/service-role/AmazonEBSCSIDriverPolicy'\n",
    ")\n",
    "\n",
    "attach_policy_response = iam_client.attach_role_policy(\n",
    "    RoleName='AmazonEKS_EBS_CSI_DriverRole',\n",
    "    PolicyArn=amazon_eks_ebs_csi_driver_policy_arn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa839d9c-00b3-41f3-ba6b-f70575889176",
   "metadata": {},
   "source": [
    "# Install CSI Driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e2451fff-9fa9-4f93-b6c1-f3b26924ac3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "serviceaccount/ebs-csi-controller-sa created\n"
     ]
    }
   ],
   "source": [
    "!kubectl create sa ebs-csi-controller-sa -n kube-system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fc410c14-878b-4af9-b960-40503d451a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "serviceaccount/ebs-csi-controller-sa annotated\n"
     ]
    }
   ],
   "source": [
    "!kubectl annotate sa ebs-csi-controller-sa -n kube-system eks.amazonaws.com/role-arn=$ebs_csi_driver_role_arn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c15647b1-29f9-445e-a115-68db3fbec3d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "serviceaccount/ebs-csi-controller-sa annotated\n"
     ]
    }
   ],
   "source": [
    "!kubectl annotate sa ebs-csi-controller-sa -n kube-system meta.helm.sh/release-namespace=kube-system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "feef5937-e243-4ae6-84ee-5be572ad1c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "serviceaccount/ebs-csi-controller-sa annotated\n"
     ]
    }
   ],
   "source": [
    "!kubectl annotate sa ebs-csi-controller-sa -n kube-system meta.helm.sh/release-name=aws-ebs-csi-driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b0b85917-e62c-42aa-8f71-7a0001265efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "serviceaccount/ebs-csi-controller-sa labeled\n"
     ]
    }
   ],
   "source": [
    "!kubectl label sa ebs-csi-controller-sa -n kube-system app.kubernetes.io/managed-by=Helm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "65c7ca0d-34d7-4b6f-93cb-f39379235bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Release \"aws-ebs-csi-driver\" does not exist. Installing it now.\n",
      "NAME: aws-ebs-csi-driver\n",
      "LAST DEPLOYED: Sun Feb  9 15:09:50 2025\n",
      "NAMESPACE: kube-system\n",
      "STATUS: deployed\n",
      "REVISION: 1\n",
      "NOTES:\n",
      "To verify that aws-ebs-csi-driver has started, run:\n",
      "\n",
      "    kubectl get pod -n kube-system -l \"app.kubernetes.io/name=aws-ebs-csi-driver,app.kubernetes.io/instance=aws-ebs-csi-driver\"\n",
      "\n",
      "[ACTION REQUIRED] Update to the EBS CSI Driver IAM Policy\n",
      "\n",
      "Due to an upcoming change in handling of IAM polices for the CreateVolume API when creating a volume from an EBS snapshot, a change to your EBS CSI Driver policy may be needed. For more information and remediation steps, see GitHub issue #2190 (https://github.com/kubernetes-sigs/aws-ebs-csi-driver/issues/2190). This change affects all versions of the EBS CSI Driver and action may be required even on clusters where the driver is not upgraded.\n"
     ]
    }
   ],
   "source": [
    "!helm upgrade --install aws-ebs-csi-driver \\\n",
    "    --namespace kube-system \\\n",
    "    aws-ebs-csi-driver/aws-ebs-csi-driver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a715b85e-d05a-44d1-9e94-e4ceb5327d8c",
   "metadata": {},
   "source": [
    "# Install AWS Ingress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "30c611ad-7fc2-41d1-b868-4b1319efd6bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: INSTALLATION FAILED: chart \"aws-alb-ingress-controller\" matching  not found in eks index. (try 'helm repo update'): no chart name found\n"
     ]
    }
   ],
   "source": [
    "!helm install aws-alb-ingress-controller eks/aws-alb-ingress-controller"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f599762-1e35-459c-a195-464eb9bd7f1c",
   "metadata": {},
   "source": [
    "### == End of Code Specific to AWS =="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bcb2b6-3c4b-4d3e-9d6c-2daf8baadfee",
   "metadata": {},
   "source": [
    "# Install Nvidia Device Plugin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2715339c-3b4c-4203-92e0-15b51e2fd75d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "daemonset.apps/nvidia-device-plugin-daemonset unchanged\n"
     ]
    }
   ],
   "source": [
    "!kubectl apply -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/refs/heads/main/deployments/static/nvidia-device-plugin.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5a0d46-7a29-4c21-80d0-1ba0f5f9b5fd",
   "metadata": {},
   "source": [
    "# Apply Helm Charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3b606826-23b1-423b-8a87-b078d28afda9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'vol-0848b8a532803d785'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "volume_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e803a8bf-61c4-422f-8ed9-8c9f04a46916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: INSTALLATION FAILED: cannot re-use a name that is still in use\n"
     ]
    }
   ],
   "source": [
    "!helm install storage /helm/storage/ --set volumeID=$volume_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b905deee-a92e-4bb3-b23c-789c090924a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Release \"lb\" has been upgraded. Happy Helming!\n",
      "NAME: lb\n",
      "LAST DEPLOYED: Sun Feb  9 15:55:13 2025\n",
      "NAMESPACE: default\n",
      "STATUS: deployed\n",
      "REVISION: 4\n",
      "TEST SUITE: None\n"
     ]
    }
   ],
   "source": [
    "!helm upgrade --install lb /helm/lb/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "373f30d2-a911-47ac-99ea-9f402c13db23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Release \"fastapi\" has been upgraded. Happy Helming!\n",
      "NAME: fastapi\n",
      "LAST DEPLOYED: Sun Feb  9 15:55:16 2025\n",
      "NAMESPACE: default\n",
      "STATUS: deployed\n",
      "REVISION: 3\n",
      "TEST SUITE: None\n"
     ]
    }
   ],
   "source": [
    "!helm upgrade --install fastapi /helm/eberron-agent-server/ --set awsAccountId=$aws_account_id --set region=$REGION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97f5d85-1064-4d39-b205-71fddb658689",
   "metadata": {},
   "source": [
    "# Get URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6ec37f9a-6ffd-4e4e-b9a2-f0955ae0423a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fastapi-677965765c-nbvnq'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wait_until(get_one_running_pod, {}, lambda x: x is not None, timeout=8 * 60)\n",
    "pod_name = get_one_running_pod()\n",
    "assert pod_name is not None\n",
    "pod_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3cee78c8-55c3-43d9-bd54-258451b2520d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('add21ab1dbda64e8b9e3616e05932aef-798754673.ca-central-1.elb.amazonaws.com',\n",
       " '80')"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = !kubectl get service lb --no-headers\n",
    "fields = re.split(r'\\s+', output[0])\n",
    "external_url = fields[3]\n",
    "port = fields[4].split('/')[0].split(':')[0]\n",
    "external_url, port"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8bf520ca-59d0-427f-8251-bc940cad8e46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://add21ab1dbda64e8b9e3616e05932aef-798754673.ca-central-1.elb.amazonaws.com:80/'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = f'http://{external_url}:{port}/'\n",
    "url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c7f019-64b1-4b2d-a343-b536fca16633",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "438b3f0b-0cc8-4aad-b796-5e7bb32fd110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Defaulted container \"kubyterlab-llm\" out of: kubyterlab-llm, init-container (init)\n",
    "# True\n",
    "# 1\n",
    "!kubectl exec $pod_name bash -- python3 -c 'import torch; print(torch.cuda.is_available()); print(torch.cuda.device_count())'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6bfde574-66a3-4a21-b26d-0cab8eac0893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory.total [MiB], memory.used [MiB], memory.free [MiB]\n",
      "15360 MiB, 4087 MiB, 11009 MiB\n"
     ]
    }
   ],
   "source": [
    "!kubectl exec $pod_name bash -- bash -c 'nvidia-smi --query-gpu=memory.total,memory.used,memory.free --format=csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a3c0a9bf-9d43-4149-ac2c-229059865aaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The languages in Eberron include, but are not limited to:\n",
       "\n",
       "1. Abyssal: Common tongue of all fiends, also known as \"Khyber’s Speech.\"\n",
       "2. Argon: Spoken by barbarians of Argonnessen.\n",
       "3. Auran: Spoken by air-based creatures like elves.\n",
       "4. Common: The language of the Five Nations and the language of trade in Khorvaire, known by most of its people.\n",
       "5. Daan: Spoken by formians, lawful outsiders, and other daelkyr aberrations.\n",
       "6. Daelkyr: Spoken by the daelkyr, mind flayers, and other creatures of Xoriat.\n",
       "7. Draconic: Spoken by kobolds, troglodytes, lizardfolk, dragons, and other draconic creatures.\n",
       "8. Druidic: Spoken by druids (only).\n",
       "9. Dwarven: Spoken by dwarves.\n",
       "10. Elven: Spoken by elves and drow.\n",
       "11. Giant: Spoken by ogres, giants, and drow.\n",
       "12. Goblin: Spoken by goblins, hobgoblins, and bugbears.\n",
       "13. Halfling: Spoken by halflings.\n",
       "14. Ignan: Spoken by fire-based creatures.\n",
       "15. Infernal: Spoken by devils of Shavarath.\n",
       "16. Irial: Spoken by ravids, positive energy users.\n",
       "17. Kythric: Spoken by slaadi, chaotic outsiders.\n",
       "18. Mabran: Spoken by nightshades, shadows, and draconic creatures of Mabar.\n",
       "19. Ore: Spoken by orcs.\n",
       "20. Quori: Spoken by the Inspired, kalashtar.\n",
       "21. Riedran: Spoken by the lower classes of Sarlona.\n",
       "22. Risian: Spoken by ice-based creatures.\n",
       "23. Sylvan: Spoken by dryads, eladrins, and creatures of Thelanis.\n",
       "24. Syranian: Spoken by angels of Syrania.\n",
       "25. Terran: Spoken by xorns and other earth-based creatures.\n",
       "26. Undercommon: Spoken by chokers and underground Daelkyr denizens.\n",
       "\n",
       "This list is not exhaustive, and there may be other languages in Eberron. The choice of languages can be adjusted with the DM's approval."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "from IPython.display import clear_output, display, HTML, Markdown\n",
    "\n",
    "response = requests.post(f\"{url}/respond\", \n",
    "                         headers={\"Content-Type\": \"application/json\", \n",
    "                                  \"Accept\": \"text/event-stream\"}, \n",
    "                         json={\"content\": \"What are the languages in Eberron?\"}, \n",
    "                         stream=True)\n",
    "md = \"\"\n",
    "for chunk in response:\n",
    "    md += chunk.decode(\"utf-8\")\n",
    "    clear_output()\n",
    "    display(Markdown(md.replace('\\\\n', '\\n')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba23253-67dc-49a3-b87d-253967ec580b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0e4ae1ff-9361-4709-8343-2b2310999403",
   "metadata": {},
   "source": [
    "# == End of Procedure =="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54d53ff-124d-409d-95f3-6985f7382654",
   "metadata": {},
   "source": [
    "## == Do Not Continue: rest of the code is only for reference for troubleshooting =="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bd4f0a-fa1d-4578-89d6-84b9c30c2179",
   "metadata": {},
   "source": [
    "# Troubleshoot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2af3512-4723-4393-8d3a-56bdc6fa64f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# curl -X POST -H \"Content-Type: application/json\" -H \"Accept: text/event-stream\" -d '{\"key\":\"value\"}' http://your-api-endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f10cb1-1b0f-4212-a689-052928a4924d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that for the following to work, you need to attach the policy AmazonSSMManagedInstanceCore, in the relevant cell above, during the deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "44b4b8fb-d7fb-4c9c-87ba-777bf78f1f7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'9528f9bd-ba96-49a8-b6f9-45df5433a905'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = !aws ssm send-command \\\n",
    "    --document-name \"AWS-RunShellScript\" \\\n",
    "    --targets \"Key=instanceIds,Values=i-02f3c1f8a118eb352\" \\\n",
    "    --parameters 'commands=[\"nvidia-smi --query-gpu=memory.total,memory.used,memory.free --format=csv\"]' \\\n",
    "    --region $REGION\n",
    "command_id = json.loads(''.join(output))['Command']['CommandId']\n",
    "command_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a45b62bf-644f-41ce-a4bf-f745311de116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory.total [MiB], memory.used [MiB], memory.free [MiB]\n",
      "15360 MiB, 1 MiB, 15095 MiB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output = !aws ssm list-command-invocations \\\n",
    "    --command-id $command_id \\\n",
    "    --details\n",
    "output = json.loads(''.join(output))['CommandInvocations'][0]['CommandPlugins'][0]['Output']\n",
    "for line in output.split('\\n'):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0480a8c3-9aad-4a76-9f62-20de23589501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAST SEEN   TYPE      REASON                    OBJECT                                             MESSAGE\n",
      "28m         Normal    Starting                  node/ip-10-0-1-102.ca-central-1.compute.internal   \n",
      "28m         Normal    Starting                  node/ip-10-0-1-252.ca-central-1.compute.internal   \n",
      "29m         Normal    Starting                  node/ip-10-0-1-228.ca-central-1.compute.internal   \n",
      "29m         Normal    Starting                  node/ip-10-0-1-228.ca-central-1.compute.internal   Starting kubelet.\n",
      "29m         Warning   CgroupV1                  node/ip-10-0-1-228.ca-central-1.compute.internal   Cgroup v1 support is in maintenance mode, please migrate to Cgroup v2.\n",
      "29m         Warning   InvalidDiskCapacity       node/ip-10-0-1-228.ca-central-1.compute.internal   invalid capacity 0 on image filesystem\n",
      "29m         Normal    NodeAllocatableEnforced   node/ip-10-0-1-228.ca-central-1.compute.internal   Updated Node Allocatable limit across pods\n",
      "29m         Normal    NodeHasSufficientPID      node/ip-10-0-1-228.ca-central-1.compute.internal   Node ip-10-0-1-228.ca-central-1.compute.internal status is now: NodeHasSufficientPID\n",
      "29m         Normal    NodeHasNoDiskPressure     node/ip-10-0-1-228.ca-central-1.compute.internal   Node ip-10-0-1-228.ca-central-1.compute.internal status is now: NodeHasNoDiskPressure\n",
      "29m         Normal    NodeHasSufficientMemory   node/ip-10-0-1-228.ca-central-1.compute.internal   Node ip-10-0-1-228.ca-central-1.compute.internal status is now: NodeHasSufficientMemory\n",
      "29m         Normal    Synced                    node/ip-10-0-1-228.ca-central-1.compute.internal   Node synced successfully\n",
      "29m         Normal    RegisteredNode            node/ip-10-0-1-228.ca-central-1.compute.internal   Node ip-10-0-1-228.ca-central-1.compute.internal event: Registered Node ip-10-0-1-228.ca-central-1.compute.internal in Controller\n",
      "29m         Normal    NodeReady                 node/ip-10-0-1-228.ca-central-1.compute.internal   Node ip-10-0-1-228.ca-central-1.compute.internal status is now: NodeReady\n",
      "28m         Warning   CgroupV1                  node/ip-10-0-1-102.ca-central-1.compute.internal   Cgroup v1 support is in maintenance mode, please migrate to Cgroup v2.\n",
      "28m         Normal    NodeHasSufficientPID      node/ip-10-0-1-102.ca-central-1.compute.internal   Node ip-10-0-1-102.ca-central-1.compute.internal status is now: NodeHasSufficientPID\n",
      "28m         Normal    NodeHasNoDiskPressure     node/ip-10-0-1-102.ca-central-1.compute.internal   Node ip-10-0-1-102.ca-central-1.compute.internal status is now: NodeHasNoDiskPressure\n",
      "28m         Normal    NodeHasSufficientMemory   node/ip-10-0-1-102.ca-central-1.compute.internal   Node ip-10-0-1-102.ca-central-1.compute.internal status is now: NodeHasSufficientMemory\n",
      "28m         Normal    NodeAllocatableEnforced   node/ip-10-0-1-102.ca-central-1.compute.internal   Updated Node Allocatable limit across pods\n",
      "28m         Warning   InvalidDiskCapacity       node/ip-10-0-1-102.ca-central-1.compute.internal   invalid capacity 0 on image filesystem\n",
      "28m         Normal    Starting                  node/ip-10-0-1-102.ca-central-1.compute.internal   Starting kubelet.\n",
      "28m         Warning   InvalidDiskCapacity       node/ip-10-0-1-252.ca-central-1.compute.internal   invalid capacity 0 on image filesystem\n",
      "28m         Normal    Synced                    node/ip-10-0-1-102.ca-central-1.compute.internal   Node synced successfully\n",
      "28m         Normal    Starting                  node/ip-10-0-1-252.ca-central-1.compute.internal   Starting kubelet.\n",
      "28m         Warning   CgroupV1                  node/ip-10-0-1-252.ca-central-1.compute.internal   Cgroup v1 support is in maintenance mode, please migrate to Cgroup v2.\n",
      "28m         Normal    NodeHasSufficientPID      node/ip-10-0-1-252.ca-central-1.compute.internal   Node ip-10-0-1-252.ca-central-1.compute.internal status is now: NodeHasSufficientPID\n",
      "28m         Normal    NodeAllocatableEnforced   node/ip-10-0-1-252.ca-central-1.compute.internal   Updated Node Allocatable limit across pods\n",
      "28m         Normal    Synced                    node/ip-10-0-1-252.ca-central-1.compute.internal   Node synced successfully\n",
      "28m         Normal    NodeHasNoDiskPressure     node/ip-10-0-1-252.ca-central-1.compute.internal   Node ip-10-0-1-252.ca-central-1.compute.internal status is now: NodeHasNoDiskPressure\n",
      "28m         Normal    NodeHasSufficientMemory   node/ip-10-0-1-252.ca-central-1.compute.internal   Node ip-10-0-1-252.ca-central-1.compute.internal status is now: NodeHasSufficientMemory\n",
      "28m         Normal    RegisteredNode            node/ip-10-0-1-252.ca-central-1.compute.internal   Node ip-10-0-1-252.ca-central-1.compute.internal event: Registered Node ip-10-0-1-252.ca-central-1.compute.internal in Controller\n",
      "28m         Normal    RegisteredNode            node/ip-10-0-1-102.ca-central-1.compute.internal   Node ip-10-0-1-102.ca-central-1.compute.internal event: Registered Node ip-10-0-1-102.ca-central-1.compute.internal in Controller\n",
      "28m         Normal    Scheduled                 pod/fastapi-698c4dc749-l5qsk                       Successfully assigned default/fastapi-698c4dc749-l5qsk to ip-10-0-1-228.ca-central-1.compute.internal\n",
      "28m         Normal    SuccessfulCreate          replicaset/fastapi-698c4dc749                      Created pod: fastapi-698c4dc749-l5qsk\n",
      "28m         Normal    ScalingReplicaSet         deployment/fastapi                                 Scaled up replica set fastapi-698c4dc749 to 1\n",
      "28m         Normal    NodeReady                 node/ip-10-0-1-102.ca-central-1.compute.internal   Node ip-10-0-1-102.ca-central-1.compute.internal status is now: NodeReady\n",
      "28m         Normal    NodeReady                 node/ip-10-0-1-252.ca-central-1.compute.internal   Node ip-10-0-1-252.ca-central-1.compute.internal status is now: NodeReady\n",
      "28m         Normal    SuccessfulAttachVolume    pod/fastapi-698c4dc749-l5qsk                       AttachVolume.Attach succeeded for volume \"pv-llm\"\n",
      "27m         Normal    Pulling                   pod/fastapi-698c4dc749-l5qsk                       Pulling image \"275678099358.dkr.ecr.ca-central-1.amazonaws.com/multi-agent/eberron-agent-server:25.02\"\n",
      "23m         Normal    Pulled                    pod/fastapi-698c4dc749-l5qsk                       Successfully pulled image \"275678099358.dkr.ecr.ca-central-1.amazonaws.com/multi-agent/eberron-agent-server:25.02\" in 4m28.297s (4m28.297s including waiting). Image size: 14071664304 bytes.\n",
      "23m         Normal    Created                   pod/fastapi-698c4dc749-l5qsk                       Created container fastapi\n",
      "23m         Normal    Started                   pod/fastapi-698c4dc749-l5qsk                       Started container fastapi\n",
      "9m9s        Normal    EnsuringLoadBalancer      service/lb                                         Ensuring load balancer\n",
      "9m8s        Normal    EnsuredLoadBalancer       service/lb                                         Ensured load balancer\n"
     ]
    }
   ],
   "source": [
    "!kubectl get events --sort-by='.lastTimestamp'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39f876b-ea67-4dd5-a9ee-92af8f218ce9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a8992595-dd14-4067-8607-3596e7fa746c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                  READY   STATUS    RESTARTS   AGE\n",
      "ebs-csi-controller-659976db95-2rlcv   5/5     Running   0          6m7s\n",
      "ebs-csi-controller-659976db95-dmq9b   5/5     Running   0          6m7s\n",
      "ebs-csi-node-72vkb                    3/3     Running   0          6m7s\n",
      "ebs-csi-node-k4rs9                    3/3     Running   0          6m7s\n",
      "ebs-csi-node-z8pmp                    3/3     Running   0          6m7s\n"
     ]
    }
   ],
   "source": [
    "!kubectl get pod -n kube-system -l \"app.kubernetes.io/name=aws-ebs-csi-driver,app.kubernetes.io/instance=aws-ebs-csi-driver\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3b9460f8-e4d4-4247-908f-22706f2d876d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                          GPU\n",
      "ip-10-0-1-102.ca-central-1.compute.internal   <none>\n",
      "ip-10-0-1-228.ca-central-1.compute.internal   1\n",
      "ip-10-0-1-252.ca-central-1.compute.internal   <none>\n"
     ]
    }
   ],
   "source": [
    "# NAME                                          GPU\n",
    "# ip-10-0-1-129.ca-central-1.compute.internal   <none>\n",
    "# ip-10-0-1-223.ca-central-1.compute.internal   <none>\n",
    "# ip-10-0-1-8.ca-central-1.compute.internal     1z\n",
    "!kubectl get nodes \"-o=custom-columns=NAME:.metadata.name,GPU:.status.allocatable.nvidia\\.com/gpu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5d4e2db7-eec8-484d-af50-ff23a339c8f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                          TAINTS\n",
      "ip-10-0-1-102.ca-central-1.compute.internal   <none>\n",
      "ip-10-0-1-228.ca-central-1.compute.internal   [map[effect:NoSchedule key:nvidia.com/gpu value:true]]\n",
      "ip-10-0-1-252.ca-central-1.compute.internal   <none>\n"
     ]
    }
   ],
   "source": [
    "# NAME                                          TAINTS\n",
    "# ip-10-0-1-107.ca-central-1.compute.internal   <none>\n",
    "# ip-10-0-1-175.ca-central-1.compute.internal   [map[effect:NoSchedule key:nvidia.com/gpu value:true]]\n",
    "# ip-10-0-1-90.ca-central-1.compute.internal    <none>\n",
    "!kubectl get nodes -o custom-columns=NAME:.metadata.name,TAINTS:.spec.taints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "35f0a916-0089-4030-97aa-a074fe340403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME         TYPE           CLUSTER-IP       EXTERNAL-IP                                                                 PORT(S)        AGE\n",
      "fastapi      ClusterIP      172.20.172.247   <none>                                                                      80/TCP         6m9s\n",
      "kubernetes   ClusterIP      172.20.0.1       <none>                                                                      443/TCP        9m58s\n",
      "lb           LoadBalancer   172.20.209.126   add21ab1dbda64e8b9e3616e05932aef-798754673.ca-central-1.elb.amazonaws.com   80:30623/TCP   6m10s\n"
     ]
    }
   ],
   "source": [
    "# NAME                     TYPE           CLUSTER-IP     EXTERNAL-IP                                                                  PORT(S)          AGE\n",
    "# kubernetes               ClusterIP      172.20.0.1     <none>                                                                       443/TCP          37m\n",
    "# kubyterlab-llm-service   LoadBalancer   172.20.129.8   a4740e3e56bfe40ac81121bd46071903-1377611187.ca-central-1.elb.amazonaws.com   8888:31434/TCP   13s\n",
    "!kubectl get service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fde9f7f4-b4cc-4834-9ccc-e69f70ffbbad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME      READY   UP-TO-DATE   AVAILABLE   AGE\n",
      "fastapi   1/1     1            1           6m11s\n"
     ]
    }
   ],
   "source": [
    "# NAME               READY   UP-TO-DATE   AVAILABLE   AGE\n",
    "# kubyterlab-llm-pod   1/1     1            1           31m\n",
    "!kubectl get deployments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9c927c44-b74d-4fa4-839b-775a50c1c8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kubectl describe deployment kubyterlab-llm-pod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3d2ac1bd-3d25-4fa8-b2dd-fb7fa10e42d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                       READY   STATUS    RESTARTS   AGE\n",
      "fastapi-698c4dc749-l5qsk   1/1     Running   0          6m15s\n"
     ]
    }
   ],
   "source": [
    "!kubectl get pods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0a2da721-c219-436c-8ba5-11d905c507d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME      STATUS   VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   VOLUMEATTRIBUTESCLASS   AGE\n",
      "pvc-llm   Bound    pv-llm   500Gi      RWO            manual         <unset>                 6m20s\n"
     ]
    }
   ],
   "source": [
    "!kubectl get pvc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "23b517a1-9d50-47a0-bcc8-e9a0f787c216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME     CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM             STORAGECLASS   VOLUMEATTRIBUTESCLASS   REASON   AGE\n",
      "pv-llm   500Gi      RWO            Retain           Bound    default/pvc-llm   manual         <unset>                          67m\n"
     ]
    }
   ],
   "source": [
    "!kubectl get pv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6a28c8a8-575e-4182-b448-4c5498d74666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========\n",
      "== CUDA ==\n",
      "==========\n",
      "\n",
      "CUDA Version 12.6.1\n",
      "\n",
      "Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n",
      "\n",
      "This container image and its contents are governed by the NVIDIA Deep Learning Container License.\n",
      "By pulling and using the container, you accept the terms and conditions of this license:\n",
      "https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license\n",
      "\n",
      "A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.\n",
      "\n",
      "2025-02-09 15:14:53.245880: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-09 15:14:53.261270: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1739114093.280822       1 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1739114093.286809       1 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-09 15:14:53.306200: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/app/models.py:41: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=embedding_model_path,\n"
     ]
    }
   ],
   "source": [
    "!kubectl logs $pod_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6c6f80b2-a78a-45e6-bd1e-0fc3bd307b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:             fastapi-698c4dc749-l5qsk\n",
      "Namespace:        default\n",
      "Priority:         0\n",
      "Service Account:  default\n",
      "Node:             ip-10-0-1-228.ca-central-1.compute.internal/10.0.1.228\n",
      "Start Time:       Sun, 09 Feb 2025 15:09:55 +0000\n",
      "Labels:           app=fastapi\n",
      "                  pod-template-hash=698c4dc749\n",
      "Annotations:      <none>\n",
      "Status:           Running\n",
      "IP:               10.0.1.238\n",
      "IPs:\n",
      "  IP:           10.0.1.238\n",
      "Controlled By:  ReplicaSet/fastapi-698c4dc749\n",
      "Containers:\n",
      "  fastapi:\n",
      "    Container ID:   containerd://a15615861206a39f38823254d93970ca29766b92277c793c2856fb57bc0cfd11\n",
      "    Image:          275678099358.dkr.ecr.ca-central-1.amazonaws.com/multi-agent/eberron-agent-server:25.02\n",
      "    Image ID:       275678099358.dkr.ecr.ca-central-1.amazonaws.com/multi-agent/eberron-agent-server@sha256:0a7ec392240e2f7a4ba9ce05b279df68209e085074bbe4fb8f465e047156b2ff\n",
      "    Port:           80/TCP\n",
      "    Host Port:      0/TCP\n",
      "    State:          Running\n",
      "      Started:      Sun, 09 Feb 2025 15:14:49 +0000\n",
      "    Ready:          True\n",
      "    Restart Count:  0\n",
      "    Environment:\n",
      "      ARTEFACT_VERSION:        04\n",
      "      ARTEFACT_ROOT_FOLDER:    /jupyterlab/artefacts\n",
      "      HF_HOME:                 /jupyterlab/models/hf\n",
      "      MODEL_NAME:              Mistral-7B-Instruct-v0.3\n",
      "      MODEL_ORG:               mistralai\n",
      "      COMMIT_HASH:             e0bc86c23ce5aae1db576c8cca6f06f1f73af2db\n",
      "      TOKENIZERS_PARALLELISM:  true\n",
      "      TRANSFORMERS_OFFLINE:    1\n",
      "      HF_DATASETS_OFFLINE:     1\n",
      "    Mounts:\n",
      "      /jupyterlab from pvc-llm (rw)\n",
      "      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-klc6j (ro)\n",
      "Conditions:\n",
      "  Type                        Status\n",
      "  PodReadyToStartContainers   True \n",
      "  Initialized                 True \n",
      "  Ready                       True \n",
      "  ContainersReady             True \n",
      "  PodScheduled                True \n",
      "Volumes:\n",
      "  pvc-llm:\n",
      "    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)\n",
      "    ClaimName:  pvc-llm\n",
      "    ReadOnly:   false\n",
      "  kube-api-access-klc6j:\n",
      "    Type:                    Projected (a volume that contains injected data from multiple sources)\n",
      "    TokenExpirationSeconds:  3607\n",
      "    ConfigMapName:           kube-root-ca.crt\n",
      "    ConfigMapOptional:       <nil>\n",
      "    DownwardAPI:             true\n",
      "QoS Class:                   BestEffort\n",
      "Node-Selectors:              <none>\n",
      "Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n",
      "                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\n",
      "                             nvidia.com/gpu:NoSchedule op=Exists\n",
      "Events:\n",
      "  Type    Reason                  Age   From                     Message\n",
      "  ----    ------                  ----  ----                     -------\n",
      "  Normal  Scheduled               21m   default-scheduler        Successfully assigned default/fastapi-698c4dc749-l5qsk to ip-10-0-1-228.ca-central-1.compute.internal\n",
      "  Normal  SuccessfulAttachVolume  21m   attachdetach-controller  AttachVolume.Attach succeeded for volume \"pv-llm\"\n",
      "  Normal  Pulling                 21m   kubelet                  Pulling image \"275678099358.dkr.ecr.ca-central-1.amazonaws.com/multi-agent/eberron-agent-server:25.02\"\n",
      "  Normal  Pulled                  16m   kubelet                  Successfully pulled image \"275678099358.dkr.ecr.ca-central-1.amazonaws.com/multi-agent/eberron-agent-server:25.02\" in 4m28.297s (4m28.297s including waiting). Image size: 14071664304 bytes.\n",
      "  Normal  Created                 16m   kubelet                  Created container fastapi\n",
      "  Normal  Started                 16m   kubelet                  Started container fastapi\n"
     ]
    }
   ],
   "source": [
    "!kubectl describe pod $pod_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad2f7fa-c856-4866-96bb-54eefb245b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "attachdetach-controller  AttachVolume.Attach failed for volume \"pv-llm\" : rpc error: code = Internal desc = Could not attach volume \"volume_id\" to node \"i-055aa2853ecdeac3b\": could not attach volume \"volume_id\" to node \"i-055aa2853ecdeac3b\": operation error EC2: AttachVolume, https response error StatusCode: 400, RequestID: a7250de3-c285-4963-84eb-5c46b28c2b96, api error InvalidParameterValue: The volume ID 'volume_id' is malformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6fbb74-4125-4ad3-96b4-f7117b82e1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl exec $pod_name bash -- nvidia-smi --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e4308b32-7f70-4861-81f6-7bc0702cbc87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulted container \"kubyterlab-llm\" out of: kubyterlab-llm, init-container (init)\n",
      "Mon Jan 13 22:15:38 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       On  |   00000000:00:1E.0 Off |                    0 |\n",
      "| N/A   25C    P8             11W /   70W |       1MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!kubectl exec $pod_name bash -- nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5a68c2d5-dc22-4529-9e8f-a7935f360340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulted container \"kubyterlab-llm\" out of: kubyterlab-llm, init-container (init)\n",
      "GPU 0: Tesla T4 (UUID: GPU-fe1acad6-41f9-c441-44ed-d4ead6db6dc2)\n"
     ]
    }
   ],
   "source": [
    "!kubectl exec $pod_name bash -- nvidia-smi -L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8d229310-97eb-4788-bf0e-22203dffe078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulted container \"kubyterlab-llm\" out of: kubyterlab-llm, init-container (init)\n",
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2024 NVIDIA Corporation\n",
      "Built on Wed_Aug_14_10:10:22_PDT_2024\n",
      "Cuda compilation tools, release 12.6, V12.6.68\n",
      "Build cuda_12.6.r12.6/compiler.34714021_0\n"
     ]
    }
   ],
   "source": [
    "!kubectl exec $pod_name bash -- nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722a4aa7-a0c3-462e-9e37-60230261fcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl describe pod $pod_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "116fd849-a569-472e-93d6-23624e93c7e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                   READY   STATUS    RESTARTS   AGE\n",
      "aws-node-cvzp2                         2/2     Running   0          4m59s\n",
      "aws-node-kmnjj                         2/2     Running   0          4m57s\n",
      "aws-node-zlmn8                         2/2     Running   0          6m35s\n",
      "coredns-749d5dbdd9-cdd8v               1/1     Running   0          8m29s\n",
      "coredns-749d5dbdd9-xtrc8               1/1     Running   0          8m29s\n",
      "ebs-csi-controller-59b6797bf-fhcph     5/5     Running   0          4m6s\n",
      "ebs-csi-controller-59b6797bf-w9mns     5/5     Running   0          4m6s\n",
      "ebs-csi-node-mp9sr                     3/3     Running   0          4m6s\n",
      "ebs-csi-node-mqt55                     3/3     Running   0          4m6s\n",
      "ebs-csi-node-q8dfl                     3/3     Running   0          4m6s\n",
      "kube-proxy-ctjcd                       1/1     Running   0          6m35s\n",
      "kube-proxy-nv8tx                       1/1     Running   0          4m57s\n",
      "kube-proxy-qmwmh                       1/1     Running   0          4m59s\n",
      "nvidia-device-plugin-daemonset-4b76p   1/1     Running   0          4m4s\n",
      "nvidia-device-plugin-daemonset-ltcdg   1/1     Running   0          4m4s\n",
      "nvidia-device-plugin-daemonset-wv2rd   1/1     Running   0          4m4s\n"
     ]
    }
   ],
   "source": [
    "!kubectl get pod -n kube-system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "90495808-55bb-4d5b-84e8-c00ca25250b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:          pvc-llm\n",
      "Namespace:     default\n",
      "StorageClass:  manual\n",
      "Status:        Bound\n",
      "Volume:        pv-llm\n",
      "Labels:        <none>\n",
      "Annotations:   pv.kubernetes.io/bind-completed: yes\n",
      "               pv.kubernetes.io/bound-by-controller: yes\n",
      "Finalizers:    [kubernetes.io/pvc-protection]\n",
      "Capacity:      10Gi\n",
      "Access Modes:  RWO\n",
      "VolumeMode:    Filesystem\n",
      "Used By:       kubyterlab-llm-pod-67f5cf95dc-s2mrn\n",
      "Events:        <none>\n"
     ]
    }
   ],
   "source": [
    "!kubectl describe pvc pvc-llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "da047557-f797-4492-b7f6-1cdcb16a3283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:            pv-llm\n",
      "Labels:          <none>\n",
      "Annotations:     pv.kubernetes.io/bound-by-controller: yes\n",
      "Finalizers:      [kubernetes.io/pv-protection]\n",
      "StorageClass:    manual\n",
      "Status:          Bound\n",
      "Claim:           default/pvc-llm\n",
      "Reclaim Policy:  Retain\n",
      "Access Modes:    RWO\n",
      "VolumeMode:      Filesystem\n",
      "Capacity:        10Gi\n",
      "Node Affinity:   <none>\n",
      "Message:         \n",
      "Source:\n",
      "    Type:       AWSElasticBlockStore (a Persistent Disk resource in AWS)\n",
      "    VolumeID:   vol-04ce08a2c05b8e1da\n",
      "    FSType:     ext4\n",
      "    Partition:  0\n",
      "    ReadOnly:   false\n",
      "Events:         <none>\n"
     ]
    }
   ],
   "source": [
    "!kubectl describe pv pv-llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "80827487-9836-450d-806d-5f56b00ab821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:                   kubyterlab-llm-pod\n",
      "Namespace:              default\n",
      "CreationTimestamp:      Mon, 02 Dec 2024 20:05:56 +0000\n",
      "Labels:                 <none>\n",
      "Annotations:            deployment.kubernetes.io/revision: 1\n",
      "Selector:               app=kubyterlab-llm\n",
      "Replicas:               1 desired | 1 updated | 1 total | 0 available | 1 unavailable\n",
      "StrategyType:           RollingUpdate\n",
      "MinReadySeconds:        0\n",
      "RollingUpdateStrategy:  25% max unavailable, 25% max surge\n",
      "Pod Template:\n",
      "  Labels:  app=kubyterlab-llm\n",
      "  Containers:\n",
      "   kubyterlab-llm:\n",
      "    Image:      kubyterlab:24.10\n",
      "    Port:       8888/TCP\n",
      "    Host Port:  0/TCP\n",
      "    Limits:\n",
      "      cpu:             1\n",
      "      memory:          8Gi\n",
      "      nvidia.com/gpu:  1\n",
      "    Requests:\n",
      "      cpu:             1\n",
      "      memory:          8Gi\n",
      "      nvidia.com/gpu:  1\n",
      "    Environment:\n",
      "      JUPYTERLAB_SETTINGS_DIR:  /jupyterlab/config\n",
      "      MISTRAL_MODEL:            /models/mistral\n",
      "    Mounts:\n",
      "      /corpus from pv-llm (rw)\n",
      "      /jupyterlab from pv-llm (rw)\n",
      "      /models from pv-llm (rw)\n",
      "  Volumes:\n",
      "   pv-llm:\n",
      "    Type:          PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)\n",
      "    ClaimName:     pvc-llm\n",
      "    ReadOnly:      false\n",
      "  Node-Selectors:  <none>\n",
      "  Tolerations:     nvidia.com/gpu:NoSchedule op=Exists\n",
      "Conditions:\n",
      "  Type           Status  Reason\n",
      "  ----           ------  ------\n",
      "  Available      False   MinimumReplicasUnavailable\n",
      "  Progressing    True    ReplicaSetUpdated\n",
      "OldReplicaSets:  <none>\n",
      "NewReplicaSet:   kubyterlab-llm-pod-67f5cf95dc (1/1 replicas created)\n",
      "Events:\n",
      "  Type    Reason             Age    From                   Message\n",
      "  ----    ------             ----   ----                   -------\n",
      "  Normal  ScalingReplicaSet  5m46s  deployment-controller  Scaled up replica set kubyterlab-llm-pod-67f5cf95dc to 1\n"
     ]
    }
   ],
   "source": [
    "!kubectl describe deployment kubyterlab-llm-pod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ce402031-be81-4578-8877-15bcd40ad8e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                          STATUS   ROLES    AGE   VERSION\n",
      "ip-10-0-1-100.ca-central-1.compute.internal   Ready    <none>   47m   v1.30.7-eks-59bf375\n",
      "ip-10-0-1-25.ca-central-1.compute.internal    Ready    <none>   47m   v1.30.7-eks-59bf375\n",
      "ip-10-0-1-50.ca-central-1.compute.internal    Ready    <none>   48m   v1.30.7-eks-59bf375\n"
     ]
    }
   ],
   "source": [
    "!kubectl get nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d0bb4314-1002-45c7-b0c2-4d706b5d533f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME               DATA   AGE\n",
      "kube-root-ca.crt   1      50m\n"
     ]
    }
   ],
   "source": [
    "!kubectl get configmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6d65a766-6f17-413c-a7f5-9f8e07fd0a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                     READY   STATUS    RESTARTS   AGE\n",
      "pod/ebs-csi-controller-59b6797bf-vfvs2   5/5     Running   0          47m\n",
      "pod/ebs-csi-controller-59b6797bf-xt7fv   5/5     Running   0          47m\n",
      "pod/ebs-csi-node-dwzz7                   3/3     Running   0          47m\n",
      "pod/ebs-csi-node-gplqp                   3/3     Running   0          47m\n",
      "pod/ebs-csi-node-l5nh6                   3/3     Running   0          47m\n",
      "\n",
      "NAME                          DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE\n",
      "daemonset.apps/ebs-csi-node   3         3         3       3            3           kubernetes.io/os=linux   47m\n",
      "\n",
      "NAME                                 READY   UP-TO-DATE   AVAILABLE   AGE\n",
      "deployment.apps/ebs-csi-controller   2/2     2            2           47m\n",
      "\n",
      "NAME                                           DESIRED   CURRENT   READY   AGE\n",
      "replicaset.apps/ebs-csi-controller-59b6797bf   2         2         2       47m\n"
     ]
    }
   ],
   "source": [
    "!kubectl get all -l app.kubernetes.io/name=aws-ebs-csi-driver -n kube-system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3bf4b6bf-4a67-48e4-ba16-8207e90b7fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"OpenIDConnectProviderList\": [\n",
      "        {\n",
      "            \"Arn\": \"arn:aws:iam::275678099358:oidc-provider/oidc.eks.ca-central-1.amazonaws.com/id/8D7619520212428BD59C46B20BF19338\"\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!aws iam list-open-id-connect-providers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "65c00bf0-9326-417c-964f-530ab62c85da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 pods, using pod/ebs-csi-controller-59b6797bf-vfvs2\n",
      "I0113 21:28:06.936951       1 main.go:153] \"Initializing metadata\"\n",
      "I0113 21:28:06.943481       1 metadata.go:48] \"Retrieved metadata from IMDS\"\n",
      "I0113 21:28:06.944585       1 driver.go:69] \"Driver Information\" Driver=\"ebs.csi.aws.com\" Version=\"v1.38.1\"\n",
      "I0113 22:01:16.236299       1 controller.go:410] \"ControllerPublishVolume: attaching\" volumeID=\"vol-0ed5d3cc8cb5a989e\" nodeID=\"i-0fd8ede93d6ac59a4\"\n",
      "I0113 22:01:17.882498       1 controller.go:419] \"ControllerPublishVolume: attached\" volumeID=\"vol-0ed5d3cc8cb5a989e\" nodeID=\"i-0fd8ede93d6ac59a4\" devicePath=\"/dev/xvdaa\"\n"
     ]
    }
   ],
   "source": [
    "!kubectl logs deployment/ebs-csi-controller -n kube-system -c ebs-plugin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8ab7f84b-398b-41fb-9c35-30f40c179f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 pods, using pod/ebs-csi-node-l5nh6\n",
      "I0113 21:27:55.828513       1 main.go:153] \"Initializing metadata\"\n",
      "I0113 21:27:55.832956       1 metadata.go:48] \"Retrieved metadata from IMDS\"\n",
      "I0113 21:27:55.833450       1 driver.go:69] \"Driver Information\" Driver=\"ebs.csi.aws.com\" Version=\"v1.38.1\"\n",
      "E0113 21:27:56.896438       1 node.go:856] \"Unexpected failure when attempting to remove node taint(s)\" err=\"isAllocatableSet: driver not found on node ip-10-0-1-50.ca-central-1.compute.internal\"\n",
      "I0113 21:27:57.410789       1 node.go:936] \"CSINode Allocatable value is set\" nodeName=\"ip-10-0-1-50.ca-central-1.compute.internal\" count=24\n",
      "I0113 22:01:18.817732       1 node.go:204] \"NodeStageVolume: invalid partition config, will ignore.\" partition=\"0\"\n",
      "I0113 22:01:18.980008       1 mount_linux.go:295] Detected OS without systemd\n"
     ]
    }
   ],
   "source": [
    "!kubectl logs daemonset/ebs-csi-node -n kube-system -c ebs-plugin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f2fd1fcf-f18f-4b19-abeb-1c207ddd58de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 pods, using pod/ebs-csi-controller-59b6797bf-vfvs2\n",
      "W0113 21:28:09.566495       1 feature_gate.go:354] Setting GA feature gate Topology=true. It will be removed in a future release.\n",
      "I0113 21:28:09.566789       1 feature_gate.go:387] feature gates: {map[Topology:true]}\n",
      "I0113 21:28:09.566840       1 csi-provisioner.go:154] Version: v5.1.0\n",
      "I0113 21:28:09.566851       1 csi-provisioner.go:177] Building kube configs for running in cluster...\n",
      "I0113 21:28:09.568886       1 common.go:143] \"Probing CSI driver for readiness\"\n",
      "I0113 21:28:09.598446       1 csi-provisioner.go:230] Detected CSI driver ebs.csi.aws.com\n",
      "I0113 21:28:09.598619       1 csi-provisioner.go:240] Supports migration from in-tree plugin: kubernetes.io/aws-ebs\n",
      "I0113 21:28:09.600672       1 common.go:143] \"Probing CSI driver for readiness\"\n",
      "I0113 21:28:09.603385       1 csi-provisioner.go:299] CSI driver supports PUBLISH_UNPUBLISH_VOLUME, watching VolumeAttachments\n",
      "I0113 21:28:09.604750       1 controller.go:744] \"Using saving PVs to API server in background\"\n",
      "I0113 21:28:09.605162       1 leaderelection.go:254] attempting to acquire leader lease kube-system/ebs-csi-aws-com...\n",
      "I0113 21:28:09.627768       1 leaderelection.go:268] successfully acquired lease kube-system/ebs-csi-aws-com\n",
      "I0113 21:28:09.629736       1 leader_election.go:184] \"became leader, starting\"\n",
      "I0113 21:28:09.629845       1 envvar.go:172] \"Feature gate default state\" feature=\"WatchListClient\" enabled=false\n",
      "I0113 21:28:09.629863       1 envvar.go:172] \"Feature gate default state\" feature=\"InformerResourceVersion\" enabled=false\n",
      "I0113 21:28:09.640053       1 reflector.go:368] Caches populated for *v1.StorageClass from k8s.io/client-go@v0.31.0/tools/cache/reflector.go:243\n",
      "I0113 21:28:09.651560       1 reflector.go:368] Caches populated for *v1.PersistentVolumeClaim from k8s.io/client-go@v0.31.0/tools/cache/reflector.go:243\n",
      "I0113 21:28:09.656122       1 reflector.go:368] Caches populated for *v1.Node from k8s.io/client-go@v0.31.0/tools/cache/reflector.go:243\n",
      "I0113 21:28:09.656502       1 reflector.go:368] Caches populated for *v1.VolumeAttachment from k8s.io/client-go@v0.31.0/tools/cache/reflector.go:243\n",
      "I0113 21:28:09.657442       1 reflector.go:368] Caches populated for *v1.CSINode from k8s.io/client-go@v0.31.0/tools/cache/reflector.go:243\n",
      "I0113 21:28:09.729875       1 controller.go:824] \"Starting provisioner controller\" component=\"ebs.csi.aws.com_ebs-csi-controller-59b6797bf-vfvs2_1c292ac0-a9d9-48ff-8621-9ba1f75e01b8\"\n",
      "I0113 21:28:09.729957       1 volume_store.go:98] \"Starting save volume queue\"\n",
      "I0113 21:28:09.734566       1 reflector.go:368] Caches populated for *v1.StorageClass from k8s.io/client-go@v0.31.0/tools/cache/reflector.go:243\n",
      "I0113 21:28:09.734567       1 reflector.go:368] Caches populated for *v1.PersistentVolume from k8s.io/client-go@v0.31.0/tools/cache/reflector.go:243\n",
      "I0113 21:28:09.831005       1 controller.go:873] \"Started provisioner controller\" component=\"ebs.csi.aws.com_ebs-csi-controller-59b6797bf-vfvs2_1c292ac0-a9d9-48ff-8621-9ba1f75e01b8\"\n"
     ]
    }
   ],
   "source": [
    "!kubectl logs deployment/ebs-csi-controller -n kube-system -c csi-provisioner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "49134a19-5818-4030-8a7a-6c9e04d6157c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 pods, using pod/ebs-csi-controller-59b6797bf-vfvs2\n",
      "I0113 21:28:12.211572       1 main.go:109] \"Version\" version=\"v4.7.0\"\n",
      "I0113 21:28:12.215865       1 common.go:143] \"Probing CSI driver for readiness\"\n",
      "I0113 21:28:12.238251       1 main.go:169] \"CSI driver name\" driver=\"ebs.csi.aws.com\"\n",
      "I0113 21:28:12.240448       1 common.go:143] \"Probing CSI driver for readiness\"\n",
      "I0113 21:28:12.242734       1 main.go:249] \"CSI driver supports ControllerPublishUnpublish, using real CSI handler\" driver=\"ebs.csi.aws.com\"\n",
      "I0113 21:28:12.243624       1 leaderelection.go:254] attempting to acquire leader lease kube-system/external-attacher-leader-ebs-csi-aws-com...\n",
      "I0113 21:28:12.265555       1 leaderelection.go:268] successfully acquired lease kube-system/external-attacher-leader-ebs-csi-aws-com\n",
      "I0113 21:28:12.266787       1 leader_election.go:184] \"became leader, starting\"\n",
      "I0113 21:28:12.266811       1 controller.go:129] \"Starting CSI attacher\"\n",
      "I0113 21:28:12.267257       1 envvar.go:172] \"Feature gate default state\" feature=\"InformerResourceVersion\" enabled=false\n",
      "I0113 21:28:12.267536       1 envvar.go:172] \"Feature gate default state\" feature=\"WatchListClient\" enabled=false\n",
      "I0113 21:28:12.271734       1 reflector.go:368] Caches populated for *v1.PersistentVolume from k8s.io/client-go@v0.31.0/tools/cache/reflector.go:243\n",
      "I0113 21:28:12.272191       1 reflector.go:368] Caches populated for *v1.CSINode from k8s.io/client-go@v0.31.0/tools/cache/reflector.go:243\n",
      "I0113 21:28:12.272546       1 reflector.go:368] Caches populated for *v1.VolumeAttachment from k8s.io/client-go@v0.31.0/tools/cache/reflector.go:243\n",
      "I0113 22:01:16.209140       1 csi_handler.go:261] \"Attaching\" VolumeAttachment=\"csi-ed0e5d922afcde1b934ffafc0f0d3a9543d0760d7dfa4ce99585f3fc2c2922b0\"\n",
      "I0113 22:01:17.882946       1 csi_handler.go:273] \"Attached\" VolumeAttachment=\"csi-ed0e5d922afcde1b934ffafc0f0d3a9543d0760d7dfa4ce99585f3fc2c2922b0\"\n"
     ]
    }
   ],
   "source": [
    "!kubectl logs deployment/ebs-csi-controller -n kube-system -c csi-attacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6fdb0f08-aba7-4d63-8e01-2b25d1b06b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"eks.amazonaws.com/role-arn\":\"\",\"meta.helm.sh/release-name\":\"aws-ebs-csi-driver\",\"meta.helm.sh/release-namespace\":\"kube-system\"}"
     ]
    }
   ],
   "source": [
    "!kubectl get sa ebs-csi-controller-sa -n kube-system -o jsonpath='{.metadata.annotations}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a85d88b-946c-40e7-8b59-0e69a4567d5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
