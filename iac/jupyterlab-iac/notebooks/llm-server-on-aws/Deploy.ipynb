{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d02ca0b4-1c4a-4b8e-a7a9-8fe3c9ba40b1",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38534870-6679-40ae-bcf1-f778377e5998",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from time import sleep\n",
    "from time import time as unixtime\n",
    "from typing import Callable, List\n",
    "import random\n",
    "from math import ceil\n",
    "import yaml\n",
    "import string\n",
    "import json\n",
    "from urllib.parse import urlparse\n",
    "from time import sleep\n",
    "import re\n",
    "from operator import itemgetter\n",
    "import subprocess\n",
    "from tempfile import NamedTemporaryFile\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "import boto3\n",
    "import kubernetes\n",
    "from kubernetes.client.rest import ApiException\n",
    "\n",
    "from pyhelm3 import Client as HelmClient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a98f1d3-e2b6-4c0a-ad3d-c07f5416d684",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv('/.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08774840-c227-48d4-844c-165c6f5dc8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import wait_until\n",
    "from helper.ec2 import (get_vpcs_ids, get_internet_gateway_ids_attached_to_vpc, \n",
    "                        get_route_table_ids_for_vpc, route_to_gateway_exists, \n",
    "                        get_subnet_ids_in_vpc, get_security_group_ids)\n",
    "from helper.k8s import (get_one_running_pod, get_jupyter_token_from_pod)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1800b74-d019-445d-942e-7223e770f332",
   "metadata": {},
   "source": [
    "# Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3b9f759-4820-40ad-8916-e935d6e57232",
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = 'ca-central-1'\n",
    "CLUSTER_NAME = 'kubyterlab-llm'\n",
    "EBS_VOLUME_SIZE = 500 # GiB\n",
    "TAGS = {'cluster': CLUSTER_NAME, 'purpose': 'llm'}  # Do not change the keys, they are hardcoded throughout.\n",
    "CLUSTER_TAGS = {'cluster': CLUSTER_NAME}\n",
    "VOLUME_FILTERS = [\n",
    "    {'Name': f'tag:purpose', \n",
    "     'Values': ['kubyterlab-llm', 'llm']}]\n",
    "K8S_VERSION = os.environ['K8S_VERSION']  # '1.30'\n",
    "K8S_VERSION = '.'.join(K8S_VERSION.split('.')[:2]) if len(K8S_VERSION.split('.')) > 2 else K8S_VERSION\n",
    "INSTANCE_TYPES = {'gpu': ['g4dn.2xlarge'], 'default': ['t3.medium']}\n",
    "ALLOWED_PORTS = [80, 443, 22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d859c1f-c31c-43b5-9a3a-fa9ed0ae7006",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_all_vpcs_available(response: dict) -> True:\n",
    "    if not 'Vpcs' in response:\n",
    "        raise ValueError\n",
    "    return all([vpc.get('State', '') == 'available' for vpc in response['Vpcs']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70ff5d92-c3be-4255-9d40-afea872ff121",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_cluster_active(response: dict) -> bool:\n",
    "    status = response['cluster']['status']\n",
    "    clear_output(wait=True)\n",
    "    display(status)\n",
    "    return status == 'ACTIVE'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8983294-0e11-457c-abb6-2ec996243d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_node_group_active(response: dict) -> bool:\n",
    "    status = response['nodegroup']['status']\n",
    "    clear_output(wait=True)\n",
    "    display(status)\n",
    "    return status in ['ACTIVE', 'CREATE_FAILED']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9938e8cf-fc3a-42f8-819b-7748181fffc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_snapshot_completed(response: dict) -> bool:\n",
    "    state = response['Snapshots'][0]['State']\n",
    "    clear_output(wait=True)\n",
    "    display(state)\n",
    "    return state.lower() == 'completed'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f79a307-fbfa-4d1e-9d75-9f77405a7dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_volume_available(response: dict) -> bool:\n",
    "    state = response['Volumes'][0]['State']\n",
    "    clear_output(wait=True)\n",
    "    display(state)\n",
    "    return state.lower() == 'available'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73ff831-37b6-4fbf-9aad-1068cfb8efe7",
   "metadata": {},
   "source": [
    "# Instantiate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97c98c25-b95b-40da-be00-f65f5952d5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = boto3.Session(region_name=REGION)\n",
    "eks_client = session.client('eks')\n",
    "ec2_client = session.client('ec2')\n",
    "iam_client = session.client('iam')\n",
    "\n",
    "aws_account_id = boto3.client('sts').get_caller_identity().get('Account')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbf7c2f-2e86-48c9-b369-1047bd8fea6c",
   "metadata": {},
   "source": [
    "# Create or Restore Volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "025247fd-7e39-46c3-ba9c-8bf58a3dcc19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    response = ec2_client.describe_volumes(Filters=VOLUME_FILTERS)\n",
    "    volumes = response.get('Volumes', [])\n",
    "except RuntimeError:\n",
    "    volumes = []\n",
    "volume_ids = [volume['VolumeId'] for volume in volumes]\n",
    "volume_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "728edb7d-b012-4778-8f98-34f00f520042",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(volume_ids) <= 1  # TODO: Get the latest one if more than one.\n",
    "if volume_ids:\n",
    "    volume_id = volume_ids[0]\n",
    "    availability_zone = volumes[0]['AvailabilityZone']\n",
    "else:\n",
    "    volume_id = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf65cde1-f0ad-4b9a-9c91-49c20d09b2f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "snap-070f65816ea732b4e\n"
     ]
    }
   ],
   "source": [
    "if not volume_id:\n",
    "    response = ec2_client.describe_snapshots(Filters=VOLUME_FILTERS)\n",
    "    snapshots = response.get('Snapshots', [])\n",
    "    if snapshots:\n",
    "        sorted_snapshots = sorted(snapshots, key=itemgetter('StartTime'), reverse=True)\n",
    "        snapshot_id = sorted_snapshots[0]['SnapshotId']\n",
    "        print(snapshot_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5d85302-63d3-4ef2-a07e-bd59dc564768",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'available'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('vol-0b2f7361fa13c5275', 'ca-central-1a')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not volume_id:\n",
    "    response = ec2_client.describe_availability_zones()\n",
    "    availability_zones = response['AvailabilityZones']\n",
    "    availability_zone = availability_zones[0]['ZoneName']\n",
    "    # availability_zone = f'{REGION}a'\n",
    "    if snapshots:\n",
    "        # TODO: Change this to the latest snapshot!!\n",
    "        response = ec2_client.create_volume(\n",
    "            SnapshotId=snapshot_id,\n",
    "            Size=EBS_VOLUME_SIZE,\n",
    "            AvailabilityZone=availability_zone,\n",
    "            VolumeType='gp3',\n",
    "            TagSpecifications=[\n",
    "                {\n",
    "                    'ResourceType': 'volume',\n",
    "                    'Tags': [{'Key': 'purpose', 'Value': 'llm'}]\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "    else:\n",
    "        response = ec2_client.create_volume(\n",
    "            Size=EBS_VOLUME_SIZE,\n",
    "            AvailabilityZone=availability_zones[0]['ZoneName'],\n",
    "            VolumeType='gp3',\n",
    "            TagSpecifications=[\n",
    "                {\n",
    "                    'ResourceType': 'volume',\n",
    "                    'Tags': [{'Key': 'purpose', 'Value': 'llm'}]\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "    wait_until(ec2_client.describe_volumes, {'VolumeIds': [response['VolumeId']]}, is_volume_available)\n",
    "    volume_id = response['VolumeId']\n",
    "volume_id, availability_zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e815d5e-1d1e-4f94-ac53-9fa53edbdd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Change the parts below to use Terraform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbda3f9d-a108-48f4-8411-f7dd2e0e1f43",
   "metadata": {},
   "source": [
    "# Create VPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "429b8e60-e14a-460b-a448-45bc392e6c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating VPC...\n",
      "Creating Internet Gateway...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('vpc-0d873bc388d2dfd91', 'igw-000c8b40a11ef13a8', 'rtb-0874add4d53244b3d')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vpc_ids = get_vpcs_ids(ec2_client, TAGS)\n",
    "vpc_exists = len(vpc_ids) > 0\n",
    "\n",
    "if len(vpc_ids) > 1:\n",
    "    raise RuntimeError\n",
    "elif vpc_exists:\n",
    "    vpc_id = vpc_ids[0]\n",
    "\n",
    "# Create VPC\n",
    "if not vpc_exists:\n",
    "    print('Creating VPC...')\n",
    "    vpc_response = ec2_client.create_vpc(\n",
    "        CidrBlock='10.0.0.0/16',\n",
    "        TagSpecifications=[\n",
    "            {\n",
    "                'ResourceType': 'vpc',\n",
    "                'Tags': [{'Key': tag, 'Value': TAGS[tag] }for tag in TAGS]\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    vpc_id = vpc_response['Vpc']['VpcId']\n",
    "wait_until(ec2_client.describe_vpcs, {'VpcIds': [vpc_id]}, check_all_vpcs_available)\n",
    "\n",
    "# Create Internet Gateway\n",
    "igw_ids = get_internet_gateway_ids_attached_to_vpc(ec2_client, vpc_id)\n",
    "igw_exists = len(igw_ids) > 0\n",
    "\n",
    "if len(igw_ids) > 1:\n",
    "    raise RuntimeError\n",
    "elif igw_exists:\n",
    "    igw_id = igw_ids[0]\n",
    "\n",
    "\n",
    "if not igw_exists:\n",
    "    print('Creating Internet Gateway...')\n",
    "    # Create an Internet Gateway\n",
    "    igw_response = ec2_client.create_internet_gateway()\n",
    "    igw_id = igw_response['InternetGateway']['InternetGatewayId']\n",
    "    \n",
    "    # Attach Internet Gateway to VPC\n",
    "    ec2_client.attach_internet_gateway(\n",
    "        InternetGatewayId=igw_id,\n",
    "        VpcId=vpc_id\n",
    "    )\n",
    "\n",
    "# Create a route table\n",
    "route_table_ids = get_route_table_ids_for_vpc(ec2_client, vpc_id)\n",
    "route_table_exists = len(route_table_ids) > 0\n",
    "\n",
    "if not route_table_exists:\n",
    "    print('Creating a route table...')\n",
    "    # Create a route table\n",
    "    route_table_response = ec2_client.create_route_table(VpcId=vpc_id)\n",
    "    route_table_id = route_table_response['RouteTable']['RouteTableId']\n",
    "\n",
    "is_route_created = False\n",
    "for route_table_id in route_table_ids:\n",
    "    if route_to_gateway_exists(ec2_client, route_table_id, igw_id):\n",
    "        is_route_created = True\n",
    "        break\n",
    "\n",
    "if not is_route_created:\n",
    "    # Create a route to the Internet Gateway\n",
    "    ec2_client.create_route(\n",
    "        RouteTableId=route_table_id,\n",
    "        DestinationCidrBlock='0.0.0.0/0',\n",
    "        GatewayId=igw_id\n",
    "    )\n",
    "\n",
    "vpc_id, igw_id, route_table_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbfdb24-1435-47bb-8293-b87b9d4075b7",
   "metadata": {},
   "source": [
    "# Create Subnets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b32166a-ce50-45f6-aac3-3f809836f103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating subnet #1...\n",
      "Creating subnet #2...\n"
     ]
    }
   ],
   "source": [
    "subnet_ids = get_subnet_ids_in_vpc(ec2_client, vpc_id)\n",
    "min_subnet_count = 2\n",
    "current_subnet_count = len(subnet_ids)\n",
    "\n",
    "# if current_subnet_count < min_subnet_count:\n",
    "    # response = ec2_client.describe_availability_zones()\n",
    "    \n",
    "    # availability_zones = [az['ZoneName'] for az in response['AvailabilityZones']]\n",
    "    # random.shuffle(availability_zones)\n",
    "    # TODO: Following logic is still necessary for situations with larger subnets. Change to something reasonable.\n",
    "    # availability_zones *= ceil(min_subnet_count / len(availability_zones))\n",
    "    \n",
    "\n",
    "while current_subnet_count < min_subnet_count:\n",
    "    i = current_subnet_count + 1\n",
    "    print(f'Creating subnet #{i}...')\n",
    "    subnet_response = ec2_client.create_subnet(\n",
    "        VpcId=vpc_id,\n",
    "        CidrBlock=f'10.0.{i}.0/24',\n",
    "        # AvailabilityZone=availability_zone,\n",
    "        AvailabilityZone=availability_zones[current_subnet_count]['ZoneName'],\n",
    "        TagSpecifications=[\n",
    "            {\n",
    "                'ResourceType': 'subnet',\n",
    "                'Tags': [{'Key': tag, 'Value': TAGS[tag]} for tag in TAGS]\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    subnet_id = subnet_response['Subnet']['SubnetId']\n",
    "    subnet_ids.append(subnet_id)\n",
    "    current_subnet_count = len(subnet_ids)\n",
    "\n",
    "# check if any subnet is public, add the table to first subnet if not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c837c746-05e1-432f-8619-e3c0581650f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "subnets_response = ec2_client.describe_subnets(SubnetIds=subnet_ids)\n",
    "assert len(subnets_response) == len(subnet_ids) == min_subnet_count\n",
    "is_any_subnet_open_to_public = False\n",
    "for subnet in subnets_response['Subnets']:\n",
    "    if subnets_response.get('MapPublicIpOnLaunch', False):\n",
    "        # I could also look at the route tables, and see if they are assigned to any subnet. ec2_client.describe_route_tables(RouteTableIds=[route_table_id]) RouteTables.SubnetId (optional parameter)\n",
    "        public_subnet_id = subnet['SubnetId']\n",
    "        is_any_subnet_open_to_public = True\n",
    "\n",
    "if not is_any_subnet_open_to_public:\n",
    "    for subnet in subnets_response['Subnets']:\n",
    "        if subnet['CidrBlock'] == '10.0.1.0/24':\n",
    "            public_subnet_id = subnet['SubnetId']\n",
    "\n",
    "            # Associate the public subnet with the route table\n",
    "            ec2_client.associate_route_table(\n",
    "                RouteTableId=route_table_id,\n",
    "                SubnetId=public_subnet_id\n",
    "            )\n",
    "            \n",
    "            # Modify the public subnet to auto-assign public IPs\n",
    "            ec2_client.modify_subnet_attribute(\n",
    "                SubnetId=public_subnet_id,\n",
    "                MapPublicIpOnLaunch={\"Value\": True}\n",
    "            )\n",
    "            break\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95af95c-f3a4-46d3-b5f8-b7d953eda3a9",
   "metadata": {},
   "source": [
    "# Create Security Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf11870e-3dee-4564-9c82-c78a0262ce77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Security Group...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'sg-0cc5d66a795596b94'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "security_group_ids = get_security_group_ids(ec2_client, TAGS)\n",
    "\n",
    "security_group_exists = len(security_group_ids) > 0\n",
    "\n",
    "if len(security_group_ids) > 1:\n",
    "    raise RuntimeError\n",
    "elif security_group_exists:\n",
    "    security_group_id = security_group_ids[0]\n",
    "\n",
    "# The security group gets torn down when deleted, so there is no need to check the rules and rewrite all of them.\n",
    "\n",
    "if not security_group_exists:\n",
    "    print('Creating Security Group...')\n",
    "    response = ec2_client.create_security_group(\n",
    "        GroupName=f'eks-cluster-sg-{CLUSTER_NAME}',\n",
    "        TagSpecifications=[\n",
    "            {\n",
    "                'ResourceType': 'security-group', \n",
    "                'Tags': [{'Key': tag, 'Value': TAGS[tag]} for tag in TAGS]\n",
    "            }\n",
    "        ],\n",
    "        Description=f'Security group for EKS cluster: {CLUSTER_NAME}',\n",
    "        VpcId=vpc_id\n",
    "    )\n",
    "    security_group_id = response['GroupId']\n",
    "\n",
    "    # TODO: Can I make the CidrIp more restrictive for the next deployment? Load Balancer needs to have a static IP, probably through the Kubernetes YAML?\n",
    "    for port in ALLOWED_PORTS:\n",
    "        ec2_client.authorize_security_group_ingress(\n",
    "            GroupId=security_group_id,\n",
    "            IpPermissions=[\n",
    "                {\n",
    "                    'IpProtocol': 'tcp',\n",
    "                    'FromPort': port,\n",
    "                    'ToPort': port,\n",
    "                    'IpRanges': [{'CidrIp': '0.0.0.0/0'}]\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "    \n",
    "security_group_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6a6e50f1-8540-4dcf-9aa6-46c0fd668aea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('vol-0b2f7361fa13c5275',\n",
       " 'vpc-0d873bc388d2dfd91',\n",
       " 'igw-000c8b40a11ef13a8',\n",
       " 'rtb-0874add4d53244b3d',\n",
       " 'subnet-094faf3e919da233a',\n",
       " ['subnet-094faf3e919da233a', 'subnet-001821e9b0ed9d7e4'],\n",
       " 'sg-0cc5d66a795596b94')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "volume_id, vpc_id, igw_id, route_table_id, public_subnet_id, subnet_ids, security_group_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5e3e6a-1650-4a59-91b7-bff33b937a52",
   "metadata": {},
   "source": [
    "# Create Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e5e5fec8-eea5-4ef7-806c-4bdea1f61845",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ACTIVE'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tear_down_and_rebuild = False\n",
    "response = eks_client.list_clusters()\n",
    "clusters = response['clusters']\n",
    "if CLUSTER_NAME not in clusters:\n",
    "    eks_client.create_cluster(name=CLUSTER_NAME, \n",
    "                              version=K8S_VERSION, \n",
    "                              roleArn=f'arn:aws:iam::{aws_account_id}:role/EKS_Cluster_Role', \n",
    "                              resourcesVpcConfig={'subnetIds': subnet_ids,\n",
    "                                                  'securityGroupIds': [security_group_id],\n",
    "                                                  'endpointPublicAccess': True,\n",
    "                                                  'endpointPrivateAccess': False\n",
    "                              },\n",
    "                              tags=TAGS,\n",
    "                             )\n",
    "else:\n",
    "    if tear_down_and_rebuild:\n",
    "        pass\n",
    "        # TODO: Create a new cluster with a temp name.\n",
    "        # Wait until the cluster has finished forming\n",
    "        # Delete the old cluster\n",
    "        # Wait until deletion is complete\n",
    "        # Rename the new cluster\n",
    "        \n",
    "wait_until(eks_client.describe_cluster, {'name': CLUSTER_NAME}, is_cluster_active, timeout=7 * 60)\n",
    "response = eks_client.describe_cluster(name=CLUSTER_NAME)\n",
    "assert response['cluster']['status'] == 'ACTIVE'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7a812b-4a62-4b3b-a88e-90b0db82a194",
   "metadata": {},
   "source": [
    "# Create IAM Role for Node Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "49bcf925-9e7a-47af-9d1a-d160959148cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Role EKS_Cluster_Role already exists. Arn: arn:aws:iam::275678099358:role/EKS_Cluster_Role.\n",
      "Attached policy AmazonEKSWorkerNodePolicy to role EKS_Cluster_Role.\n",
      "Attached policy AmazonEC2ContainerRegistryReadOnly to role EKS_Cluster_Role.\n",
      "Attached policy AmazonEKS_CNI_Policy to role EKS_Cluster_Role.\n",
      "Attached policy AmazonEKSClusterPolicy to role EKS_Cluster_Role.\n"
     ]
    }
   ],
   "source": [
    "# Define the role name\n",
    "role_name = 'EKS_Cluster_Role'  # WARNING: I get a weird error if I call this role anything else. The call looks for this particular name, and I do not know how to override it.\n",
    "\n",
    "# Create the trust policy for the role\n",
    "trust_policy = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\n",
    "                \"Service\": [\n",
    "                    \"eks.amazonaws.com\",\n",
    "                    \"ec2.amazonaws.com\"\n",
    "                ]\n",
    "            },\n",
    "            \"Action\": \"sts:AssumeRole\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create the IAM role\n",
    "try:\n",
    "    response = iam_client.create_role(\n",
    "        RoleName=role_name,\n",
    "        AssumeRolePolicyDocument=json.dumps(trust_policy),\n",
    "        Description='Role for EKS Node Group'\n",
    "    )\n",
    "    node_role_arn = response['Role']['Arn']\n",
    "    print(f\"Created role: {node_role_arn}\")\n",
    "except iam_client.exceptions.EntityAlreadyExistsException:\n",
    "    response = iam_client.get_role(RoleName=role_name)\n",
    "    node_role_arn = response['Role']['Arn']\n",
    "    print(f\"Role {role_name} already exists. Arn: {node_role_arn}.\")\n",
    "    # TODO: Check if trust_policy is correct.\n",
    "\n",
    "# Attach necessary policies\n",
    "policies = [\n",
    "    'AmazonEKSWorkerNodePolicy',\n",
    "    'AmazonEC2ContainerRegistryReadOnly',\n",
    "    'AmazonEKS_CNI_Policy',\n",
    "    'AmazonEKSClusterPolicy',\n",
    "    # 'AmazonSSMManagedInstanceCore',\n",
    "]\n",
    "# Policy AmazonSSMManagedInstanceCore is not necessary, I used it for debugging, to connect to the node and run commands. \n",
    "\n",
    "for policy in policies:\n",
    "    try:\n",
    "        iam_client.attach_role_policy(\n",
    "            RoleName=role_name,\n",
    "            PolicyArn=f'arn:aws:iam::aws:policy/{policy}'\n",
    "        )\n",
    "        print(f\"Attached policy {policy} to role {role_name}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error attaching policy {policy}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8c8bd2-f80d-4a36-b648-4895429988ab",
   "metadata": {},
   "source": [
    "# Add Node Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "11c771d0-9929-4c01-8ae6-e7c0845760d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: This part may also need a wait."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "75cfa5fb-e965-4e51-bee6-8414c32a7689",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CREATING'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_groups = eks_client.list_nodegroups(clusterName=CLUSTER_NAME)\n",
    "# TODO: Make this static and put to the top, with different `diskSize` and `scalingConfig`\n",
    "node_group_name = 'gpu'\n",
    "ami_type = 'AL2_x86_64_GPU'\n",
    "if node_group_name in node_groups['nodegroups']:\n",
    "    response = eks_client.describe_nodegroup(clusterName=CLUSTER_NAME, nodegroupName=node_group_name)\n",
    "    status = response['nodegroup']['status']\n",
    "    if status == 'CREATE_FAILED':\n",
    "        print('Node group exists, but failed to create. Deleting...')\n",
    "        failed_node_group_info = response\n",
    "\n",
    "        eks_client.delete_nodegroup(clusterName=CLUSTER_NAME, nodegroupName=node_group_name)\n",
    "    \n",
    "        wait_until(eks_client.list_nodegroups, {'clusterName': CLUSTER_NAME}, lambda x: node_group_name not in x['nodegroups'])\n",
    "        node_groups = eks_client.list_nodegroups(clusterName=CLUSTER_NAME)\n",
    "    else:\n",
    "        print('Node group exists.')\n",
    "\n",
    "\n",
    "\n",
    "if not node_groups['nodegroups']:\n",
    "    print('Creating node group...')\n",
    "    response = eks_client.create_nodegroup(\n",
    "        clusterName=CLUSTER_NAME,\n",
    "        nodegroupName=node_group_name,\n",
    "        scalingConfig={\n",
    "            'desiredSize': 1,\n",
    "            'minSize': 1,\n",
    "            'maxSize': 1\n",
    "        },\n",
    "        diskSize=50,  # Size in GiB\n",
    "        subnets=[public_subnet_id],\n",
    "        nodeRole=node_role_arn,\n",
    "        amiType=ami_type,\n",
    "        instanceTypes=INSTANCE_TYPES[node_group_name],\n",
    "        labels={\n",
    "            'gpu-memory': 'true'\n",
    "        },\n",
    "        taints=[\n",
    "            {\n",
    "                'key': 'nvidia.com/gpu',\n",
    "                'value': 'true',\n",
    "                'effect': 'NO_SCHEDULE'\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "wait_until(eks_client.describe_nodegroup, {'clusterName': CLUSTER_NAME, 'nodegroupName': node_group_name}, is_node_group_active)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a69858b4-e5f2-4951-aa3d-69cd404c0e27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CREATING'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_groups = eks_client.list_nodegroups(clusterName=CLUSTER_NAME)\n",
    "# TODO: Make this static and put to the top, with different `diskSize` and `scalingConfig`\n",
    "node_group_name = 'default'\n",
    "ami_type = 'AL2_x86_64'\n",
    "if node_group_name in node_groups['nodegroups']:\n",
    "    response = eks_client.describe_nodegroup(clusterName=CLUSTER_NAME, nodegroupName=node_group_name)\n",
    "    status = response['nodegroup']['status']\n",
    "    if status == 'CREATE_FAILED':\n",
    "        print('Node group exists, but failed to create. Deleting...')\n",
    "        failed_node_group_info = response\n",
    "\n",
    "        eks_client.delete_nodegroup(clusterName=CLUSTER_NAME, nodegroupName=node_group_name)\n",
    "    \n",
    "        wait_until(eks_client.list_nodegroups, {'clusterName': CLUSTER_NAME}, lambda x: node_group_name not in x['nodegroups'])\n",
    "        node_groups = eks_client.list_nodegroups(clusterName=CLUSTER_NAME)\n",
    "    else:\n",
    "        print('Node group exists.')\n",
    "\n",
    "\n",
    "# TODO: See if lower disksize will work.\n",
    "if node_group_name not in node_groups['nodegroups']:\n",
    "    print('Creating node group...')\n",
    "    response = eks_client.create_nodegroup(\n",
    "        clusterName=CLUSTER_NAME,\n",
    "        nodegroupName=node_group_name,\n",
    "        scalingConfig={\n",
    "            'desiredSize': 2,\n",
    "            'minSize': 2,\n",
    "            'maxSize': 2\n",
    "        },\n",
    "        diskSize=1000,  # Size in GiB\n",
    "        subnets=[public_subnet_id],\n",
    "        nodeRole=node_role_arn,\n",
    "        amiType=ami_type,\n",
    "        instanceTypes=INSTANCE_TYPES[node_group_name],\n",
    "    )\n",
    "wait_until(eks_client.describe_nodegroup, {'clusterName': CLUSTER_NAME, 'nodegroupName': node_group_name}, is_node_group_active)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b295fbb6-d655-4386-94f0-84dfe0cbb454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instance i-0da26cffc3155cdf8 is of type g4dn.xlarge\n"
     ]
    }
   ],
   "source": [
    "# This is for double-checking the type of instance.\n",
    "asg_client = boto3.client(\"autoscaling\")\n",
    "node_group = eks_client.describe_nodegroup(clusterName=CLUSTER_NAME, nodegroupName='gpu')\n",
    "asg_name = node_group[\"nodegroup\"][\"resources\"][\"autoScalingGroups\"][0][\"name\"]\n",
    "asg_details = asg_client.describe_auto_scaling_groups(AutoScalingGroupNames=[asg_name])\n",
    "instance_ids = [instance[\"InstanceId\"] for instance in asg_details[\"AutoScalingGroups\"][0][\"Instances\"]]\n",
    "for instance_id in instance_ids:\n",
    "    instance_details = ec2_client.describe_instances(InstanceIds=[instance_id])\n",
    "    instance_type = instance_details[\"Reservations\"][0][\"Instances\"][0][\"InstanceType\"]\n",
    "    print(f\"Instance {instance_id} is of type {instance_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9df50a-ede3-4b60-af87-f7295796c94c",
   "metadata": {},
   "source": [
    "# Initialize Kubernetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "77edf8ec-0ec3-42d2-a8a6-f8dbf606364f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated context arn:aws:eks:ca-central-1:275678099358:cluster/kubyterlab-llm in /root/.kube/config\n"
     ]
    }
   ],
   "source": [
    "!aws eks update-kubeconfig --name $CLUSTER_NAME --region $REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8ac935-b3bd-491f-9a4c-ec60bd3f3271",
   "metadata": {},
   "source": [
    "# Initialize Helm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9ec6ffe2-7458-4d1b-90dd-0b6fbbb0e921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"aws-ebs-csi-driver\" already exists with the same configuration, skipping\n"
     ]
    }
   ],
   "source": [
    "!helm repo add aws-ebs-csi-driver https://kubernetes-sigs.github.io/aws-ebs-csi-driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1d3707f7-5505-4195-8666-748cda44c02a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"eks\" already exists with the same configuration, skipping\n"
     ]
    }
   ],
   "source": [
    "!helm repo add eks https://aws.github.io/eks-charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5c979c04-3511-4ad9-a1f8-f2af1cdac0e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"ingress-nginx\" already exists with the same configuration, skipping\n"
     ]
    }
   ],
   "source": [
    "!helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "79fbbaa4-e44b-4a43-bfd1-04d3a9dcf4fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hang tight while we grab the latest from your chart repositories...\n",
      "...Successfully got an update from the \"ingress-nginx\" chart repository\n",
      "...Successfully got an update from the \"aws-ebs-csi-driver\" chart repository\n",
      "...Successfully got an update from the \"eks\" chart repository\n",
      "Update Complete. ⎈Happy Helming!⎈\n"
     ]
    }
   ],
   "source": [
    "!helm repo update"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1566d62-4c89-4f87-84a1-5d465d8d89d4",
   "metadata": {},
   "source": [
    "# Create a Role to Install the CSI Driver and Give Permissions Using the OIDC Provider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "baafae37-f87f-47bb-a642-fc1c1c6cb5a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://oidc.eks.ca-central-1.amazonaws.com/id/AFE8CC0B699B011A4B5CD4E88D8834FC'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_response = eks_client.describe_cluster(name=CLUSTER_NAME)\n",
    "oidc_id = cluster_response['cluster']['identity']['oidc']['issuer'].split('/')[-1]\n",
    "oidc_url = f'https://oidc.eks.{REGION}.amazonaws.com/id/{oidc_id}'\n",
    "oidc_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "47f875e6-9a22-4615-98a9-eb43c520fd94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m2025-03-04 02:50:35 [ℹ]  will create IAM Open ID Connect provider for cluster \"kubyterlab-llm\" in \"ca-central-1\"\n",
      "\u001b[36m2025-03-04 02:50:36 [✔]  created IAM Open ID Connect provider for cluster \"kubyterlab-llm\" in \"ca-central-1\"\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!eksctl utils associate-iam-oidc-provider --cluster $CLUSTER_NAME --approve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "023b4e7f-f182-4474-a035-4027c0c75351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arn:aws:iam::275678099358:policy/AmazonEKS_EBS_CSI_Driver_Policy'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazon_eks_ebs_csi_driver_policy_name = 'AmazonEKS_EBS_CSI_Driver_Policy'\n",
    "amazon_eks_ebs_csi_driver_policy = {\n",
    "  \"Version\": \"2012-10-17\",\n",
    "  \"Statement\": [\n",
    "    {\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Action\": [\n",
    "        \"ec2:CreateSnapshot\",\n",
    "        \"ec2:AttachVolume\",\n",
    "        \"ec2:DetachVolume\",\n",
    "        \"ec2:ModifyVolume\",\n",
    "        \"ec2:DescribeAvailabilityZones\",\n",
    "        \"ec2:DescribeInstances\",\n",
    "        \"ec2:DescribeSnapshots\",\n",
    "        \"ec2:DescribeTags\",\n",
    "        \"ec2:DescribeVolumes\",\n",
    "        \"ec2:DescribeVolumesModifications\"\n",
    "      ],\n",
    "      \"Resource\": \"*\"\n",
    "    },\n",
    "    {\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Action\": [\n",
    "        \"ec2:CreateTags\"\n",
    "      ],\n",
    "      \"Resource\": [\n",
    "        \"arn:aws:ec2:*:*:volume/*\",\n",
    "        \"arn:aws:ec2:*:*:snapshot/*\"\n",
    "      ],\n",
    "      \"Condition\": {\n",
    "        \"StringEquals\": {\n",
    "          \"ec2:CreateAction\": [\n",
    "            \"CreateVolume\",\n",
    "            \"CreateSnapshot\"\n",
    "          ]\n",
    "        }\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Action\": [\n",
    "        \"ec2:DeleteTags\"\n",
    "      ],\n",
    "      \"Resource\": [\n",
    "        \"arn:aws:ec2:*:*:volume/*\",\n",
    "        \"arn:aws:ec2:*:*:snapshot/*\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Action\": [\n",
    "        \"ec2:CreateVolume\"\n",
    "      ],\n",
    "      \"Resource\": \"*\",\n",
    "      \"Condition\": {\n",
    "        \"StringLike\": {\n",
    "          \"aws:RequestTag/ebs.csi.aws.com/cluster\": \"true\"\n",
    "        }\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Action\": [\n",
    "        \"ec2:CreateVolume\"\n",
    "      ],\n",
    "      \"Resource\": \"*\",\n",
    "      \"Condition\": {\n",
    "        \"StringLike\": {\n",
    "          \"aws:RequestTag/CSIVolumeName\": \"*\"\n",
    "        }\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Action\": [\n",
    "        \"ec2:DeleteVolume\"\n",
    "      ],\n",
    "      \"Resource\": \"*\",\n",
    "      \"Condition\": {\n",
    "        \"StringLike\": {\n",
    "          \"ec2:ResourceTag/CSIVolumeName\": \"*\"\n",
    "        }\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Action\": [\n",
    "        \"ec2:DeleteVolume\"\n",
    "      ],\n",
    "      \"Resource\": \"*\",\n",
    "      \"Condition\": {\n",
    "        \"StringLike\": {\n",
    "          \"ec2:ResourceTag/ebs.csi.aws.com/cluster\": \"true\"\n",
    "        }\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Action\": [\n",
    "        \"ec2:DeleteSnapshot\"\n",
    "      ],\n",
    "      \"Resource\": \"*\",\n",
    "      \"Condition\": {\n",
    "        \"StringLike\": {\n",
    "          \"ec2:ResourceTag/CSIVolumeSnapshotName\": \"*\"\n",
    "        }\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Action\": [\n",
    "        \"ec2:DeleteSnapshot\"\n",
    "      ],\n",
    "      \"Resource\": \"*\",\n",
    "      \"Condition\": {\n",
    "        \"StringLike\": {\n",
    "          \"ec2:ResourceTag/ebs.csi.aws.com/cluster\": \"true\"\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "try:\n",
    "    response = iam_client.create_policy(\n",
    "        PolicyName=amazon_eks_ebs_csi_driver_policy_name,\n",
    "        PolicyDocument=json.dumps(amazon_eks_ebs_csi_driver_policy)\n",
    "    )\n",
    "    amazon_eks_ebs_csi_driver_policy_arn = response['Policy']['Arn']\n",
    "except iam_client.exceptions.EntityAlreadyExistsException:\n",
    "    amazon_eks_ebs_csi_driver_policy_arn = f'arn:aws:iam::{aws_account_id}:policy/{amazon_eks_ebs_csi_driver_policy_name}'\n",
    "    response = iam_client.get_policy(PolicyArn=amazon_eks_ebs_csi_driver_policy_arn)\n",
    "\n",
    "    # TODO: If policy body is not the same, update. Code to update follows.\n",
    "    # iam_client.create_policy_version(\n",
    "    #     PolicyArn=policy_arn,\n",
    "    #     PolicyDocument=policy_document_json,\n",
    "    #     SetAsDefault=True\n",
    "    # )\n",
    "\n",
    "amazon_eks_ebs_csi_driver_policy_arn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e4e0ebf6-a8ef-42f7-bc4d-ba852964f0da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Role already exists. Updating the trust policy.\n"
     ]
    }
   ],
   "source": [
    "trust_policy = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\n",
    "                \"Federated\": f\"arn:aws:iam::{aws_account_id}:oidc-provider/oidc.eks.{REGION}.amazonaws.com/id/{oidc_id}\"\n",
    "            },\n",
    "            \"Action\": \"sts:AssumeRoleWithWebIdentity\",\n",
    "            \"Condition\": {\n",
    "                \"StringEquals\": {\n",
    "                    f\"oidc.eks.{REGION}.amazonaws.com/id/{oidc_id}:aud\": \"sts.amazonaws.com\",\n",
    "                    f\"oidc.eks.{REGION}.amazonaws.com/id/{oidc_id}:sub\": \"system:serviceaccount:kube-system:ebs-csi-controller-sa\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "try:\n",
    "    create_role_response = iam_client.create_role(\n",
    "        RoleName='AmazonEKS_EBS_CSI_DriverRole',\n",
    "        \n",
    "        AssumeRolePolicyDocument=json.dumps(trust_policy),\n",
    "        Description='Role for Amazon EKS EBS CSI Driver'\n",
    "    )\n",
    "    print(f\"Role created successfully: {create_role_response['Role']['Arn']}\")\n",
    "except iam_client.exceptions.EntityAlreadyExistsException:\n",
    "    print(\"Role already exists. Updating the trust policy.\")\n",
    "    iam_client.update_assume_role_policy(\n",
    "        RoleName='AmazonEKS_EBS_CSI_DriverRole',\n",
    "        PolicyDocument=json.dumps(trust_policy)\n",
    "    )\n",
    "\n",
    "attach_policy_response = iam_client.attach_role_policy(\n",
    "    RoleName='AmazonEKS_EBS_CSI_DriverRole',\n",
    "    PolicyArn='arn:aws:iam::aws:policy/service-role/AmazonEBSCSIDriverPolicy'\n",
    ")\n",
    "\n",
    "attach_policy_response = iam_client.attach_role_policy(\n",
    "    RoleName='AmazonEKS_EBS_CSI_DriverRole',\n",
    "    PolicyArn=amazon_eks_ebs_csi_driver_policy_arn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa839d9c-00b3-41f3-ba6b-f70575889176",
   "metadata": {},
   "source": [
    "# Install CSI Driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e2451fff-9fa9-4f93-b6c1-f3b26924ac3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "serviceaccount/ebs-csi-controller-sa created\n"
     ]
    }
   ],
   "source": [
    "!kubectl create sa ebs-csi-controller-sa -n kube-system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fc410c14-878b-4af9-b960-40503d451a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "serviceaccount/ebs-csi-controller-sa annotated\n"
     ]
    }
   ],
   "source": [
    "!kubectl annotate sa ebs-csi-controller-sa -n kube-system eks.amazonaws.com/role-arn=$ebs_csi_driver_role_arn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c15647b1-29f9-445e-a115-68db3fbec3d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "serviceaccount/ebs-csi-controller-sa annotated\n"
     ]
    }
   ],
   "source": [
    "!kubectl annotate sa ebs-csi-controller-sa -n kube-system meta.helm.sh/release-namespace=kube-system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "feef5937-e243-4ae6-84ee-5be572ad1c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "serviceaccount/ebs-csi-controller-sa annotated\n"
     ]
    }
   ],
   "source": [
    "!kubectl annotate sa ebs-csi-controller-sa -n kube-system meta.helm.sh/release-name=aws-ebs-csi-driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b0b85917-e62c-42aa-8f71-7a0001265efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "serviceaccount/ebs-csi-controller-sa not labeled\n"
     ]
    }
   ],
   "source": [
    "!kubectl label sa ebs-csi-controller-sa -n kube-system app.kubernetes.io/managed-by=Helm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "65c7ca0d-34d7-4b6f-93cb-f39379235bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Release \"aws-ebs-csi-driver\" has been upgraded. Happy Helming!\n",
      "NAME: aws-ebs-csi-driver\n",
      "LAST DEPLOYED: Tue Mar  4 02:50:41 2025\n",
      "NAMESPACE: kube-system\n",
      "STATUS: deployed\n",
      "REVISION: 2\n",
      "NOTES:\n",
      "To verify that aws-ebs-csi-driver has started, run:\n",
      "\n",
      "    kubectl get pod -n kube-system -l \"app.kubernetes.io/name=aws-ebs-csi-driver,app.kubernetes.io/instance=aws-ebs-csi-driver\"\n",
      "\n",
      "[Deprecation announcement] AWS Snow Family device support for the EBS CSI Driver\n",
      "\n",
      "Support for the EBS CSI Driver on [AWS Snow Family devices](https://aws.amazon.com/snowball/) is deprecated, effective immediately. No further Snow-specific bugfixes or feature requests will be merged. The existing functionality for Snow devices will be removed the 1.43 release of the EBS CSI Driver. This announcement does not affect the support of the EBS CSI Driver on other platforms, such as [Amazon EC2](https://aws.amazon.com/ec2/) or EC2 on [AWS Outposts](https://aws.amazon.com/outposts/). For any questions related to this announcement, please comment on this issue [#2365](https://github.com/kubernetes-sigs/aws-ebs-csi-driver/issues/2365) or open a new issue.\n"
     ]
    }
   ],
   "source": [
    "!helm upgrade --install aws-ebs-csi-driver \\\n",
    "    --namespace kube-system \\\n",
    "    aws-ebs-csi-driver/aws-ebs-csi-driver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f599762-1e35-459c-a195-464eb9bd7f1c",
   "metadata": {},
   "source": [
    "### == End of Code Specific to AWS =="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bcb2b6-3c4b-4d3e-9d6c-2daf8baadfee",
   "metadata": {},
   "source": [
    "# Install Nvidia Device Plugin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2715339c-3b4c-4203-92e0-15b51e2fd75d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "daemonset.apps/nvidia-device-plugin-daemonset unchanged\n"
     ]
    }
   ],
   "source": [
    "!kubectl apply -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/refs/heads/main/deployments/static/nvidia-device-plugin.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5a0d46-7a29-4c21-80d0-1ba0f5f9b5fd",
   "metadata": {},
   "source": [
    "# Apply Helm Charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3b606826-23b1-423b-8a87-b078d28afda9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'vol-0b2f7361fa13c5275'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "volume_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e803a8bf-61c4-422f-8ed9-8c9f04a46916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME: storage\n",
      "LAST DEPLOYED: Tue Mar  4 02:50:45 2025\n",
      "NAMESPACE: default\n",
      "STATUS: deployed\n",
      "REVISION: 1\n",
      "TEST SUITE: None\n"
     ]
    }
   ],
   "source": [
    "!helm install storage /helm/storage/ --set volumeID=$volume_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "66755be9-07cc-4d24-a0f3-bae41cecd91a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME: ingress-nginx\n",
      "LAST DEPLOYED: Tue Mar  4 02:50:46 2025\n",
      "NAMESPACE: ingress-nginx\n",
      "STATUS: deployed\n",
      "REVISION: 1\n",
      "TEST SUITE: None\n",
      "NOTES:\n",
      "The ingress-nginx controller has been installed.\n",
      "It may take a few minutes for the load balancer IP to be available.\n",
      "You can watch the status by running 'kubectl get service --namespace ingress-nginx ingress-nginx-controller --output wide --watch'\n",
      "\n",
      "An example Ingress that makes use of the controller:\n",
      "  apiVersion: networking.k8s.io/v1\n",
      "  kind: Ingress\n",
      "  metadata:\n",
      "    name: example\n",
      "    namespace: foo\n",
      "  spec:\n",
      "    ingressClassName: nginx\n",
      "    rules:\n",
      "      - host: www.example.com\n",
      "        http:\n",
      "          paths:\n",
      "            - pathType: Prefix\n",
      "              backend:\n",
      "                service:\n",
      "                  name: exampleService\n",
      "                  port:\n",
      "                    number: 80\n",
      "              path: /\n",
      "    # This section is only required if TLS is to be enabled for the Ingress\n",
      "    tls:\n",
      "      - hosts:\n",
      "        - www.example.com\n",
      "        secretName: example-tls\n",
      "\n",
      "If TLS is enabled for the Ingress, a Secret containing the certificate and key must also be provided:\n",
      "\n",
      "  apiVersion: v1\n",
      "  kind: Secret\n",
      "  metadata:\n",
      "    name: example-tls\n",
      "    namespace: foo\n",
      "  data:\n",
      "    tls.crt: <base64 encoded cert>\n",
      "    tls.key: <base64 encoded key>\n",
      "  type: kubernetes.io/tls\n"
     ]
    }
   ],
   "source": [
    "!helm install ingress-nginx ingress-nginx/ingress-nginx --namespace ingress-nginx --create-namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b905deee-a92e-4bb3-b23c-789c090924a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !helm upgrade --install lb /helm/lb-ingress/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "17474e33-1f9b-448f-9819-f2b432063da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !helm delete lb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1dfa6262-ddc4-45ff-95ab-90d75fb6c46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Need to wait before applying the ingress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "373f30d2-a911-47ac-99ea-9f402c13db23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Release \"fastapi\" does not exist. Installing it now.\n",
      "NAME: fastapi\n",
      "LAST DEPLOYED: Tue Mar  4 02:51:15 2025\n",
      "NAMESPACE: default\n",
      "STATUS: deployed\n",
      "REVISION: 1\n",
      "TEST SUITE: None\n"
     ]
    }
   ],
   "source": [
    "!helm upgrade --install fastapi /helm/eberron-agent-server/ --set awsAccountId=$aws_account_id --set region=$REGION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "74e4bd90-d8bd-421f-b0cb-9b90ecb046e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Release \"frontend\" does not exist. Installing it now.\n",
      "NAME: frontend\n",
      "LAST DEPLOYED: Tue Mar  4 02:51:17 2025\n",
      "NAMESPACE: default\n",
      "STATUS: deployed\n",
      "REVISION: 1\n",
      "TEST SUITE: None\n"
     ]
    }
   ],
   "source": [
    "!helm upgrade --install frontend /helm/eberron-agent-frontend/ --set awsAccountId=$aws_account_id --set region=$REGION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f0e0d633-e0aa-43d3-96ee-71ebe1163431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Release \"ingress\" has been upgraded. Happy Helming!\n",
      "NAME: ingress\n",
      "LAST DEPLOYED: Tue Mar  4 04:06:14 2025\n",
      "NAMESPACE: default\n",
      "STATUS: deployed\n",
      "REVISION: 10\n",
      "TEST SUITE: None\n"
     ]
    }
   ],
   "source": [
    "!helm upgrade --install ingress /helm/eberron-agent-ingress/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97f5d85-1064-4d39-b205-71fddb658689",
   "metadata": {},
   "source": [
    "# Get URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6ec37f9a-6ffd-4e4e-b9a2-f0955ae0423a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fastapi-677965765c-6cbjw'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Update get_one_running_pod to use staring with names.\n",
    "wait_until(get_one_running_pod, {'prefix': 'fastapi-'}, lambda x: x is not None, timeout=8 * 60)\n",
    "pod_name = get_one_running_pod(prefix='fastapi-')\n",
    "assert pod_name is not None\n",
    "pod_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3cee78c8-55c3-43d9-bd54-258451b2520d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output = !kubectl get service lb --no-headers\n",
    "# fields = re.split(r'\\s+', output[0])\n",
    "# external_url = fields[3]\n",
    "# port = fields[4].split('/')[0].split(':')[0]\n",
    "# external_url, port"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1ba7411d-d1ea-46b9-b6d9-eafe4f8fc546",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ab2781576bfaa46ac9445ed997849abb-1499725304.ca-central-1.elb.amazonaws.com',\n",
       " '80')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "output = !kubectl get service ingress-nginx-controller --namespace ingress-nginx --no-headers\n",
    "fields = re.split(r'\\s+', output[0])\n",
    "external_url = fields[3]\n",
    "port = fields[4].split('/')[0].split(':')[0]\n",
    "external_url, port"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8bf520ca-59d0-427f-8251-bc940cad8e46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://ab2781576bfaa46ac9445ed997849abb-1499725304.ca-central-1.elb.amazonaws.com:80'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = f'http://{external_url}:{port}'\n",
    "url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c7f019-64b1-4b2d-a343-b536fca16633",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "438b3f0b-0cc8-4aad-b796-5e7bb32fd110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Defaulted container \"kubyterlab-llm\" out of: kubyterlab-llm, init-container (init)\n",
    "# True\n",
    "# 1\n",
    "!kubectl exec $pod_name bash -- python3 -c 'import torch; print(torch.cuda.is_available()); print(torch.cuda.device_count())'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6bfde574-66a3-4a21-b26d-0cab8eac0893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory.total [MiB], memory.used [MiB], memory.free [MiB]\n",
      "15360 MiB, 909 MiB, 14187 MiB\n"
     ]
    }
   ],
   "source": [
    "!kubectl exec $pod_name bash -- bash -c 'nvidia-smi --query-gpu=memory.total,memory.used,memory.free --format=csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a3c0a9bf-9d43-4149-ac2c-229059865aaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "In Eberron, there are various languages that reflect culture, geography, and the planes of existence. Some of the common languages include:\n",
       "\n",
       "1. Common: The language of the Five Nations and the language of trade in Khorvaire, known by most of its people.\n",
       "2. Abyssal: The common tongue of all fiends, sometimes called “Khyber’s Speech.”\n",
       "3. Celestial: The tongue of Siberys, used by archons of Shavarath'.\n",
       "4. Draconic: Used by dragons, kobolds, troglodytes, lizardfolk, and other draconic creatures.\n",
       "5. Elven: Used by elves and drow.\n",
       "6. Dwarven: Used by dwarves.\n",
       "7. Goblin: The trade language of the goblin empire of Dhakaan and survives as the primary language in Darguun, Droaam, and the Shadow Marches.\n",
       "8. Halfling: The language of halflings.\n",
       "9. Ignan: The language of fire-based creatures.\n",
       "10. Infernal: The language of devils of Shavarath'.\n",
       "11. Kythric: The language of Slaadi, chaotic outsiders.\n",
       "12. Mabran: The language of Nightshades, shadows, and Draconic creatures of Mabar.\n",
       "13. Ore: The language of Orcs.\n",
       "14. Quori: The language of Quori, the Inspired, and kalashtar.\n",
       "15. Risian: The language of ice-based creatures.\n",
       "16. Sylvan: The language of dryads, eladrins, and creatures of Thelanis.\n",
       "17. Terran: The language of Xorns and other earth-based creatures.\n",
       "18. Undercommon: The language of chokers and underground Daelkyr denizens.\n",
       "\n",
       "These languages have their own unique alphabets and scripts, and some languages are more commonly spoken in certain regions or among certain races. The historical development of languages and cultures also plays a role in the scripts used to write various languages. For example, the Orc language is written using the Goblin script, because the orcs of Khorvaire learned writing from the goblins."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "from IPython.display import clear_output, display, HTML, Markdown\n",
    "\n",
    "response = requests.post(f\"{url}/respond\", \n",
    "                         headers={\"Content-Type\": \"application/json\", \n",
    "                                  \"Accept\": \"text/event-stream\"}, \n",
    "                         json={\"content\": \"What are the languages in Eberron?\"}, \n",
    "                         stream=True)\n",
    "md = \"\"\n",
    "for chunk in response:\n",
    "    md += chunk.decode(\"utf-8\")\n",
    "    clear_output()\n",
    "    display(Markdown(md.replace('\\\\n', '\\n')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4ae1ff-9361-4709-8343-2b2310999403",
   "metadata": {},
   "source": [
    "# == End of Procedure =="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54d53ff-124d-409d-95f3-6985f7382654",
   "metadata": {},
   "source": [
    "## == Do Not Continue: rest of the code is only for reference for troubleshooting =="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bd4f0a-fa1d-4578-89d6-84b9c30c2179",
   "metadata": {},
   "source": [
    "# Troubleshoot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2af3512-4723-4393-8d3a-56bdc6fa64f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# curl -X POST -H \"Content-Type: application/json\" -H \"Accept: text/event-stream\" -d '{\"key\":\"value\"}' http://your-api-endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f10cb1-1b0f-4212-a689-052928a4924d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that for the following to work, you need to attach the policy AmazonSSMManagedInstanceCore, in the relevant cell above, during the deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "44b4b8fb-d7fb-4c9c-87ba-777bf78f1f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = !aws ssm send-command \\\n",
    "    --document-name \"AWS-RunShellScript\" \\\n",
    "    --targets \"Key=instanceIds,Values=i-02f3c1f8a118eb352\" \\\n",
    "    --parameters 'commands=[\"nvidia-smi --query-gpu=memory.total,memory.used,memory.free --format=csv\"]' \\\n",
    "    --region $REGION\n",
    "command_id = json.loads(''.join(output))['Command']['CommandId']\n",
    "command_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45b62bf-644f-41ce-a4bf-f745311de116",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = !aws ssm list-command-invocations \\\n",
    "    --command-id $command_id \\\n",
    "    --details\n",
    "output = json.loads(''.join(output))['CommandInvocations'][0]['CommandPlugins'][0]['Output']\n",
    "for line in output.split('\\n'):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0480a8c3-9aad-4a76-9f62-20de23589501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAST SEEN   TYPE      REASON                    OBJECT                                             MESSAGE\n",
      "38m         Normal    Starting                  node/ip-10-0-1-19.ca-central-1.compute.internal    \n",
      "39m         Normal    Starting                  node/ip-10-0-1-118.ca-central-1.compute.internal   \n",
      "38m         Normal    Starting                  node/ip-10-0-1-35.ca-central-1.compute.internal    \n",
      "39m         Normal    NodeAllocatableEnforced   node/ip-10-0-1-118.ca-central-1.compute.internal   Updated Node Allocatable limit across pods\n",
      "39m         Normal    NodeHasSufficientPID      node/ip-10-0-1-118.ca-central-1.compute.internal   Node ip-10-0-1-118.ca-central-1.compute.internal status is now: NodeHasSufficientPID\n",
      "39m         Normal    NodeHasNoDiskPressure     node/ip-10-0-1-118.ca-central-1.compute.internal   Node ip-10-0-1-118.ca-central-1.compute.internal status is now: NodeHasNoDiskPressure\n",
      "39m         Normal    NodeHasSufficientMemory   node/ip-10-0-1-118.ca-central-1.compute.internal   Node ip-10-0-1-118.ca-central-1.compute.internal status is now: NodeHasSufficientMemory\n",
      "39m         Warning   InvalidDiskCapacity       node/ip-10-0-1-118.ca-central-1.compute.internal   invalid capacity 0 on image filesystem\n",
      "39m         Warning   CgroupV1                  node/ip-10-0-1-118.ca-central-1.compute.internal   Cgroup v1 support is in maintenance mode, please migrate to Cgroup v2.\n",
      "39m         Normal    Starting                  node/ip-10-0-1-118.ca-central-1.compute.internal   Starting kubelet.\n",
      "39m         Normal    Synced                    node/ip-10-0-1-118.ca-central-1.compute.internal   Node synced successfully\n",
      "39m         Normal    RegisteredNode            node/ip-10-0-1-118.ca-central-1.compute.internal   Node ip-10-0-1-118.ca-central-1.compute.internal event: Registered Node ip-10-0-1-118.ca-central-1.compute.internal in Controller\n",
      "39m         Normal    NodeReady                 node/ip-10-0-1-118.ca-central-1.compute.internal   Node ip-10-0-1-118.ca-central-1.compute.internal status is now: NodeReady\n",
      "38m         Warning   InvalidDiskCapacity       node/ip-10-0-1-35.ca-central-1.compute.internal    invalid capacity 0 on image filesystem\n",
      "38m         Normal    Starting                  node/ip-10-0-1-35.ca-central-1.compute.internal    Starting kubelet.\n",
      "38m         Warning   CgroupV1                  node/ip-10-0-1-35.ca-central-1.compute.internal    Cgroup v1 support is in maintenance mode, please migrate to Cgroup v2.\n",
      "38m         Normal    NodeHasSufficientMemory   node/ip-10-0-1-35.ca-central-1.compute.internal    Node ip-10-0-1-35.ca-central-1.compute.internal status is now: NodeHasSufficientMemory\n",
      "38m         Normal    NodeHasNoDiskPressure     node/ip-10-0-1-35.ca-central-1.compute.internal    Node ip-10-0-1-35.ca-central-1.compute.internal status is now: NodeHasNoDiskPressure\n",
      "38m         Normal    NodeHasSufficientPID      node/ip-10-0-1-35.ca-central-1.compute.internal    Node ip-10-0-1-35.ca-central-1.compute.internal status is now: NodeHasSufficientPID\n",
      "38m         Normal    NodeAllocatableEnforced   node/ip-10-0-1-35.ca-central-1.compute.internal    Updated Node Allocatable limit across pods\n",
      "38m         Normal    Synced                    node/ip-10-0-1-35.ca-central-1.compute.internal    Node synced successfully\n",
      "38m         Warning   InvalidDiskCapacity       node/ip-10-0-1-19.ca-central-1.compute.internal    invalid capacity 0 on image filesystem\n",
      "38m         Normal    Starting                  node/ip-10-0-1-19.ca-central-1.compute.internal    Starting kubelet.\n",
      "38m         Warning   CgroupV1                  node/ip-10-0-1-19.ca-central-1.compute.internal    Cgroup v1 support is in maintenance mode, please migrate to Cgroup v2.\n",
      "38m         Normal    Synced                    node/ip-10-0-1-19.ca-central-1.compute.internal    Node synced successfully\n",
      "38m         Normal    NodeHasSufficientMemory   node/ip-10-0-1-19.ca-central-1.compute.internal    Node ip-10-0-1-19.ca-central-1.compute.internal status is now: NodeHasSufficientMemory\n",
      "38m         Normal    NodeHasNoDiskPressure     node/ip-10-0-1-19.ca-central-1.compute.internal    Node ip-10-0-1-19.ca-central-1.compute.internal status is now: NodeHasNoDiskPressure\n",
      "38m         Normal    NodeHasSufficientPID      node/ip-10-0-1-19.ca-central-1.compute.internal    Node ip-10-0-1-19.ca-central-1.compute.internal status is now: NodeHasSufficientPID\n",
      "38m         Normal    NodeAllocatableEnforced   node/ip-10-0-1-19.ca-central-1.compute.internal    Updated Node Allocatable limit across pods\n",
      "38m         Normal    RegisteredNode            node/ip-10-0-1-35.ca-central-1.compute.internal    Node ip-10-0-1-35.ca-central-1.compute.internal event: Registered Node ip-10-0-1-35.ca-central-1.compute.internal in Controller\n",
      "38m         Normal    RegisteredNode            node/ip-10-0-1-19.ca-central-1.compute.internal    Node ip-10-0-1-19.ca-central-1.compute.internal event: Registered Node ip-10-0-1-19.ca-central-1.compute.internal in Controller\n",
      "38m         Warning   ProvisioningFailed        persistentvolumeclaim/pvc-llm                      storageclass.storage.k8s.io \"manual\" not found\n",
      "38m         Normal    NodeReady                 node/ip-10-0-1-35.ca-central-1.compute.internal    Node ip-10-0-1-35.ca-central-1.compute.internal status is now: NodeReady\n",
      "38m         Normal    NodeReady                 node/ip-10-0-1-19.ca-central-1.compute.internal    Node ip-10-0-1-19.ca-central-1.compute.internal status is now: NodeReady\n",
      "38m         Normal    Scheduled                 pod/fastapi-677965765c-6cbjw                       Successfully assigned default/fastapi-677965765c-6cbjw to ip-10-0-1-118.ca-central-1.compute.internal\n",
      "38m         Normal    SuccessfulCreate          replicaset/fastapi-677965765c                      Created pod: fastapi-677965765c-6cbjw\n",
      "38m         Normal    ScalingReplicaSet         deployment/fastapi                                 Scaled up replica set fastapi-677965765c to 1\n",
      "38m         Normal    ScalingReplicaSet         deployment/frontend                                Scaled up replica set frontend-7dc87ddddd to 1\n",
      "38m         Normal    SuccessfulCreate          replicaset/frontend-7dc87ddddd                     Created pod: frontend-7dc87ddddd-6x8q8\n",
      "38m         Normal    Pulling                   pod/frontend-7dc87ddddd-6x8q8                      Pulling image \"275678099358.dkr.ecr.ca-central-1.amazonaws.com/multi-agent/eberron-agent-frontend:25.02\"\n",
      "38m         Normal    Scheduled                 pod/frontend-7dc87ddddd-6x8q8                      Successfully assigned default/frontend-7dc87ddddd-6x8q8 to ip-10-0-1-19.ca-central-1.compute.internal\n",
      "38m         Normal    SuccessfulAttachVolume    pod/fastapi-677965765c-6cbjw                       AttachVolume.Attach succeeded for volume \"pv-llm\"\n",
      "38m         Normal    Pulling                   pod/fastapi-677965765c-6cbjw                       Pulling image \"275678099358.dkr.ecr.ca-central-1.amazonaws.com/multi-agent/eberron-agent-server:25.02\"\n",
      "38m         Normal    Pulled                    pod/frontend-7dc87ddddd-6x8q8                      Successfully pulled image \"275678099358.dkr.ecr.ca-central-1.amazonaws.com/multi-agent/eberron-agent-frontend:25.02\" in 12.418s (12.418s including waiting). Image size: 292894546 bytes.\n",
      "38m         Normal    Created                   pod/frontend-7dc87ddddd-6x8q8                      Created container frontend\n",
      "38m         Normal    Started                   pod/frontend-7dc87ddddd-6x8q8                      Started container frontend\n",
      "32m         Normal    Created                   pod/fastapi-677965765c-6cbjw                       Created container fastapi\n",
      "32m         Normal    Pulled                    pod/fastapi-677965765c-6cbjw                       Successfully pulled image \"275678099358.dkr.ecr.ca-central-1.amazonaws.com/multi-agent/eberron-agent-server:25.02\" in 5m32.478s (5m32.478s including waiting). Image size: 14071664304 bytes.\n",
      "32m         Normal    Started                   pod/fastapi-677965765c-6cbjw                       Started container fastapi\n"
     ]
    }
   ],
   "source": [
    "!kubectl get events --sort-by='.lastTimestamp'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d39f876b-ea67-4dd5-a9ee-92af8f218ce9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'instance_id' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[94], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43minstance_id\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'instance_id' is not defined"
     ]
    }
   ],
   "source": [
    "instance_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a8992595-dd14-4067-8607-3596e7fa746c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                  READY   STATUS    RESTARTS   AGE\n",
      "ebs-csi-controller-6cd4bccd45-272bl   5/5     Running   0          38m\n",
      "ebs-csi-controller-6cd4bccd45-g4vws   5/5     Running   0          38m\n",
      "ebs-csi-node-lnggw                    3/3     Running   0          38m\n",
      "ebs-csi-node-psgw7                    3/3     Running   0          38m\n",
      "ebs-csi-node-qtmfb                    3/3     Running   0          38m\n"
     ]
    }
   ],
   "source": [
    "!kubectl get pod -n kube-system -l \"app.kubernetes.io/name=aws-ebs-csi-driver,app.kubernetes.io/instance=aws-ebs-csi-driver\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3b9460f8-e4d4-4247-908f-22706f2d876d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                          GPU\n",
      "ip-10-0-1-118.ca-central-1.compute.internal   1\n",
      "ip-10-0-1-19.ca-central-1.compute.internal    <none>\n",
      "ip-10-0-1-35.ca-central-1.compute.internal    <none>\n"
     ]
    }
   ],
   "source": [
    "# NAME                                          GPU\n",
    "# ip-10-0-1-129.ca-central-1.compute.internal   <none>\n",
    "# ip-10-0-1-223.ca-central-1.compute.internal   <none>\n",
    "# ip-10-0-1-8.ca-central-1.compute.internal     1\n",
    "!kubectl get nodes \"-o=custom-columns=NAME:.metadata.name,GPU:.status.allocatable.nvidia\\.com/gpu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5d4e2db7-eec8-484d-af50-ff23a339c8f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                          TAINTS\n",
      "ip-10-0-1-118.ca-central-1.compute.internal   [map[effect:NoSchedule key:nvidia.com/gpu value:true]]\n",
      "ip-10-0-1-19.ca-central-1.compute.internal    <none>\n",
      "ip-10-0-1-35.ca-central-1.compute.internal    <none>\n"
     ]
    }
   ],
   "source": [
    "# NAME                                          TAINTS\n",
    "# ip-10-0-1-107.ca-central-1.compute.internal   <none>\n",
    "# ip-10-0-1-175.ca-central-1.compute.internal   [map[effect:NoSchedule key:nvidia.com/gpu value:true]]\n",
    "# ip-10-0-1-90.ca-central-1.compute.internal    <none>\n",
    "!kubectl get nodes -o custom-columns=NAME:.metadata.name,TAINTS:.spec.taints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "35f0a916-0089-4030-97aa-a074fe340403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE\n",
      "fastapi      ClusterIP   172.20.116.75   <none>        80/TCP    38m\n",
      "frontend     ClusterIP   172.20.41.240   <none>        80/TCP    38m\n",
      "kubernetes   ClusterIP   172.20.0.1      <none>        443/TCP   42m\n"
     ]
    }
   ],
   "source": [
    "# NAME                     TYPE           CLUSTER-IP     EXTERNAL-IP                                                                  PORT(S)          AGE\n",
    "# kubernetes               ClusterIP      172.20.0.1     <none>                                                                       443/TCP          37m\n",
    "# kubyterlab-llm-service   LoadBalancer   172.20.129.8   a4740e3e56bfe40ac81121bd46071903-1377611187.ca-central-1.elb.amazonaws.com   8888:31434/TCP   13s\n",
    "!kubectl get service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3cf151d7-7c1f-4bf8-963b-b510e1affaca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                        READY   STATUS    RESTARTS   AGE\n",
      "fastapi-677965765c-6cbjw    1/1     Running   0          38m\n",
      "frontend-7dc87ddddd-6x8q8   1/1     Running   0          38m\n"
     ]
    }
   ],
   "source": [
    "!kubectl get pod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fde9f7f4-b4cc-4834-9ccc-e69f70ffbbad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME       READY   UP-TO-DATE   AVAILABLE   AGE\n",
      "fastapi    1/1     1            1           104m\n",
      "frontend   1/1     1            1           104m\n"
     ]
    }
   ],
   "source": [
    "# NAME               READY   UP-TO-DATE   AVAILABLE   AGE\n",
    "# kubyterlab-llm-pod   1/1     1            1           31m\n",
    "!kubectl get deployments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5663728c-8285-4265-bb0f-eef82a989578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No resources found in default namespace.\n"
     ]
    }
   ],
   "source": [
    "!kubectl get ingress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "54de0206-648b-48ef-a2f0-ccca684a3293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME              STATUS   AGE\n",
      "default           Active   78m\n",
      "ingress-nginx     Active   75m\n",
      "kube-node-lease   Active   78m\n",
      "kube-public       Active   78m\n",
      "kube-system       Active   78m\n"
     ]
    }
   ],
   "source": [
    "!kubectl get namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5a81be83-efd5-4afe-8e91-45f1bfa25703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME      CLASS   HOSTS       ADDRESS                                                                      PORTS   AGE     LABELS\n",
      "ingress   nginx   localhost   a16eb9ebc41444b9c8b8a9f27f4f47c7-1630223309.ca-central-1.elb.amazonaws.com   80      2m42s   app.kubernetes.io/managed-by=Helm\n"
     ]
    }
   ],
   "source": [
    "!kubectl get ingress --show-labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3f118af3-dd86-43fd-b6b3-ead78a96dbb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                 TYPE           CLUSTER-IP      EXTERNAL-IP                                                                  PORT(S)                      AGE\n",
      "ingress-nginx-controller             LoadBalancer   172.20.74.194   a16eb9ebc41444b9c8b8a9f27f4f47c7-1630223309.ca-central-1.elb.amazonaws.com   80:31683/TCP,443:30812/TCP   107m\n",
      "ingress-nginx-controller-admission   ClusterIP      172.20.15.158   <none>                                                                       443/TCP                      107m\n"
     ]
    }
   ],
   "source": [
    "!kubectl get svc -n ingress-nginx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c6834900-1ce4-4684-ae72-878f87519318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                          STATUS   ROLES    AGE   VERSION\n",
      "ip-10-0-1-118.ca-central-1.compute.internal   Ready    <none>   84m   v1.31.5-eks-5d632ec\n",
      "ip-10-0-1-19.ca-central-1.compute.internal    Ready    <none>   83m   v1.31.5-eks-5d632ec\n",
      "ip-10-0-1-35.ca-central-1.compute.internal    Ready    <none>   83m   v1.31.5-eks-5d632ec\n"
     ]
    }
   ],
   "source": [
    "!kubectl get nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c1f58371-742b-41ae-a44e-f9d9dcb6f977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I0226 19:21:54.285668       7 controller.go:196] \"Configuration changes detected, backend reload required\"\n",
      "I0226 19:21:54.385355       7 controller.go:216] \"Backend successfully reloaded\"\n",
      "I0226 19:21:54.385954       7 event.go:377] Event(v1.ObjectReference{Kind:\"Pod\", Namespace:\"ingress-nginx\", Name:\"ingress-nginx-controller-7657f6db5f-kxrd8\", UID:\"8cf97803-4eea-4e08-a9d8-b7873de2222b\", APIVersion:\"v1\", ResourceVersion:\"3848\", FieldPath:\"\"}): type: 'Normal' reason: 'RELOAD' NGINX reload triggered due to a change in configuration\n",
      "2025/02/26 19:22:01 [error] 99#99: *42838 connect() failed (111: Connection refused) while connecting to upstream, client: 10.0.1.216, server: _, request: \"GET / HTTP/1.1\", upstream: \"http://10.0.1.4:4200/\", host: \"a16eb9ebc41444b9c8b8a9f27f4f47c7-1630223309.ca-central-1.elb.amazonaws.com\", referrer: \"http://localhost:8890/\"\n",
      "2025/02/26 19:22:01 [error] 99#99: *42838 connect() failed (111: Connection refused) while connecting to upstream, client: 10.0.1.216, server: _, request: \"GET / HTTP/1.1\", upstream: \"http://10.0.1.4:4200/\", host: \"a16eb9ebc41444b9c8b8a9f27f4f47c7-1630223309.ca-central-1.elb.amazonaws.com\", referrer: \"http://localhost:8890/\"\n",
      "2025/02/26 19:22:01 [error] 99#99: *42838 connect() failed (111: Connection refused) while connecting to upstream, client: 10.0.1.216, server: _, request: \"GET / HTTP/1.1\", upstream: \"http://10.0.1.4:4200/\", host: \"a16eb9ebc41444b9c8b8a9f27f4f47c7-1630223309.ca-central-1.elb.amazonaws.com\", referrer: \"http://localhost:8890/\"\n",
      "10.0.1.216 - - [26/Feb/2025:19:22:01 +0000] \"GET / HTTP/1.1\" 502 552 \"http://localhost:8890/\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/133.0.0.0 Safari/537.36\" 548 0.000 [default-frontend-4200] [] 10.0.1.4:4200, 10.0.1.4:4200, 10.0.1.4:4200 0, 0, 0 0.001, 0.000, 0.000 502, 502, 502 8ecb848dd6b650d605db1cf8a8f5c1ca\n",
      "I0226 19:22:12.107667       7 status.go:304] \"updating Ingress status\" namespace=\"default\" ingress=\"ingress\" currentValue=null newValue=[{\"hostname\":\"a16eb9ebc41444b9c8b8a9f27f4f47c7-1630223309.ca-central-1.elb.amazonaws.com\"}]\n",
      "I0226 19:22:12.118074       7 event.go:377] Event(v1.ObjectReference{Kind:\"Ingress\", Namespace:\"default\", Name:\"ingress\", UID:\"7cf3890e-d612-4492-8ca0-ccd48ee3d1c0\", APIVersion:\"networking.k8s.io/v1\", ResourceVersion:\"26544\", FieldPath:\"\"}): type: 'Normal' reason: 'Sync' Scheduled for sync\n",
      "10.0.1.110 - - [26/Feb/2025:19:23:57 +0000] \"POST /respond HTTP/1.1\" 200 3042 \"-\" \"python-requests/2.32.3\" 328 107.191 [default-fastapi-8000] [] 10.0.1.120:8000 3042 107.191 200 f01e2ee5c9e828650291a8ca13b95bdc\n"
     ]
    }
   ],
   "source": [
    "!kubectl logs -l app.kubernetes.io/name=ingress-nginx -n ingress-nginx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "191dac73-b376-4f60-9792-a12df4721e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME              \tNAMESPACE    \tREVISION\tUPDATED                                \tSTATUS  \tCHART                    \tAPP VERSION\n",
      "aws-ebs-csi-driver\tkube-system  \t1       \t2025-02-26 17:25:46.294689009 +0000 UTC\tdeployed\taws-ebs-csi-driver-2.40.1\t1.40.0     \n",
      "fastapi           \tdefault      \t4       \t2025-02-26 19:37:58.362187382 +0000 UTC\tdeployed\tfastapi-0.1.0            \t25.02      \n",
      "frontend          \tdefault      \t5       \t2025-02-26 19:38:00.37656958 +0000 UTC \tdeployed\tfrontend-0.1.0           \t25.02      \n",
      "ingress           \tdefault      \t8       \t2025-02-26 19:37:56.093484512 +0000 UTC\tdeployed\tingress-0.1.0            \t25.02      \n",
      "ingress-nginx     \tingress-nginx\t1       \t2025-02-26 17:36:55.117771706 +0000 UTC\tdeployed\tingress-nginx-4.12.0     \t1.12.0     \n",
      "storage           \tdefault      \t1       \t2025-02-26 17:36:53.654556216 +0000 UTC\tdeployed\tstorage-0.1.0            \t           \n"
     ]
    }
   ],
   "source": [
    "!helm list -A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8b31af91-2cff-4bd4-811d-a405ccbe7939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER-SUPPLIED VALUES:\n",
      "null\n"
     ]
    }
   ],
   "source": [
    "!helm get values ingress-nginx -n ingress-nginx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2621d2b0-aa9a-43e1-a7ef-1b07c1be73c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME    CONTROLLER             PARAMETERS   AGE\n",
      "nginx   k8s.io/ingress-nginx   <none>       108m\n"
     ]
    }
   ],
   "source": [
    "!kubectl get ingressclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "0d727659-1be2-45c8-97db-48133f64865f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:             ingress\n",
      "Labels:           app.kubernetes.io/managed-by=Helm\n",
      "Namespace:        default\n",
      "Address:          ab2781576bfaa46ac9445ed997849abb-1499725304.ca-central-1.elb.amazonaws.com\n",
      "Ingress Class:    nginx\n",
      "Default backend:  <default>\n",
      "Rules:\n",
      "  Host        Path  Backends\n",
      "  ----        ----  --------\n",
      "  *           \n",
      "              /respond   fastapi:80 (10.0.1.216:8000)\n",
      "              /          frontend:80 (10.0.1.159:8501)\n",
      "Annotations:  meta.helm.sh/release-name: ingress\n",
      "              meta.helm.sh/release-namespace: default\n",
      "              nginx.ingress.kubernetes.io/cors-allow-methods: POST,GET,PUT,DELETE,OPTIONS\n",
      "              nginx.ingress.kubernetes.io/enable-methods: POST,GET,PUT,DELETE,OPTIONS\n",
      "              nginx.ingress.kubernetes.io/proxy-read-timeout: 3600\n",
      "              nginx.ingress.kubernetes.io/proxy-send-timeout: 3600\n",
      "Events:\n",
      "  Type    Reason  Age                From                      Message\n",
      "  ----    ------  ----               ----                      -------\n",
      "  Normal  Sync    15m (x2 over 15m)  nginx-ingress-controller  Scheduled for sync\n"
     ]
    }
   ],
   "source": [
    "!kubectl describe ingress ingress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "787d64b3-aded-45b9-bcb5-cfcddb81ef33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'frontend-7dc87ddddd-6x8q8'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wait_until(get_one_running_pod, {'prefix': 'frontend-'}, lambda x: x is                                       not None, timeout=8 * 60)\n",
    "frontend_pod_name = get_one_running_pod(prefix='frontend-')\n",
    "assert frontend_pod_name is not None\n",
    "frontend_pod_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2ca977a7-493f-428d-a15f-d142beb64492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
      "\n",
      "\n",
      "  You can now view your Streamlit app in your browser.\n",
      "\n",
      "  Local URL: http://localhost:8501\n",
      "  Network URL: http://10.0.1.139:8501\n",
      "  External URL: http://15.223.2.202:8501\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!kubectl logs $frontend_pod_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b715f925-0cee-4bf1-9368-f39e45740e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pod \"frontend-7dc87ddddd-6x8q8\" deleted\n"
     ]
    }
   ],
   "source": [
    "!kubectl delete pod $frontend_pod_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e459b1a6-a8cd-408f-9e24-5b9a810940b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                          STATUS   ROLES    AGE   VERSION\n",
      "ip-10-0-1-118.ca-central-1.compute.internal   Ready    <none>   93m   v1.31.5-eks-5d632ec\n",
      "ip-10-0-1-19.ca-central-1.compute.internal    Ready    <none>   92m   v1.31.5-eks-5d632ec\n",
      "ip-10-0-1-35.ca-central-1.compute.internal    Ready    <none>   92m   v1.31.5-eks-5d632ec\n"
     ]
    }
   ],
   "source": [
    "!kubectl get nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f63db4e1-29e3-479e-8db9-13b6b2a1c8fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ingress-nginx-controller-7657f6db5f-sc4l6'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = !kubectl get pods -n ingress-nginx --no-headers\n",
    "fields = re.split(r'\\s+', output[0])\n",
    "ingress_controller_pod = fields[0]\n",
    "ingress_controller_pod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a6b04770-c2d7-400f-aa14-4ffed17a9258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Host frontend.default.svc.cluster.local:80 was resolved.\n",
      "* IPv6: (none)\n",
      "* IPv4: 172.20.41.240\n",
      "*   Trying 172.20.41.240:80...\n",
      "* Connected to frontend.default.svc.cluster.local (172.20.41.240) port 80\n",
      "* using HTTP/1.x\n",
      "> GET / HTTP/1.1\n",
      "> Host: frontend.default.svc.cluster.local\n",
      "> User-Agent: curl/8.11.1\n",
      "> Accept: */*\n",
      "> \n",
      "* Request completely sent off\n",
      "< HTTP/1.1 200 OK\n",
      "< Server: TornadoServer/6.4.2\n",
      "< Content-Type: text/html\n",
      "< Date: Tue, 04 Mar 2025 04:12:02 GMT\n",
      "< Accept-Ranges: bytes\n",
      "< Etag: \"dbab17a21eaa5481e5ea74ed9b2a58f9cddc1fa1c62651ba7aaaf6016c1781d9d3404eda0c9aa965c79e88d417929d1dfdda09325d0e845b3b42b7b0b6a642cd\"\n",
      "< Last-Modified: Thu, 27 Feb 2025 03:24:13 GMT\n",
      "< Cache-Control: no-cache\n",
      "< Content-Length: 1837\n",
      "< Vary: Accept-Encoding\n",
      "< \n",
      "<!--\n",
      " Copyright (c) Streamlit Inc. (2018-2022) Snowflake Inc. (2022-2025)\n",
      "\n",
      " Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      " you may not use this file except in compliance with the License.\n",
      " You may obtain a copy of the License at\n",
      "\n",
      "     http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      " Unless required by applicable law or agreed to in writing, software\n",
      " distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      " WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      " See the License for the specific language governing permissions and\n",
      " limitations under the License.\n",
      "-->\n",
      "\n",
      "<!DOCTYPE html>\n",
      "<html lang=\"en\">\n",
      "  <head>\n",
      "    <meta charset=\"UTF-8\" />\n",
      "    <meta\n",
      "      name=\"viewport\"\n",
      "      content=\"width=device-width, initial-scale=1, shrink-to-fit=no\"\n",
      "    />\n",
      "    <link rel=\"shortcut icon\" href=\"./favicon.png\" />\n",
      "    <link\n",
      "      rel=\"preload\"\n",
      "      href=\"./static/media/SourceSansPro-Regular.DZLUzqI4.woff2\"\n",
      "      as=\"font\"\n",
      "      type=\"font/woff2\"\n",
      "      crossorigin\n",
      "    />\n",
      "    <link\n",
      "      rel=\"preload\"\n",
      "      href=\"./static/media/SourceSansPro-SemiBold.sKQIyTMz.woff2\"\n",
      "      as=\"font\"\n",
      "      type=\"font/woff2\"\n",
      "      crossorigin\n",
      "    />\n",
      "    <link\n",
      "      rel=\"preload\"\n",
      "      href=\"./static/media/SourceSansPro-Bold.-6c9oR8J.woff2\"\n",
      "      as=\"font\"\n",
      "      type=\"font/woff2\"\n",
      "      crossorigin\n",
      "    />\n",
      "\n",
      "    <title>Streamlit</title>\n",
      "\n",
      "    <!-- initialize window.prerenderReady to false and then set to true in React app when app is ready for indexing -->\n",
      "    <script>\n",
      "      window.prerenderReady = false\n",
      "    </script>\n",
      "    <script type=\"module\" crossorigin src=\"./static/js/index.tKq1MI69.js\"></script>\n",
      "    <link rel=\"stylesheet\" crossorigin href=\"./static/css/index.mUTQuMqR.css\">\n",
      "  </head>\n",
      "  <body>\n",
      "    <noscript>You need to enable JavaScript to run this app.</noscript>\n",
      "    <div id=\"root\"></div>\n",
      "  </body>\n",
      "</html>\n",
      "* Connection #0 to host frontend.default.svc.cluster.local left intact\n"
     ]
    }
   ],
   "source": [
    "!kubectl exec -it $ingress_controller_pod -n ingress-nginx -- curl -v frontend.default.svc.cluster.local:80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "79853d58-a743-441b-971c-218dea063d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Host frontend.default.svc.cluster.local:80 was resolved.\n",
      "* IPv6: (none)\n",
      "* IPv4: 172.20.41.240\n",
      "*   Trying 172.20.41.240:80...\n",
      "* Connected to frontend.default.svc.cluster.local (172.20.41.240) port 80\n",
      "* using HTTP/1.x\n",
      "> GET / HTTP/1.1\n",
      "> Host: frontend.default.svc.cluster.local\n",
      "> User-Agent: curl/8.11.1\n",
      "> Accept: */*\n",
      "> \n",
      "* Request completely sent off\n",
      "< HTTP/1.1 200 OK\n",
      "< Server: TornadoServer/6.4.2\n",
      "< Content-Type: text/html\n",
      "< Date: Tue, 04 Mar 2025 04:12:03 GMT\n",
      "< Accept-Ranges: bytes\n",
      "< Etag: \"dbab17a21eaa5481e5ea74ed9b2a58f9cddc1fa1c62651ba7aaaf6016c1781d9d3404eda0c9aa965c79e88d417929d1dfdda09325d0e845b3b42b7b0b6a642cd\"\n",
      "< Last-Modified: Thu, 27 Feb 2025 03:24:13 GMT\n",
      "< Cache-Control: no-cache\n",
      "< Content-Length: 1837\n",
      "< Vary: Accept-Encoding\n",
      "< \n",
      "<!--\n",
      " Copyright (c) Streamlit Inc. (2018-2022) Snowflake Inc. (2022-2025)\n",
      "\n",
      " Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      " you may not use this file except in compliance with the License.\n",
      " You may obtain a copy of the License at\n",
      "\n",
      "     http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      " Unless required by applicable law or agreed to in writing, software\n",
      " distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      " WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      " See the License for the specific language governing permissions and\n",
      " limitations under the License.\n",
      "-->\n",
      "\n",
      "<!DOCTYPE html>\n",
      "<html lang=\"en\">\n",
      "  <head>\n",
      "    <meta charset=\"UTF-8\" />\n",
      "    <meta\n",
      "      name=\"viewport\"\n",
      "      content=\"width=device-width, initial-scale=1, shrink-to-fit=no\"\n",
      "    />\n",
      "    <link rel=\"shortcut icon\" href=\"./favicon.png\" />\n",
      "    <link\n",
      "      rel=\"preload\"\n",
      "      href=\"./static/media/SourceSansPro-Regular.DZLUzqI4.woff2\"\n",
      "      as=\"font\"\n",
      "      type=\"font/woff2\"\n",
      "      crossorigin\n",
      "    />\n",
      "    <link\n",
      "      rel=\"preload\"\n",
      "      href=\"./static/media/SourceSansPro-SemiBold.sKQIyTMz.woff2\"\n",
      "      as=\"font\"\n",
      "      type=\"font/woff2\"\n",
      "      crossorigin\n",
      "    />\n",
      "    <link\n",
      "      rel=\"preload\"\n",
      "      href=\"./static/media/SourceSansPro-Bold.-6c9oR8J.woff2\"\n",
      "      as=\"font\"\n",
      "      type=\"font/woff2\"\n",
      "      crossorigin\n",
      "    />\n",
      "\n",
      "    <title>Streamlit</title>\n",
      "\n",
      "    <!-- initialize window.prerenderReady to false and then set to true in React app when app is ready for indexing -->\n",
      "    <script>\n",
      "      window.prerenderReady = false\n",
      "    </script>\n",
      "    <script type=\"module\" crossorigin src=\"./static/js/index.tKq1MI69.js\"></script>\n",
      "    <link rel=\"stylesheet\" crossorigin href=\"./static/css/index.mUTQuMqR.css\">\n",
      "  </head>\n",
      "  <body>\n",
      "    <noscript>You need to enable JavaScript to run this app.</noscript>\n",
      "    <div id=\"root\"></div>\n",
      "  </body>\n",
      "</html>\n",
      "* Connection #0 to host frontend.default.svc.cluster.local left intact\n"
     ]
    }
   ],
   "source": [
    "!kubectl exec -it $ingress_controller_pod -n ingress-nginx -- curl -v frontend.default.svc.cluster.local:80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "048615e9-b5c6-4cc8-abfc-0446ed55c1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Host fastapi.default.svc.cluster.local:80 was resolved.\n",
      "* IPv6: (none)\n",
      "* IPv4: 172.20.116.75\n",
      "*   Trying 172.20.116.75:80...\n",
      "* Connected to fastapi.default.svc.cluster.local (172.20.116.75) port 80\n",
      "* using HTTP/1.x\n",
      "> GET / HTTP/1.1\n",
      "> Host: fastapi.default.svc.cluster.local\n",
      "> User-Agent: curl/8.11.1\n",
      "> Accept: */*\n",
      "> \n",
      "* Request completely sent off\n",
      "< HTTP/1.1 200 OK\n",
      "< date: Tue, 04 Mar 2025 04:12:03 GMT\n",
      "< server: uvicorn\n",
      "< content-length: 27\n",
      "< content-type: application/json\n",
      "< \n",
      "* Connection #0 to host fastapi.default.svc.cluster.local left intact\n",
      "{\"message\":\"Hello, world!\"}"
     ]
    }
   ],
   "source": [
    "!kubectl exec -it $ingress_controller_pod -n ingress-nginx -- curl -v fastapi.default.svc.cluster.local:80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "28175614-e54a-4ec6-b84a-0280737ba5dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Host fastapi.default.svc.cluster.local:8000 was resolved.\n",
      "* IPv6: (none)\n",
      "* IPv4: 172.20.116.75\n",
      "*   Trying 172.20.116.75:8000...\n",
      "* connect to 172.20.116.75 port 8000 from 10.0.1.140 port 49998 failed: Operation timed out\n",
      "* Failed to connect to fastapi.default.svc.cluster.local port 8000 after 130824 ms: Could not connect to server\n",
      "* closing connection #0\n",
      "curl: (28) Failed to connect to fastapi.default.svc.cluster.local port 8000 after 130824 ms: Could not connect to server\n",
      "command terminated with exit code 28\n"
     ]
    }
   ],
   "source": [
    "!kubectl exec -it $ingress_controller_pod -n ingress-nginx -- curl -v -X POST fastapi.default.svc.cluster.local:8000/respond --no-buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8c6d2510-2c09-43d3-bb80-db1227c96a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error: pod, type/name or --filename must be specified\n"
     ]
    }
   ],
   "source": [
    "!kubectl exec -it $ingress_controller_pod -n ingress-nginx -- curl -v -X POST fastapi.default.svc.cluster.local:8000/respond --no-buffer -H \"Content-Type: application/json\" -d '{\"content\": \"What are the languages in Eberron?\"}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "0d63949c-c95c-40c5-8629-04ffce5c061c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*   Trying 10.0.1.49:8000...\n",
      "* connect to 10.0.1.49 port 8000 from 10.0.1.140 port 58704 failed: Host is unreachable\n",
      "* Failed to connect to 10.0.1.49 port 8000 after 3060 ms: Could not connect to server\n",
      "* closing connection #0\n",
      "curl: (7) Failed to connect to 10.0.1.49 port 8000 after 3060 ms: Could not connect to server\n",
      "command terminated with exit code 7\n"
     ]
    }
   ],
   "source": [
    "!kubectl exec -it $ingress_controller_pod -n ingress-nginx -- curl -v 10.0.1.49:8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "72ff6e74-4202-45f1-af0a-9d5b6299a9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kubectl exec $ingress_controller_pod -n ingress-nginx -- cat /etc/nginx/nginx.conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "8d9430ed-d98f-4214-b59b-99edfdd606fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Could not resolve proxy: POST\n",
      "* shutting down connection #0\n",
      "curl: (5) Could not resolve proxy: POST\n",
      "command terminated with exit code 5\n"
     ]
    }
   ],
   "source": [
    "!kubectl exec -it $ingress_controller_pod -n ingress-nginx -- curl -v -x POST 10.0.1.49:8000/respond --no-buffer -H \"Content-Type: application/json\" -d '{\"content\": \"What are the languages in Eberron?\"}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "092efbc0-177c-4386-8e51-433562dc3ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error: pod, type/name or --filename must be specified\n"
     ]
    }
   ],
   "source": [
    "!kubectl exec -it $ingress_controller_pod -n ingress-nginx -- nslookup fastapi.default.svc.cluster.local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "ac7d6a8b-d3a4-4659-9a44-10d21c7c43ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server:\t\t172.20.0.10\n",
      "Address:\t172.20.0.10:53\n",
      "\n",
      "\n",
      "Name:\tfastapi.default.svc.cluster.local\n",
      "Address: 172.20.157.106\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!kubectl exec -it $ingress_controller_pod -n ingress-nginx -- nslookup fastapi.default.svc.cluster.local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "176374ad-2ad0-4ca4-a19d-41e3c688265c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME      TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE\n",
      "fastapi   ClusterIP   172.20.157.106   <none>        8000/TCP   82m\n"
     ]
    }
   ],
   "source": [
    "!kubectl get svc fastapi -n default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "4f2502d1-940c-4257-b1a1-5ef1b1519931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*   Trying 172.20.157.106:8000...\n",
      "* Connected to 172.20.157.106 (172.20.157.106) port 8000\n",
      "* using HTTP/1.x\n",
      "> GET / HTTP/1.1\n",
      "> Host: 172.20.157.106:8000\n",
      "> User-Agent: curl/8.11.1\n",
      "> Accept: */*\n",
      "> \n",
      "* Request completely sent off\n",
      "< HTTP/1.1 200 OK\n",
      "< date: Sat, 22 Feb 2025 21:16:29 GMT\n",
      "< server: uvicorn\n",
      "< content-length: 27\n",
      "< content-type: application/json\n",
      "< \n",
      "* Connection #0 to host 172.20.157.106 left intact\n",
      "{\"message\":\"Hello, world!\"}"
     ]
    }
   ],
   "source": [
    "!kubectl exec -it $ingress_controller_pod -n ingress-nginx -- curl -v 172.20.157.106:8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "41168437-7564-4b3f-9a0d-e1739ebd0483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*   Trying 172.20.157.106:8000...\n",
      "* Connected to fastapi (172.20.157.106) port 8000 (#0)\n",
      "> GET / HTTP/1.1\n",
      "> Host: fastapi:8000\n",
      "> User-Agent: curl/7.81.0\n",
      "> Accept: */*\n",
      "> \n",
      "* Mark bundle as not supporting multiuse\n",
      "< HTTP/1.1 200 OK\n",
      "< date: Sat, 22 Feb 2025 21:25:46 GMT\n",
      "< server: uvicorn\n",
      "< content-length: 27\n",
      "< content-type: application/json\n",
      "< \n",
      "* Connection #0 to host fastapi left intact\n",
      "{\"message\":\"Hello, world!\"}"
     ]
    }
   ],
   "source": [
    "!kubectl exec -it $pod_name -n default -- curl -v fastapi:8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "665a516c-3332-4c8e-8f13-6c3b98f6fa69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*   Trying 172.20.157.106:8000...\n",
      "* Connected to fastapi (172.20.157.106) port 8000 (#0)\n",
      "> POST /respond HTTP/1.1\n",
      "> Host: fastapi:8000\n",
      "> User-Agent: curl/7.81.0\n",
      "> Accept: */*\n",
      "> \n",
      "* Mark bundle as not supporting multiuse\n",
      "< HTTP/1.1 422 Unprocessable Entity\n",
      "< date: Sat, 22 Feb 2025 21:26:46 GMT\n",
      "< server: uvicorn\n",
      "< content-length: 82\n",
      "< content-type: application/json\n",
      "< \n",
      "{\"detail\":[{\"type\":\"missing\",\"loc\":[\"body\"],\"msg\":\"Field required\",\"input\":null}]}* Connection #0 to host fastapi left intact\n"
     ]
    }
   ],
   "source": [
    "!kubectl exec -it $pod_name -n default -- curl -v -X POST fastapi:8000/respond --no-buffer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837840e5-85b5-4b9a-a937-9a319a1b1b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl exec -it $pod_name -n default -- curl -v -X POST fastapi:8000/respond --no-buffer -H \"Content-Type: application/json\" -d '{\"content\": \"What are the languages in Eberron?\"}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "b476e154-946e-45f4-bd87-ca22a910e610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Host fastapi.default.svc.cluster.local:8000 was resolved.\n",
      "* IPv6: (none)\n",
      "* IPv4: 172.20.157.106\n",
      "*   Trying 172.20.157.106:8000...\n",
      "* Connected to fastapi.default.svc.cluster.local (172.20.157.106) port 8000\n",
      "* using HTTP/1.x\n",
      "> GET / HTTP/1.1\n",
      "> Host: fastapi.default.svc.cluster.local:8000\n",
      "> User-Agent: curl/8.11.1\n",
      "> Accept: */*\n",
      "> \n",
      "* Request completely sent off\n",
      "< HTTP/1.1 200 OK\n",
      "< date: Sat, 22 Feb 2025 21:19:01 GMT\n",
      "< server: uvicorn\n",
      "< content-length: 27\n",
      "< content-type: application/json\n",
      "< \n",
      "* Connection #0 to host fastapi.default.svc.cluster.local left intact\n",
      "{\"message\":\"Hello, world!\"}"
     ]
    }
   ],
   "source": [
    "!kubectl exec -it $ingress_controller_pod -n ingress-nginx -- curl -v fastapi.default.svc.cluster.local:8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "ee4164d1-4e5e-45c3-9072-917d7d4f222c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME      ENDPOINTS        AGE\n",
      "fastapi   10.0.1.49:8000   77m\n"
     ]
    }
   ],
   "source": [
    "!kubectl get endpoints fastapi -n default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e319c578-74f9-4274-af11-1c0bc114fe43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kubectl describe deployment kubyterlab-llm-pod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a9b485d7-ab30-4ad8-8c23-0c83a5f50f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kubectl describe pod frontend-75669c8cd8-5kkmk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "3d2ac1bd-3d25-4fa8-b2dd-fb7fa10e42d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                        READY   STATUS    RESTARTS   AGE\n",
      "fastapi-677965765c-z5kc6    1/1     Running   0          77m\n",
      "frontend-74b7bcf65b-lknld   1/1     Running   0          33m\n"
     ]
    }
   ],
   "source": [
    "!kubectl get pods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0a2da721-c219-436c-8ba5-11d905c507d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME      STATUS   VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   VOLUMEATTRIBUTESCLASS   AGE\n",
      "pvc-llm   Bound    pv-llm   500Gi      RWO            manual         <unset>                 98m\n"
     ]
    }
   ],
   "source": [
    "!kubectl get pvc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "23b517a1-9d50-47a0-bcc8-e9a0f787c216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME     CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM             STORAGECLASS   VOLUMEATTRIBUTESCLASS   REASON   AGE\n",
      "pv-llm   500Gi      RWO            Retain           Bound    default/pvc-llm   manual         <unset>                          98m\n"
     ]
    }
   ],
   "source": [
    "!kubectl get pv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "6a28c8a8-575e-4182-b448-4c5498d74666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========\n",
      "== CUDA ==\n",
      "==========\n",
      "\n",
      "CUDA Version 12.6.1\n",
      "\n",
      "Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n",
      "\n",
      "This container image and its contents are governed by the NVIDIA Deep Learning Container License.\n",
      "By pulling and using the container, you accept the terms and conditions of this license:\n",
      "https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license\n",
      "\n",
      "A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.\n",
      "\n",
      "2025-02-22 19:57:40.261589: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-22 19:57:40.277019: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1740254260.296566       1 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1740254260.302588       1 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-22 19:57:40.322043: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/app/models.py:41: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=embedding_model_path,\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [33:47<00:00, 675.85s/it]\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "INFO:     Started server process [1]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n",
      "INFO:     10.0.1.58:45566 - \"POST / HTTP/1.1\" 405 Method Not Allowed\n",
      "INFO:     10.0.1.58:40412 - \"POST / HTTP/1.1\" 405 Method Not Allowed\n",
      "INFO:     10.0.1.58:40840 - \"POST / HTTP/1.1\" 405 Method Not Allowed\n",
      "INFO:     10.0.1.58:40840 - \"POST / HTTP/1.1\" 405 Method Not Allowed\n",
      "INFO:     10.0.1.58:35784 - \"POST / HTTP/1.1\" 405 Method Not Allowed\n",
      "INFO:     10.0.1.161:47489 - \"GET / HTTP/1.1\" 200 OK\n",
      "INFO:     10.0.1.161:53110 - \"GET / HTTP/1.1\" 200 OK\n",
      "INFO:     10.0.1.161:8409 - \"GET / HTTP/1.1\" 200 OK\n",
      "INFO:     10.0.1.58:35266 - \"GET / HTTP/1.1\" 200 OK\n",
      "INFO:     10.0.1.58:53128 - \"POST / HTTP/1.1\" 405 Method Not Allowed\n",
      "INFO:     10.0.1.58:51222 - \"GET / HTTP/1.1\" 200 OK\n",
      "INFO:     10.0.1.161:7359 - \"GET / HTTP/1.1\" 200 OK\n",
      "INFO:     10.0.1.58:51596 - \"GET / HTTP/1.1\" 200 OK\n",
      "INFO:     10.0.1.161:63156 - \"GET / HTTP/1.1\" 200 OK\n",
      "INFO:     10.0.1.58:55940 - \"GET / HTTP/1.1\" 200 OK\n",
      "INFO:     10.0.1.58:40008 - \"GET / HTTP/1.1\" 200 OK\n",
      "INFO:     10.0.1.58:46336 - \"GET / HTTP/1.1\" 200 OK\n",
      "INFO:     10.0.1.58:48050 - \"GET / HTTP/1.1\" 200 OK\n",
      "INFO:     10.0.1.161:11262 - \"GET / HTTP/1.1\" 200 OK\n",
      "INFO:     10.0.1.161:30664 - \"POST /respond HTTP/1.1\" 422 Unprocessable Entity\n",
      "INFO:     10.0.1.58:60784 - \"GET / HTTP/1.1\" 200 OK\n",
      "INFO:     10.0.1.58:47764 - \"GET / HTTP/1.1\" 200 OK\n",
      "INFO:     10.0.1.58:48042 - \"POST /respond HTTP/1.1\" 422 Unprocessable Entity\n",
      "INFO:     10.0.1.58:37844 - \"POST /respond HTTP/1.1\" 422 Unprocessable Entity\n",
      "INFO:     10.0.1.58:47738 - \"POST / HTTP/1.1\" 405 Method Not Allowed\n",
      "INFO:     10.0.1.58:45666 - \"POST / HTTP/1.1\" 405 Method Not Allowed\n",
      "INFO:     10.0.1.58:54812 - \"POST / HTTP/1.1\" 405 Method Not Allowed\n",
      "INFO:     10.0.1.58:33394 - \"POST / HTTP/1.1\" 405 Method Not Allowed\n",
      "INFO:     10.0.1.58:42086 - \"POST / HTTP/1.1\" 405 Method Not Allowed\n",
      "INFO:     10.0.1.58:42098 - \"POST / HTTP/1.1\" 405 Method Not Allowed\n",
      "INFO:     10.0.1.58:47320 - \"GET / HTTP/1.1\" 200 OK\n",
      "INFO:     10.0.1.58:35926 - \"POST /respond HTTP/1.1\" 422 Unprocessable Entity\n",
      "INFO:     10.0.1.58:55868 - \"GET / HTTP/1.1\" 200 OK\n",
      "INFO:     10.0.1.58:55868 - \"GET /favicon.ico HTTP/1.1\" 404 Not Found\n",
      "INFO:     10.0.1.58:34700 - \"POST / HTTP/1.1\" 405 Method Not Allowed\n",
      "INFO:     10.0.1.58:49156 - \"POST / HTTP/1.1\" 405 Method Not Allowed\n",
      "INFO:     10.0.1.58:49164 - \"POST / HTTP/1.1\" 405 Method Not Allowed\n",
      "INFO:     10.0.1.58:34556 - \"POST / HTTP/1.1\" 405 Method Not Allowed\n",
      "INFO:     10.0.1.58:34556 - \"POST / HTTP/1.1\" 405 Method Not Allowed\n",
      "INFO:     10.0.1.58:52612 - \"POST / HTTP/1.1\" 405 Method Not Allowed\n"
     ]
    }
   ],
   "source": [
    "!kubectl logs $pod_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6c6f80b2-a78a-45e6-bd1e-0fc3bd307b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:             fastapi-698c4dc749-l5qsk\n",
      "Namespace:        default\n",
      "Priority:         0\n",
      "Service Account:  default\n",
      "Node:             ip-10-0-1-228.ca-central-1.compute.internal/10.0.1.228\n",
      "Start Time:       Sun, 09 Feb 2025 15:09:55 +0000\n",
      "Labels:           app=fastapi\n",
      "                  pod-template-hash=698c4dc749\n",
      "Annotations:      <none>\n",
      "Status:           Running\n",
      "IP:               10.0.1.238\n",
      "IPs:\n",
      "  IP:           10.0.1.238\n",
      "Controlled By:  ReplicaSet/fastapi-698c4dc749\n",
      "Containers:\n",
      "  fastapi:\n",
      "    Container ID:   containerd://a15615861206a39f38823254d93970ca29766b92277c793c2856fb57bc0cfd11\n",
      "    Image:          275678099358.dkr.ecr.ca-central-1.amazonaws.com/multi-agent/eberron-agent-server:25.02\n",
      "    Image ID:       275678099358.dkr.ecr.ca-central-1.amazonaws.com/multi-agent/eberron-agent-server@sha256:0a7ec392240e2f7a4ba9ce05b279df68209e085074bbe4fb8f465e047156b2ff\n",
      "    Port:           80/TCP\n",
      "    Host Port:      0/TCP\n",
      "    State:          Running\n",
      "      Started:      Sun, 09 Feb 2025 15:14:49 +0000\n",
      "    Ready:          True\n",
      "    Restart Count:  0\n",
      "    Environment:\n",
      "      ARTEFACT_VERSION:        04\n",
      "      ARTEFACT_ROOT_FOLDER:    /jupyterlab/artefacts\n",
      "      HF_HOME:                 /jupyterlab/models/hf\n",
      "      MODEL_NAME:              Mistral-7B-Instruct-v0.3\n",
      "      MODEL_ORG:               mistralai\n",
      "      COMMIT_HASH:             e0bc86c23ce5aae1db576c8cca6f06f1f73af2db\n",
      "      TOKENIZERS_PARALLELISM:  true\n",
      "      TRANSFORMERS_OFFLINE:    1\n",
      "      HF_DATASETS_OFFLINE:     1\n",
      "    Mounts:\n",
      "      /jupyterlab from pvc-llm (rw)\n",
      "      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-klc6j (ro)\n",
      "Conditions:\n",
      "  Type                        Status\n",
      "  PodReadyToStartContainers   True \n",
      "  Initialized                 True \n",
      "  Ready                       True \n",
      "  ContainersReady             True \n",
      "  PodScheduled                True \n",
      "Volumes:\n",
      "  pvc-llm:\n",
      "    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)\n",
      "    ClaimName:  pvc-llm\n",
      "    ReadOnly:   false\n",
      "  kube-api-access-klc6j:\n",
      "    Type:                    Projected (a volume that contains injected data from multiple sources)\n",
      "    TokenExpirationSeconds:  3607\n",
      "    ConfigMapName:           kube-root-ca.crt\n",
      "    ConfigMapOptional:       <nil>\n",
      "    DownwardAPI:             true\n",
      "QoS Class:                   BestEffort\n",
      "Node-Selectors:              <none>\n",
      "Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n",
      "                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\n",
      "                             nvidia.com/gpu:NoSchedule op=Exists\n",
      "Events:\n",
      "  Type    Reason                  Age   From                     Message\n",
      "  ----    ------                  ----  ----                     -------\n",
      "  Normal  Scheduled               21m   default-scheduler        Successfully assigned default/fastapi-698c4dc749-l5qsk to ip-10-0-1-228.ca-central-1.compute.internal\n",
      "  Normal  SuccessfulAttachVolume  21m   attachdetach-controller  AttachVolume.Attach succeeded for volume \"pv-llm\"\n",
      "  Normal  Pulling                 21m   kubelet                  Pulling image \"275678099358.dkr.ecr.ca-central-1.amazonaws.com/multi-agent/eberron-agent-server:25.02\"\n",
      "  Normal  Pulled                  16m   kubelet                  Successfully pulled image \"275678099358.dkr.ecr.ca-central-1.amazonaws.com/multi-agent/eberron-agent-server:25.02\" in 4m28.297s (4m28.297s including waiting). Image size: 14071664304 bytes.\n",
      "  Normal  Created                 16m   kubelet                  Created container fastapi\n",
      "  Normal  Started                 16m   kubelet                  Started container fastapi\n"
     ]
    }
   ],
   "source": [
    "!kubectl describe pod $pod_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad2f7fa-c856-4866-96bb-54eefb245b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "attachdetach-controller  AttachVolume.Attach failed for volume \"pv-llm\" : rpc error: code = Internal desc = Could not attach volume \"volume_id\" to node \"i-055aa2853ecdeac3b\": could not attach volume \"volume_id\" to node \"i-055aa2853ecdeac3b\": operation error EC2: AttachVolume, https response error StatusCode: 400, RequestID: a7250de3-c285-4963-84eb-5c46b28c2b96, api error InvalidParameterValue: The volume ID 'volume_id' is malformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6fbb74-4125-4ad3-96b4-f7117b82e1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl exec $pod_name bash -- nvidia-smi --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e4308b32-7f70-4861-81f6-7bc0702cbc87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulted container \"kubyterlab-llm\" out of: kubyterlab-llm, init-container (init)\n",
      "Mon Jan 13 22:15:38 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       On  |   00000000:00:1E.0 Off |                    0 |\n",
      "| N/A   25C    P8             11W /   70W |       1MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!kubectl exec $pod_name bash -- nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5a68c2d5-dc22-4529-9e8f-a7935f360340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulted container \"kubyterlab-llm\" out of: kubyterlab-llm, init-container (init)\n",
      "GPU 0: Tesla T4 (UUID: GPU-fe1acad6-41f9-c441-44ed-d4ead6db6dc2)\n"
     ]
    }
   ],
   "source": [
    "!kubectl exec $pod_name bash -- nvidia-smi -L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8d229310-97eb-4788-bf0e-22203dffe078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulted container \"kubyterlab-llm\" out of: kubyterlab-llm, init-container (init)\n",
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2024 NVIDIA Corporation\n",
      "Built on Wed_Aug_14_10:10:22_PDT_2024\n",
      "Cuda compilation tools, release 12.6, V12.6.68\n",
      "Build cuda_12.6.r12.6/compiler.34714021_0\n"
     ]
    }
   ],
   "source": [
    "!kubectl exec $pod_name bash -- nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722a4aa7-a0c3-462e-9e37-60230261fcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl describe pod $pod_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "116fd849-a569-472e-93d6-23624e93c7e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                   READY   STATUS    RESTARTS   AGE\n",
      "aws-node-cvzp2                         2/2     Running   0          4m59s\n",
      "aws-node-kmnjj                         2/2     Running   0          4m57s\n",
      "aws-node-zlmn8                         2/2     Running   0          6m35s\n",
      "coredns-749d5dbdd9-cdd8v               1/1     Running   0          8m29s\n",
      "coredns-749d5dbdd9-xtrc8               1/1     Running   0          8m29s\n",
      "ebs-csi-controller-59b6797bf-fhcph     5/5     Running   0          4m6s\n",
      "ebs-csi-controller-59b6797bf-w9mns     5/5     Running   0          4m6s\n",
      "ebs-csi-node-mp9sr                     3/3     Running   0          4m6s\n",
      "ebs-csi-node-mqt55                     3/3     Running   0          4m6s\n",
      "ebs-csi-node-q8dfl                     3/3     Running   0          4m6s\n",
      "kube-proxy-ctjcd                       1/1     Running   0          6m35s\n",
      "kube-proxy-nv8tx                       1/1     Running   0          4m57s\n",
      "kube-proxy-qmwmh                       1/1     Running   0          4m59s\n",
      "nvidia-device-plugin-daemonset-4b76p   1/1     Running   0          4m4s\n",
      "nvidia-device-plugin-daemonset-ltcdg   1/1     Running   0          4m4s\n",
      "nvidia-device-plugin-daemonset-wv2rd   1/1     Running   0          4m4s\n"
     ]
    }
   ],
   "source": [
    "!kubectl get pod -n kube-system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "90495808-55bb-4d5b-84e8-c00ca25250b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:          pvc-llm\n",
      "Namespace:     default\n",
      "StorageClass:  manual\n",
      "Status:        Bound\n",
      "Volume:        pv-llm\n",
      "Labels:        <none>\n",
      "Annotations:   pv.kubernetes.io/bind-completed: yes\n",
      "               pv.kubernetes.io/bound-by-controller: yes\n",
      "Finalizers:    [kubernetes.io/pvc-protection]\n",
      "Capacity:      10Gi\n",
      "Access Modes:  RWO\n",
      "VolumeMode:    Filesystem\n",
      "Used By:       kubyterlab-llm-pod-67f5cf95dc-s2mrn\n",
      "Events:        <none>\n"
     ]
    }
   ],
   "source": [
    "!kubectl describe pvc pvc-llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "da047557-f797-4492-b7f6-1cdcb16a3283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:            pv-llm\n",
      "Labels:          <none>\n",
      "Annotations:     pv.kubernetes.io/bound-by-controller: yes\n",
      "Finalizers:      [kubernetes.io/pv-protection]\n",
      "StorageClass:    manual\n",
      "Status:          Bound\n",
      "Claim:           default/pvc-llm\n",
      "Reclaim Policy:  Retain\n",
      "Access Modes:    RWO\n",
      "VolumeMode:      Filesystem\n",
      "Capacity:        10Gi\n",
      "Node Affinity:   <none>\n",
      "Message:         \n",
      "Source:\n",
      "    Type:       AWSElasticBlockStore (a Persistent Disk resource in AWS)\n",
      "    VolumeID:   vol-04ce08a2c05b8e1da\n",
      "    FSType:     ext4\n",
      "    Partition:  0\n",
      "    ReadOnly:   false\n",
      "Events:         <none>\n"
     ]
    }
   ],
   "source": [
    "!kubectl describe pv pv-llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "80827487-9836-450d-806d-5f56b00ab821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:                   kubyterlab-llm-pod\n",
      "Namespace:              default\n",
      "CreationTimestamp:      Mon, 02 Dec 2024 20:05:56 +0000\n",
      "Labels:                 <none>\n",
      "Annotations:            deployment.kubernetes.io/revision: 1\n",
      "Selector:               app=kubyterlab-llm\n",
      "Replicas:               1 desired | 1 updated | 1 total | 0 available | 1 unavailable\n",
      "StrategyType:           RollingUpdate\n",
      "MinReadySeconds:        0\n",
      "RollingUpdateStrategy:  25% max unavailable, 25% max surge\n",
      "Pod Template:\n",
      "  Labels:  app=kubyterlab-llm\n",
      "  Containers:\n",
      "   kubyterlab-llm:\n",
      "    Image:      kubyterlab:24.10\n",
      "    Port:       8888/TCP\n",
      "    Host Port:  0/TCP\n",
      "    Limits:\n",
      "      cpu:             1\n",
      "      memory:          8Gi\n",
      "      nvidia.com/gpu:  1\n",
      "    Requests:\n",
      "      cpu:             1\n",
      "      memory:          8Gi\n",
      "      nvidia.com/gpu:  1\n",
      "    Environment:\n",
      "      JUPYTERLAB_SETTINGS_DIR:  /jupyterlab/config\n",
      "      MISTRAL_MODEL:            /models/mistral\n",
      "    Mounts:\n",
      "      /corpus from pv-llm (rw)\n",
      "      /jupyterlab from pv-llm (rw)\n",
      "      /models from pv-llm (rw)\n",
      "  Volumes:\n",
      "   pv-llm:\n",
      "    Type:          PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)\n",
      "    ClaimName:     pvc-llm\n",
      "    ReadOnly:      false\n",
      "  Node-Selectors:  <none>\n",
      "  Tolerations:     nvidia.com/gpu:NoSchedule op=Exists\n",
      "Conditions:\n",
      "  Type           Status  Reason\n",
      "  ----           ------  ------\n",
      "  Available      False   MinimumReplicasUnavailable\n",
      "  Progressing    True    ReplicaSetUpdated\n",
      "OldReplicaSets:  <none>\n",
      "NewReplicaSet:   kubyterlab-llm-pod-67f5cf95dc (1/1 replicas created)\n",
      "Events:\n",
      "  Type    Reason             Age    From                   Message\n",
      "  ----    ------             ----   ----                   -------\n",
      "  Normal  ScalingReplicaSet  5m46s  deployment-controller  Scaled up replica set kubyterlab-llm-pod-67f5cf95dc to 1\n"
     ]
    }
   ],
   "source": [
    "!kubectl describe deployment kubyterlab-llm-pod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ce402031-be81-4578-8877-15bcd40ad8e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                          STATUS   ROLES    AGE   VERSION\n",
      "ip-10-0-1-100.ca-central-1.compute.internal   Ready    <none>   47m   v1.30.7-eks-59bf375\n",
      "ip-10-0-1-25.ca-central-1.compute.internal    Ready    <none>   47m   v1.30.7-eks-59bf375\n",
      "ip-10-0-1-50.ca-central-1.compute.internal    Ready    <none>   48m   v1.30.7-eks-59bf375\n"
     ]
    }
   ],
   "source": [
    "!kubectl get nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d0bb4314-1002-45c7-b0c2-4d706b5d533f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME               DATA   AGE\n",
      "kube-root-ca.crt   1      50m\n"
     ]
    }
   ],
   "source": [
    "!kubectl get configmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6d65a766-6f17-413c-a7f5-9f8e07fd0a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                     READY   STATUS    RESTARTS   AGE\n",
      "pod/ebs-csi-controller-59b6797bf-vfvs2   5/5     Running   0          47m\n",
      "pod/ebs-csi-controller-59b6797bf-xt7fv   5/5     Running   0          47m\n",
      "pod/ebs-csi-node-dwzz7                   3/3     Running   0          47m\n",
      "pod/ebs-csi-node-gplqp                   3/3     Running   0          47m\n",
      "pod/ebs-csi-node-l5nh6                   3/3     Running   0          47m\n",
      "\n",
      "NAME                          DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE\n",
      "daemonset.apps/ebs-csi-node   3         3         3       3            3           kubernetes.io/os=linux   47m\n",
      "\n",
      "NAME                                 READY   UP-TO-DATE   AVAILABLE   AGE\n",
      "deployment.apps/ebs-csi-controller   2/2     2            2           47m\n",
      "\n",
      "NAME                                           DESIRED   CURRENT   READY   AGE\n",
      "replicaset.apps/ebs-csi-controller-59b6797bf   2         2         2       47m\n"
     ]
    }
   ],
   "source": [
    "!kubectl get all -l app.kubernetes.io/name=aws-ebs-csi-driver -n kube-system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3bf4b6bf-4a67-48e4-ba16-8207e90b7fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"OpenIDConnectProviderList\": [\n",
      "        {\n",
      "            \"Arn\": \"arn:aws:iam::275678099358:oidc-provider/oidc.eks.ca-central-1.amazonaws.com/id/8D7619520212428BD59C46B20BF19338\"\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!aws iam list-open-id-connect-providers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "65c00bf0-9326-417c-964f-530ab62c85da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 pods, using pod/ebs-csi-controller-59b6797bf-vfvs2\n",
      "I0113 21:28:06.936951       1 main.go:153] \"Initializing metadata\"\n",
      "I0113 21:28:06.943481       1 metadata.go:48] \"Retrieved metadata from IMDS\"\n",
      "I0113 21:28:06.944585       1 driver.go:69] \"Driver Information\" Driver=\"ebs.csi.aws.com\" Version=\"v1.38.1\"\n",
      "I0113 22:01:16.236299       1 controller.go:410] \"ControllerPublishVolume: attaching\" volumeID=\"vol-0ed5d3cc8cb5a989e\" nodeID=\"i-0fd8ede93d6ac59a4\"\n",
      "I0113 22:01:17.882498       1 controller.go:419] \"ControllerPublishVolume: attached\" volumeID=\"vol-0ed5d3cc8cb5a989e\" nodeID=\"i-0fd8ede93d6ac59a4\" devicePath=\"/dev/xvdaa\"\n"
     ]
    }
   ],
   "source": [
    "!kubectl logs deployment/ebs-csi-controller -n kube-system -c ebs-plugin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8ab7f84b-398b-41fb-9c35-30f40c179f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 pods, using pod/ebs-csi-node-l5nh6\n",
      "I0113 21:27:55.828513       1 main.go:153] \"Initializing metadata\"\n",
      "I0113 21:27:55.832956       1 metadata.go:48] \"Retrieved metadata from IMDS\"\n",
      "I0113 21:27:55.833450       1 driver.go:69] \"Driver Information\" Driver=\"ebs.csi.aws.com\" Version=\"v1.38.1\"\n",
      "E0113 21:27:56.896438       1 node.go:856] \"Unexpected failure when attempting to remove node taint(s)\" err=\"isAllocatableSet: driver not found on node ip-10-0-1-50.ca-central-1.compute.internal\"\n",
      "I0113 21:27:57.410789       1 node.go:936] \"CSINode Allocatable value is set\" nodeName=\"ip-10-0-1-50.ca-central-1.compute.internal\" count=24\n",
      "I0113 22:01:18.817732       1 node.go:204] \"NodeStageVolume: invalid partition config, will ignore.\" partition=\"0\"\n",
      "I0113 22:01:18.980008       1 mount_linux.go:295] Detected OS without systemd\n"
     ]
    }
   ],
   "source": [
    "!kubectl logs daemonset/ebs-csi-node -n kube-system -c ebs-plugin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f2fd1fcf-f18f-4b19-abeb-1c207ddd58de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 pods, using pod/ebs-csi-controller-59b6797bf-vfvs2\n",
      "W0113 21:28:09.566495       1 feature_gate.go:354] Setting GA feature gate Topology=true. It will be removed in a future release.\n",
      "I0113 21:28:09.566789       1 feature_gate.go:387] feature gates: {map[Topology:true]}\n",
      "I0113 21:28:09.566840       1 csi-provisioner.go:154] Version: v5.1.0\n",
      "I0113 21:28:09.566851       1 csi-provisioner.go:177] Building kube configs for running in cluster...\n",
      "I0113 21:28:09.568886       1 common.go:143] \"Probing CSI driver for readiness\"\n",
      "I0113 21:28:09.598446       1 csi-provisioner.go:230] Detected CSI driver ebs.csi.aws.com\n",
      "I0113 21:28:09.598619       1 csi-provisioner.go:240] Supports migration from in-tree plugin: kubernetes.io/aws-ebs\n",
      "I0113 21:28:09.600672       1 common.go:143] \"Probing CSI driver for readiness\"\n",
      "I0113 21:28:09.603385       1 csi-provisioner.go:299] CSI driver supports PUBLISH_UNPUBLISH_VOLUME, watching VolumeAttachments\n",
      "I0113 21:28:09.604750       1 controller.go:744] \"Using saving PVs to API server in background\"\n",
      "I0113 21:28:09.605162       1 leaderelection.go:254] attempting to acquire leader lease kube-system/ebs-csi-aws-com...\n",
      "I0113 21:28:09.627768       1 leaderelection.go:268] successfully acquired lease kube-system/ebs-csi-aws-com\n",
      "I0113 21:28:09.629736       1 leader_election.go:184] \"became leader, starting\"\n",
      "I0113 21:28:09.629845       1 envvar.go:172] \"Feature gate default state\" feature=\"WatchListClient\" enabled=false\n",
      "I0113 21:28:09.629863       1 envvar.go:172] \"Feature gate default state\" feature=\"InformerResourceVersion\" enabled=false\n",
      "I0113 21:28:09.640053       1 reflector.go:368] Caches populated for *v1.StorageClass from k8s.io/client-go@v0.31.0/tools/cache/reflector.go:243\n",
      "I0113 21:28:09.651560       1 reflector.go:368] Caches populated for *v1.PersistentVolumeClaim from k8s.io/client-go@v0.31.0/tools/cache/reflector.go:243\n",
      "I0113 21:28:09.656122       1 reflector.go:368] Caches populated for *v1.Node from k8s.io/client-go@v0.31.0/tools/cache/reflector.go:243\n",
      "I0113 21:28:09.656502       1 reflector.go:368] Caches populated for *v1.VolumeAttachment from k8s.io/client-go@v0.31.0/tools/cache/reflector.go:243\n",
      "I0113 21:28:09.657442       1 reflector.go:368] Caches populated for *v1.CSINode from k8s.io/client-go@v0.31.0/tools/cache/reflector.go:243\n",
      "I0113 21:28:09.729875       1 controller.go:824] \"Starting provisioner controller\" component=\"ebs.csi.aws.com_ebs-csi-controller-59b6797bf-vfvs2_1c292ac0-a9d9-48ff-8621-9ba1f75e01b8\"\n",
      "I0113 21:28:09.729957       1 volume_store.go:98] \"Starting save volume queue\"\n",
      "I0113 21:28:09.734566       1 reflector.go:368] Caches populated for *v1.StorageClass from k8s.io/client-go@v0.31.0/tools/cache/reflector.go:243\n",
      "I0113 21:28:09.734567       1 reflector.go:368] Caches populated for *v1.PersistentVolume from k8s.io/client-go@v0.31.0/tools/cache/reflector.go:243\n",
      "I0113 21:28:09.831005       1 controller.go:873] \"Started provisioner controller\" component=\"ebs.csi.aws.com_ebs-csi-controller-59b6797bf-vfvs2_1c292ac0-a9d9-48ff-8621-9ba1f75e01b8\"\n"
     ]
    }
   ],
   "source": [
    "!kubectl logs deployment/ebs-csi-controller -n kube-system -c csi-provisioner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "49134a19-5818-4030-8a7a-6c9e04d6157c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 pods, using pod/ebs-csi-controller-59b6797bf-vfvs2\n",
      "I0113 21:28:12.211572       1 main.go:109] \"Version\" version=\"v4.7.0\"\n",
      "I0113 21:28:12.215865       1 common.go:143] \"Probing CSI driver for readiness\"\n",
      "I0113 21:28:12.238251       1 main.go:169] \"CSI driver name\" driver=\"ebs.csi.aws.com\"\n",
      "I0113 21:28:12.240448       1 common.go:143] \"Probing CSI driver for readiness\"\n",
      "I0113 21:28:12.242734       1 main.go:249] \"CSI driver supports ControllerPublishUnpublish, using real CSI handler\" driver=\"ebs.csi.aws.com\"\n",
      "I0113 21:28:12.243624       1 leaderelection.go:254] attempting to acquire leader lease kube-system/external-attacher-leader-ebs-csi-aws-com...\n",
      "I0113 21:28:12.265555       1 leaderelection.go:268] successfully acquired lease kube-system/external-attacher-leader-ebs-csi-aws-com\n",
      "I0113 21:28:12.266787       1 leader_election.go:184] \"became leader, starting\"\n",
      "I0113 21:28:12.266811       1 controller.go:129] \"Starting CSI attacher\"\n",
      "I0113 21:28:12.267257       1 envvar.go:172] \"Feature gate default state\" feature=\"InformerResourceVersion\" enabled=false\n",
      "I0113 21:28:12.267536       1 envvar.go:172] \"Feature gate default state\" feature=\"WatchListClient\" enabled=false\n",
      "I0113 21:28:12.271734       1 reflector.go:368] Caches populated for *v1.PersistentVolume from k8s.io/client-go@v0.31.0/tools/cache/reflector.go:243\n",
      "I0113 21:28:12.272191       1 reflector.go:368] Caches populated for *v1.CSINode from k8s.io/client-go@v0.31.0/tools/cache/reflector.go:243\n",
      "I0113 21:28:12.272546       1 reflector.go:368] Caches populated for *v1.VolumeAttachment from k8s.io/client-go@v0.31.0/tools/cache/reflector.go:243\n",
      "I0113 22:01:16.209140       1 csi_handler.go:261] \"Attaching\" VolumeAttachment=\"csi-ed0e5d922afcde1b934ffafc0f0d3a9543d0760d7dfa4ce99585f3fc2c2922b0\"\n",
      "I0113 22:01:17.882946       1 csi_handler.go:273] \"Attached\" VolumeAttachment=\"csi-ed0e5d922afcde1b934ffafc0f0d3a9543d0760d7dfa4ce99585f3fc2c2922b0\"\n"
     ]
    }
   ],
   "source": [
    "!kubectl logs deployment/ebs-csi-controller -n kube-system -c csi-attacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6fdb0f08-aba7-4d63-8e01-2b25d1b06b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"eks.amazonaws.com/role-arn\":\"\",\"meta.helm.sh/release-name\":\"aws-ebs-csi-driver\",\"meta.helm.sh/release-namespace\":\"kube-system\"}"
     ]
    }
   ],
   "source": [
    "!kubectl get sa ebs-csi-controller-sa -n kube-system -o jsonpath='{.metadata.annotations}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a85d88b-946c-40e7-8b59-0e69a4567d5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
