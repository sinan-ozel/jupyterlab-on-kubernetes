{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d02ca0b4-1c4a-4b8e-a7a9-8fe3c9ba40b1",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38534870-6679-40ae-bcf1-f778377e5998",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from time import sleep\n",
    "from time import time as unixtime\n",
    "from typing import Callable, List\n",
    "import random\n",
    "from math import ceil\n",
    "import yaml\n",
    "import string\n",
    "import json\n",
    "from urllib.parse import urlparse\n",
    "from time import sleep\n",
    "import re\n",
    "from operator import itemgetter\n",
    "import subprocess\n",
    "from tempfile import NamedTemporaryFile\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "import boto3\n",
    "import kubernetes\n",
    "from kubernetes.client.rest import ApiException\n",
    "\n",
    "from pyhelm3 import Client as HelmClient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a98f1d3-e2b6-4c0a-ad3d-c07f5416d684",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv('/.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08774840-c227-48d4-844c-165c6f5dc8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import wait_until\n",
    "from helper.ec2 import (get_vpcs_ids, get_internet_gateway_ids_attached_to_vpc, \n",
    "                        get_route_table_ids_for_vpc, route_to_gateway_exists, \n",
    "                        get_subnet_ids_in_vpc, get_security_group_ids)\n",
    "from helper.k8s import (get_one_running_pod, get_jupyter_token_from_pod)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1800b74-d019-445d-942e-7223e770f332",
   "metadata": {},
   "source": [
    "# Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3b9f759-4820-40ad-8916-e935d6e57232",
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = 'ca-central-1'\n",
    "CLUSTER_NAME = 'kubyterlab-llm'\n",
    "EBS_VOLUME_SIZE = 500 # GiB\n",
    "TAGS = {'cluster': CLUSTER_NAME, 'purpose': 'llm'}  # Do not change the keys, they are hardcoded throughout.\n",
    "CLUSTER_TAGS = {'cluster': CLUSTER_NAME}\n",
    "VOLUME_FILTERS = [\n",
    "    {'Name': f'tag:purpose', \n",
    "     'Values': ['kubyterlab-llm', 'llm']}]\n",
    "K8S_VERSION = os.environ['K8S_VERSION']  # '1.30'\n",
    "K8S_VERSION = '.'.join(K8S_VERSION.split('.')[:2]) if len(K8S_VERSION.split('.')) > 2 else K8S_VERSION\n",
    "INSTANCE_TYPES = {'gpu': ['g4dn.2xlarge'], 'default': ['t3.medium']}\n",
    "ALLOWED_PORTS = [80, 443, 22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d859c1f-c31c-43b5-9a3a-fa9ed0ae7006",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_all_vpcs_available(response: dict) -> True:\n",
    "    if not 'Vpcs' in response:\n",
    "        raise ValueError\n",
    "    return all([vpc.get('State', '') == 'available' for vpc in response['Vpcs']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70ff5d92-c3be-4255-9d40-afea872ff121",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_cluster_active(response: dict) -> bool:\n",
    "    status = response['cluster']['status']\n",
    "    clear_output(wait=True)\n",
    "    display(status)\n",
    "    return status == 'ACTIVE'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8983294-0e11-457c-abb6-2ec996243d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_node_group_active(response: dict) -> bool:\n",
    "    status = response['nodegroup']['status']\n",
    "    clear_output(wait=True)\n",
    "    display(status)\n",
    "    return status in ['ACTIVE', 'CREATE_FAILED']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9938e8cf-fc3a-42f8-819b-7748181fffc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_snapshot_completed(response: dict) -> bool:\n",
    "    state = response['Snapshots'][0]['State']\n",
    "    clear_output(wait=True)\n",
    "    display(state)\n",
    "    return state.lower() == 'completed'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f79a307-fbfa-4d1e-9d75-9f77405a7dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_volume_available(response: dict) -> bool:\n",
    "    state = response['Volumes'][0]['State']\n",
    "    clear_output(wait=True)\n",
    "    display(state)\n",
    "    return state.lower() == 'available'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73ff831-37b6-4fbf-9aad-1068cfb8efe7",
   "metadata": {},
   "source": [
    "# Instantiate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97c98c25-b95b-40da-be00-f65f5952d5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = boto3.Session(region_name=REGION)\n",
    "eks_client = session.client('eks')\n",
    "ec2_client = session.client('ec2')\n",
    "iam_client = session.client('iam')\n",
    "\n",
    "aws_account_id = boto3.client('sts').get_caller_identity().get('Account')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbf7c2f-2e86-48c9-b369-1047bd8fea6c",
   "metadata": {},
   "source": [
    "# Create or Restore Volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "025247fd-7e39-46c3-ba9c-8bf58a3dcc19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    response = ec2_client.describe_volumes(Filters=VOLUME_FILTERS)\n",
    "    volumes = response.get('Volumes', [])\n",
    "except RuntimeError:\n",
    "    volumes = []\n",
    "volume_ids = [volume['VolumeId'] for volume in volumes]\n",
    "volume_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "728edb7d-b012-4778-8f98-34f00f520042",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(volume_ids) <= 1  # TODO: Get the latest one if more than one.\n",
    "if volume_ids:\n",
    "    volume_id = volume_ids[0]\n",
    "    availability_zone = volumes[0]['AvailabilityZone']\n",
    "else:\n",
    "    volume_id = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf65cde1-f0ad-4b9a-9c91-49c20d09b2f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "snap-063c6d84a585ac97f\n"
     ]
    }
   ],
   "source": [
    "if not volume_id:\n",
    "    response = ec2_client.describe_snapshots(Filters=VOLUME_FILTERS)\n",
    "    snapshots = response.get('Snapshots', [])\n",
    "    if snapshots:\n",
    "        sorted_snapshots = sorted(snapshots, key=itemgetter('StartTime'), reverse=True)\n",
    "        snapshot_id = sorted_snapshots[0]['SnapshotId']\n",
    "        print(snapshot_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5d85302-63d3-4ef2-a07e-bd59dc564768",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'available'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('vol-08de6efda65d040f1', 'ca-central-1a')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not volume_id:\n",
    "    response = ec2_client.describe_availability_zones()\n",
    "    availability_zones = response['AvailabilityZones']\n",
    "    availability_zone = availability_zones[0]['ZoneName']\n",
    "    # availability_zone = f'{REGION}a'\n",
    "    if snapshots:\n",
    "        # TODO: Change this to the latest snapshot!!\n",
    "        response = ec2_client.create_volume(\n",
    "            SnapshotId=snapshot_id,\n",
    "            Size=EBS_VOLUME_SIZE,\n",
    "            AvailabilityZone=availability_zone,\n",
    "            VolumeType='gp3',\n",
    "            TagSpecifications=[\n",
    "                {\n",
    "                    'ResourceType': 'volume',\n",
    "                    'Tags': [{'Key': 'purpose', 'Value': 'llm'}]\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "    else:\n",
    "        response = ec2_client.create_volume(\n",
    "            Size=EBS_VOLUME_SIZE,\n",
    "            AvailabilityZone=availability_zones[0]['ZoneName'],\n",
    "            VolumeType='gp3',\n",
    "            TagSpecifications=[\n",
    "                {\n",
    "                    'ResourceType': 'volume',\n",
    "                    'Tags': [{'Key': 'purpose', 'Value': 'llm'}]\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "    wait_until(ec2_client.describe_volumes, {'VolumeIds': [response['VolumeId']]}, is_volume_available)\n",
    "    volume_id = response['VolumeId']\n",
    "volume_id, availability_zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e815d5e-1d1e-4f94-ac53-9fa53edbdd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Change the parts below to use Terraform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbda3f9d-a108-48f4-8411-f7dd2e0e1f43",
   "metadata": {},
   "source": [
    "# Create VPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "429b8e60-e14a-460b-a448-45bc392e6c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating VPC...\n",
      "Creating Internet Gateway...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('vpc-0b49c534e4deeeebe', 'igw-01baa8f4c174bd1f1', 'rtb-002b13bc9ef438ff0')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vpc_ids = get_vpcs_ids(ec2_client, TAGS)\n",
    "vpc_exists = len(vpc_ids) > 0\n",
    "\n",
    "if len(vpc_ids) > 1:\n",
    "    raise RuntimeError\n",
    "elif vpc_exists:\n",
    "    vpc_id = vpc_ids[0]\n",
    "\n",
    "# Create VPC\n",
    "if not vpc_exists:\n",
    "    print('Creating VPC...')\n",
    "    vpc_response = ec2_client.create_vpc(\n",
    "        CidrBlock='10.0.0.0/16',\n",
    "        TagSpecifications=[\n",
    "            {\n",
    "                'ResourceType': 'vpc',\n",
    "                'Tags': [{'Key': tag, 'Value': TAGS[tag] }for tag in TAGS]\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    vpc_id = vpc_response['Vpc']['VpcId']\n",
    "wait_until(ec2_client.describe_vpcs, {'VpcIds': [vpc_id]}, check_all_vpcs_available)\n",
    "\n",
    "# Create Internet Gateway\n",
    "igw_ids = get_internet_gateway_ids_attached_to_vpc(ec2_client, vpc_id)\n",
    "igw_exists = len(igw_ids) > 0\n",
    "\n",
    "if len(igw_ids) > 1:\n",
    "    raise RuntimeError\n",
    "elif igw_exists:\n",
    "    igw_id = igw_ids[0]\n",
    "\n",
    "\n",
    "if not igw_exists:\n",
    "    print('Creating Internet Gateway...')\n",
    "    # Create an Internet Gateway\n",
    "    igw_response = ec2_client.create_internet_gateway()\n",
    "    igw_id = igw_response['InternetGateway']['InternetGatewayId']\n",
    "    \n",
    "    # Attach Internet Gateway to VPC\n",
    "    ec2_client.attach_internet_gateway(\n",
    "        InternetGatewayId=igw_id,\n",
    "        VpcId=vpc_id\n",
    "    )\n",
    "\n",
    "# Create a route table\n",
    "route_table_ids = get_route_table_ids_for_vpc(ec2_client, vpc_id)\n",
    "route_table_exists = len(route_table_ids) > 0\n",
    "\n",
    "if not route_table_exists:\n",
    "    print('Creating a route table...')\n",
    "    # Create a route table\n",
    "    route_table_response = ec2_client.create_route_table(VpcId=vpc_id)\n",
    "    route_table_id = route_table_response['RouteTable']['RouteTableId']\n",
    "\n",
    "is_route_created = False\n",
    "for route_table_id in route_table_ids:\n",
    "    if route_to_gateway_exists(ec2_client, route_table_id, igw_id):\n",
    "        is_route_created = True\n",
    "        break\n",
    "\n",
    "if not is_route_created:\n",
    "    # Create a route to the Internet Gateway\n",
    "    ec2_client.create_route(\n",
    "        RouteTableId=route_table_id,\n",
    "        DestinationCidrBlock='0.0.0.0/0',\n",
    "        GatewayId=igw_id\n",
    "    )\n",
    "\n",
    "vpc_id, igw_id, route_table_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbfdb24-1435-47bb-8293-b87b9d4075b7",
   "metadata": {},
   "source": [
    "# Create Subnets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b32166a-ce50-45f6-aac3-3f809836f103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating subnet #1...\n",
      "Creating subnet #2...\n"
     ]
    }
   ],
   "source": [
    "subnet_ids = get_subnet_ids_in_vpc(ec2_client, vpc_id)\n",
    "min_subnet_count = 2\n",
    "current_subnet_count = len(subnet_ids)\n",
    "\n",
    "# if current_subnet_count < min_subnet_count:\n",
    "    # response = ec2_client.describe_availability_zones()\n",
    "    \n",
    "    # availability_zones = [az['ZoneName'] for az in response['AvailabilityZones']]\n",
    "    # random.shuffle(availability_zones)\n",
    "    # TODO: Following logic is still necessary for situations with larger subnets. Change to something reasonable.\n",
    "    # availability_zones *= ceil(min_subnet_count / len(availability_zones))\n",
    "    \n",
    "\n",
    "while current_subnet_count < min_subnet_count:\n",
    "    i = current_subnet_count + 1\n",
    "    print(f'Creating subnet #{i}...')\n",
    "    subnet_response = ec2_client.create_subnet(\n",
    "        VpcId=vpc_id,\n",
    "        CidrBlock=f'10.0.{i}.0/24',\n",
    "        # AvailabilityZone=availability_zone,\n",
    "        AvailabilityZone=availability_zones[current_subnet_count]['ZoneName'],\n",
    "        TagSpecifications=[\n",
    "            {\n",
    "                'ResourceType': 'subnet',\n",
    "                'Tags': [{'Key': tag, 'Value': TAGS[tag]} for tag in TAGS]\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    subnet_id = subnet_response['Subnet']['SubnetId']\n",
    "    subnet_ids.append(subnet_id)\n",
    "    current_subnet_count = len(subnet_ids)\n",
    "\n",
    "# check if any subnet is public, add the table to first subnet if not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c837c746-05e1-432f-8619-e3c0581650f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "subnets_response = ec2_client.describe_subnets(SubnetIds=subnet_ids)\n",
    "assert len(subnets_response) == len(subnet_ids) == min_subnet_count\n",
    "is_any_subnet_open_to_public = False\n",
    "for subnet in subnets_response['Subnets']:\n",
    "    if subnets_response.get('MapPublicIpOnLaunch', False):\n",
    "        # I could also look at the route tables, and see if they are assigned to any subnet. ec2_client.describe_route_tables(RouteTableIds=[route_table_id]) RouteTables.SubnetId (optional parameter)\n",
    "        public_subnet_id = subnet['SubnetId']\n",
    "        is_any_subnet_open_to_public = True\n",
    "\n",
    "if not is_any_subnet_open_to_public:\n",
    "    for subnet in subnets_response['Subnets']:\n",
    "        if subnet['CidrBlock'] == '10.0.1.0/24':\n",
    "            public_subnet_id = subnet['SubnetId']\n",
    "\n",
    "            # Associate the public subnet with the route table\n",
    "            ec2_client.associate_route_table(\n",
    "                RouteTableId=route_table_id,\n",
    "                SubnetId=public_subnet_id\n",
    "            )\n",
    "            \n",
    "            # Modify the public subnet to auto-assign public IPs\n",
    "            ec2_client.modify_subnet_attribute(\n",
    "                SubnetId=public_subnet_id,\n",
    "                MapPublicIpOnLaunch={\"Value\": True}\n",
    "            )\n",
    "            break\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95af95c-f3a4-46d3-b5f8-b7d953eda3a9",
   "metadata": {},
   "source": [
    "# Create Security Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf11870e-3dee-4564-9c82-c78a0262ce77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Security Group...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'sg-0ffd603f400a8beea'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "security_group_ids = get_security_group_ids(ec2_client, TAGS)\n",
    "\n",
    "security_group_exists = len(security_group_ids) > 0\n",
    "\n",
    "if len(security_group_ids) > 1:\n",
    "    raise RuntimeError\n",
    "elif security_group_exists:\n",
    "    security_group_id = security_group_ids[0]\n",
    "\n",
    "# The security group gets torn down when deleted, so there is no need to check the rules and rewrite all of them.\n",
    "\n",
    "if not security_group_exists:\n",
    "    print('Creating Security Group...')\n",
    "    response = ec2_client.create_security_group(\n",
    "        GroupName=f'eks-cluster-sg-{CLUSTER_NAME}',\n",
    "        TagSpecifications=[\n",
    "            {\n",
    "                'ResourceType': 'security-group', \n",
    "                'Tags': [{'Key': tag, 'Value': TAGS[tag]} for tag in TAGS]\n",
    "            }\n",
    "        ],\n",
    "        Description=f'Security group for EKS cluster: {CLUSTER_NAME}',\n",
    "        VpcId=vpc_id\n",
    "    )\n",
    "    security_group_id = response['GroupId']\n",
    "\n",
    "    # TODO: Can I make the CidrIp more restrictive for the next deployment? Load Balancer needs to have a static IP, probably through the Kubernetes YAML?\n",
    "    for port in ALLOWED_PORTS:\n",
    "        ec2_client.authorize_security_group_ingress(\n",
    "            GroupId=security_group_id,\n",
    "            IpPermissions=[\n",
    "                {\n",
    "                    'IpProtocol': 'tcp',\n",
    "                    'FromPort': port,\n",
    "                    'ToPort': port,\n",
    "                    'IpRanges': [{'CidrIp': '0.0.0.0/0'}]\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "    \n",
    "security_group_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6a6e50f1-8540-4dcf-9aa6-46c0fd668aea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('vol-08de6efda65d040f1',\n",
       " 'vpc-0b49c534e4deeeebe',\n",
       " 'igw-01baa8f4c174bd1f1',\n",
       " 'rtb-002b13bc9ef438ff0',\n",
       " 'subnet-0d4bbaae525f5aebc',\n",
       " ['subnet-0d4bbaae525f5aebc', 'subnet-00051a3f971a8c23b'],\n",
       " 'sg-0ffd603f400a8beea')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "volume_id, vpc_id, igw_id, route_table_id, public_subnet_id, subnet_ids, security_group_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5e3e6a-1650-4a59-91b7-bff33b937a52",
   "metadata": {},
   "source": [
    "# Create Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e5e5fec8-eea5-4ef7-806c-4bdea1f61845",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ACTIVE'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tear_down_and_rebuild = False\n",
    "response = eks_client.list_clusters()\n",
    "clusters = response['clusters']\n",
    "if CLUSTER_NAME not in clusters:\n",
    "    eks_client.create_cluster(name=CLUSTER_NAME, \n",
    "                              version=K8S_VERSION, \n",
    "                              roleArn=f'arn:aws:iam::{aws_account_id}:role/EKS_Cluster_Role', \n",
    "                              resourcesVpcConfig={'subnetIds': subnet_ids,\n",
    "                                                  'securityGroupIds': [security_group_id],\n",
    "                                                  'endpointPublicAccess': True,\n",
    "                                                  'endpointPrivateAccess': False\n",
    "                              },\n",
    "                              tags=TAGS,\n",
    "                             )\n",
    "else:\n",
    "    if tear_down_and_rebuild:\n",
    "        pass\n",
    "        # TODO: Create a new cluster with a temp name.\n",
    "        # Wait until the cluster has finished forming\n",
    "        # Delete the old cluster\n",
    "        # Wait until deletion is complete\n",
    "        # Rename the new cluster\n",
    "        \n",
    "wait_until(eks_client.describe_cluster, {'name': CLUSTER_NAME}, is_cluster_active, timeout=7 * 60)\n",
    "response = eks_client.describe_cluster(name=CLUSTER_NAME)\n",
    "assert response['cluster']['status'] == 'ACTIVE'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7a812b-4a62-4b3b-a88e-90b0db82a194",
   "metadata": {},
   "source": [
    "# Create IAM Role for Node Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "49bcf925-9e7a-47af-9d1a-d160959148cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Role EKS_Cluster_Role already exists. Arn: arn:aws:iam::275678099358:role/EKS_Cluster_Role.\n",
      "Attached policy AmazonEKSWorkerNodePolicy to role EKS_Cluster_Role.\n",
      "Attached policy AmazonEC2ContainerRegistryReadOnly to role EKS_Cluster_Role.\n",
      "Attached policy AmazonEKS_CNI_Policy to role EKS_Cluster_Role.\n",
      "Attached policy AmazonEKSClusterPolicy to role EKS_Cluster_Role.\n"
     ]
    }
   ],
   "source": [
    "# Define the role name\n",
    "role_name = 'EKS_Cluster_Role'  # WARNING: I get a weird error if I call this role anything else. The call looks for this particular name, and I do not know how to override it.\n",
    "\n",
    "# Create the trust policy for the role\n",
    "trust_policy = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\n",
    "                \"Service\": [\n",
    "                    \"eks.amazonaws.com\",\n",
    "                    \"ec2.amazonaws.com\"\n",
    "                ]\n",
    "            },\n",
    "            \"Action\": \"sts:AssumeRole\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create the IAM role\n",
    "try:\n",
    "    response = iam_client.create_role(\n",
    "        RoleName=role_name,\n",
    "        AssumeRolePolicyDocument=json.dumps(trust_policy),\n",
    "        Description='Role for EKS Node Group'\n",
    "    )\n",
    "    node_role_arn = response['Role']['Arn']\n",
    "    print(f\"Created role: {node_role_arn}\")\n",
    "except iam_client.exceptions.EntityAlreadyExistsException:\n",
    "    response = iam_client.get_role(RoleName=role_name)\n",
    "    node_role_arn = response['Role']['Arn']\n",
    "    print(f\"Role {role_name} already exists. Arn: {node_role_arn}.\")\n",
    "    # TODO: Check if trust_policy is correct.\n",
    "\n",
    "# Attach necessary policies\n",
    "policies = [\n",
    "    'AmazonEKSWorkerNodePolicy',\n",
    "    'AmazonEC2ContainerRegistryReadOnly',\n",
    "    'AmazonEKS_CNI_Policy',\n",
    "    'AmazonEKSClusterPolicy',\n",
    "    # 'AmazonSSMManagedInstanceCore',\n",
    "]\n",
    "# Policy AmazonSSMManagedInstanceCore is not necessary, I used it for debugging, to connect to the node and run commands. \n",
    "\n",
    "for policy in policies:\n",
    "    try:\n",
    "        iam_client.attach_role_policy(\n",
    "            RoleName=role_name,\n",
    "            PolicyArn=f'arn:aws:iam::aws:policy/{policy}'\n",
    "        )\n",
    "        print(f\"Attached policy {policy} to role {role_name}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error attaching policy {policy}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8c8bd2-f80d-4a36-b648-4895429988ab",
   "metadata": {},
   "source": [
    "# Add Node Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "11c771d0-9929-4c01-8ae6-e7c0845760d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: This part may also need a wait."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "75cfa5fb-e965-4e51-bee6-8414c32a7689",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CREATING'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_groups = eks_client.list_nodegroups(clusterName=CLUSTER_NAME)\n",
    "# TODO: Make this static and put to the top, with different `diskSize` and `scalingConfig`\n",
    "node_group_name = 'gpu'\n",
    "ami_type = 'AL2_x86_64_GPU'\n",
    "if node_group_name in node_groups['nodegroups']:\n",
    "    response = eks_client.describe_nodegroup(clusterName=CLUSTER_NAME, nodegroupName=node_group_name)\n",
    "    status = response['nodegroup']['status']\n",
    "    if status == 'CREATE_FAILED':\n",
    "        print('Node group exists, but failed to create. Deleting...')\n",
    "        failed_node_group_info = response\n",
    "\n",
    "        eks_client.delete_nodegroup(clusterName=CLUSTER_NAME, nodegroupName=node_group_name)\n",
    "    \n",
    "        wait_until(eks_client.list_nodegroups, {'clusterName': CLUSTER_NAME}, lambda x: node_group_name not in x['nodegroups'])\n",
    "        node_groups = eks_client.list_nodegroups(clusterName=CLUSTER_NAME)\n",
    "    else:\n",
    "        print('Node group exists.')\n",
    "\n",
    "\n",
    "\n",
    "if not node_groups['nodegroups']:\n",
    "    print('Creating node group...')\n",
    "    response = eks_client.create_nodegroup(\n",
    "        clusterName=CLUSTER_NAME,\n",
    "        nodegroupName=node_group_name,\n",
    "        scalingConfig={\n",
    "            'desiredSize': 1,\n",
    "            'minSize': 1,\n",
    "            'maxSize': 1\n",
    "        },\n",
    "        diskSize=50,  # Size in GiB\n",
    "        subnets=[public_subnet_id],\n",
    "        nodeRole=node_role_arn,\n",
    "        amiType=ami_type,\n",
    "        instanceTypes=INSTANCE_TYPES[node_group_name],\n",
    "        labels={\n",
    "            'gpu-memory': 'true'\n",
    "        },\n",
    "        taints=[\n",
    "            {\n",
    "                'key': 'nvidia.com/gpu',\n",
    "                'value': 'true',\n",
    "                'effect': 'NO_SCHEDULE'\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "wait_until(eks_client.describe_nodegroup, {'clusterName': CLUSTER_NAME, 'nodegroupName': node_group_name}, is_node_group_active)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a69858b4-e5f2-4951-aa3d-69cd404c0e27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CREATING'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_groups = eks_client.list_nodegroups(clusterName=CLUSTER_NAME)\n",
    "# TODO: Make this static and put to the top, with different `diskSize` and `scalingConfig`\n",
    "node_group_name = 'default'\n",
    "ami_type = 'AL2_x86_64'\n",
    "if node_group_name in node_groups['nodegroups']:\n",
    "    response = eks_client.describe_nodegroup(clusterName=CLUSTER_NAME, nodegroupName=node_group_name)\n",
    "    status = response['nodegroup']['status']\n",
    "    if status == 'CREATE_FAILED':\n",
    "        print('Node group exists, but failed to create. Deleting...')\n",
    "        failed_node_group_info = response\n",
    "\n",
    "        eks_client.delete_nodegroup(clusterName=CLUSTER_NAME, nodegroupName=node_group_name)\n",
    "    \n",
    "        wait_until(eks_client.list_nodegroups, {'clusterName': CLUSTER_NAME}, lambda x: node_group_name not in x['nodegroups'])\n",
    "        node_groups = eks_client.list_nodegroups(clusterName=CLUSTER_NAME)\n",
    "    else:\n",
    "        print('Node group exists.')\n",
    "\n",
    "\n",
    "# TODO: See if lower disksize will work.\n",
    "if node_group_name not in node_groups['nodegroups']:\n",
    "    print('Creating node group...')\n",
    "    response = eks_client.create_nodegroup(\n",
    "        clusterName=CLUSTER_NAME,\n",
    "        nodegroupName=node_group_name,\n",
    "        scalingConfig={\n",
    "            'desiredSize': 2,\n",
    "            'minSize': 2,\n",
    "            'maxSize': 2\n",
    "        },\n",
    "        diskSize=1000,  # Size in GiB\n",
    "        subnets=[public_subnet_id],\n",
    "        nodeRole=node_role_arn,\n",
    "        amiType=ami_type,\n",
    "        instanceTypes=INSTANCE_TYPES[node_group_name],\n",
    "    )\n",
    "wait_until(eks_client.describe_nodegroup, {'clusterName': CLUSTER_NAME, 'nodegroupName': node_group_name}, is_node_group_active)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b295fbb6-d655-4386-94f0-84dfe0cbb454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instance i-0837ea964f91d3493 is of type g4dn.xlarge\n"
     ]
    }
   ],
   "source": [
    "# This is for double-checking the type of instance.\n",
    "asg_client = boto3.client(\"autoscaling\")\n",
    "node_group = eks_client.describe_nodegroup(clusterName=CLUSTER_NAME, nodegroupName='gpu')\n",
    "asg_name = node_group[\"nodegroup\"][\"resources\"][\"autoScalingGroups\"][0][\"name\"]\n",
    "asg_details = asg_client.describe_auto_scaling_groups(AutoScalingGroupNames=[asg_name])\n",
    "instance_ids = [instance[\"InstanceId\"] for instance in asg_details[\"AutoScalingGroups\"][0][\"Instances\"]]\n",
    "for instance_id in instance_ids:\n",
    "    instance_details = ec2_client.describe_instances(InstanceIds=[instance_id])\n",
    "    instance_type = instance_details[\"Reservations\"][0][\"Instances\"][0][\"InstanceType\"]\n",
    "    print(f\"Instance {instance_id} is of type {instance_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9df50a-ede3-4b60-af87-f7295796c94c",
   "metadata": {},
   "source": [
    "# Initialize Kubernetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "77edf8ec-0ec3-42d2-a8a6-f8dbf606364f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added new context arn:aws:eks:ca-central-1:275678099358:cluster/kubyterlab-llm to /root/.kube/config\n"
     ]
    }
   ],
   "source": [
    "!aws eks update-kubeconfig --name $CLUSTER_NAME --region $REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8ac935-b3bd-491f-9a4c-ec60bd3f3271",
   "metadata": {},
   "source": [
    "# Initialize Helm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9ec6ffe2-7458-4d1b-90dd-0b6fbbb0e921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"aws-ebs-csi-driver\" has been added to your repositories\n"
     ]
    }
   ],
   "source": [
    "!helm repo add aws-ebs-csi-driver https://kubernetes-sigs.github.io/aws-ebs-csi-driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1d3707f7-5505-4195-8666-748cda44c02a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"eks\" has been added to your repositories\n"
     ]
    }
   ],
   "source": [
    "!helm repo add eks https://aws.github.io/eks-charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5c979c04-3511-4ad9-a1f8-f2af1cdac0e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"ingress-nginx\" has been added to your repositories\n"
     ]
    }
   ],
   "source": [
    "!helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "79fbbaa4-e44b-4a43-bfd1-04d3a9dcf4fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hang tight while we grab the latest from your chart repositories...\n",
      "...Successfully got an update from the \"aws-ebs-csi-driver\" chart repository\n",
      "...Successfully got an update from the \"eks\" chart repository\n",
      "...Successfully got an update from the \"ingress-nginx\" chart repository\n",
      "Update Complete. ⎈Happy Helming!⎈\n"
     ]
    }
   ],
   "source": [
    "!helm repo update"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1566d62-4c89-4f87-84a1-5d465d8d89d4",
   "metadata": {},
   "source": [
    "# Create a Role to Install the CSI Driver and Give Permissions Using the OIDC Provider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "baafae37-f87f-47bb-a642-fc1c1c6cb5a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://oidc.eks.ca-central-1.amazonaws.com/id/730C91462F857AD933881D4B1C8C7C99'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_response = eks_client.describe_cluster(name=CLUSTER_NAME)\n",
    "oidc_id = cluster_response['cluster']['identity']['oidc']['issuer'].split('/')[-1]\n",
    "oidc_url = f'https://oidc.eks.{REGION}.amazonaws.com/id/{oidc_id}'\n",
    "oidc_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "47f875e6-9a22-4615-98a9-eb43c520fd94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b]11;?\u001b\\\u001b[6n\u001b[36m2025-05-08 01:11:32 [ℹ]  will create IAM Open ID Connect provider for cluster \"kubyterlab-llm\" in \"ca-central-1\"\n",
      "\u001b[0m\u001b[36m2025-05-08 01:11:33 [✔]  created IAM Open ID Connect provider for cluster \"kubyterlab-llm\" in \"ca-central-1\"\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!eksctl utils associate-iam-oidc-provider --cluster $CLUSTER_NAME --approve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "023b4e7f-f182-4474-a035-4027c0c75351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arn:aws:iam::275678099358:policy/AmazonEKS_EBS_CSI_Driver_Policy'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazon_eks_ebs_csi_driver_policy_name = 'AmazonEKS_EBS_CSI_Driver_Policy'\n",
    "amazon_eks_ebs_csi_driver_policy = {\n",
    "  \"Version\": \"2012-10-17\",\n",
    "  \"Statement\": [\n",
    "    {\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Action\": [\n",
    "        \"ec2:CreateSnapshot\",\n",
    "        \"ec2:AttachVolume\",\n",
    "        \"ec2:DetachVolume\",\n",
    "        \"ec2:ModifyVolume\",\n",
    "        \"ec2:DescribeAvailabilityZones\",\n",
    "        \"ec2:DescribeInstances\",\n",
    "        \"ec2:DescribeSnapshots\",\n",
    "        \"ec2:DescribeTags\",\n",
    "        \"ec2:DescribeVolumes\",\n",
    "        \"ec2:DescribeVolumesModifications\"\n",
    "      ],\n",
    "      \"Resource\": \"*\"\n",
    "    },\n",
    "    {\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Action\": [\n",
    "        \"ec2:CreateTags\"\n",
    "      ],\n",
    "      \"Resource\": [\n",
    "        \"arn:aws:ec2:*:*:volume/*\",\n",
    "        \"arn:aws:ec2:*:*:snapshot/*\"\n",
    "      ],\n",
    "      \"Condition\": {\n",
    "        \"StringEquals\": {\n",
    "          \"ec2:CreateAction\": [\n",
    "            \"CreateVolume\",\n",
    "            \"CreateSnapshot\"\n",
    "          ]\n",
    "        }\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Action\": [\n",
    "        \"ec2:DeleteTags\"\n",
    "      ],\n",
    "      \"Resource\": [\n",
    "        \"arn:aws:ec2:*:*:volume/*\",\n",
    "        \"arn:aws:ec2:*:*:snapshot/*\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Action\": [\n",
    "        \"ec2:CreateVolume\"\n",
    "      ],\n",
    "      \"Resource\": \"*\",\n",
    "      \"Condition\": {\n",
    "        \"StringLike\": {\n",
    "          \"aws:RequestTag/ebs.csi.aws.com/cluster\": \"true\"\n",
    "        }\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Action\": [\n",
    "        \"ec2:CreateVolume\"\n",
    "      ],\n",
    "      \"Resource\": \"*\",\n",
    "      \"Condition\": {\n",
    "        \"StringLike\": {\n",
    "          \"aws:RequestTag/CSIVolumeName\": \"*\"\n",
    "        }\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Action\": [\n",
    "        \"ec2:DeleteVolume\"\n",
    "      ],\n",
    "      \"Resource\": \"*\",\n",
    "      \"Condition\": {\n",
    "        \"StringLike\": {\n",
    "          \"ec2:ResourceTag/CSIVolumeName\": \"*\"\n",
    "        }\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Action\": [\n",
    "        \"ec2:DeleteVolume\"\n",
    "      ],\n",
    "      \"Resource\": \"*\",\n",
    "      \"Condition\": {\n",
    "        \"StringLike\": {\n",
    "          \"ec2:ResourceTag/ebs.csi.aws.com/cluster\": \"true\"\n",
    "        }\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Action\": [\n",
    "        \"ec2:DeleteSnapshot\"\n",
    "      ],\n",
    "      \"Resource\": \"*\",\n",
    "      \"Condition\": {\n",
    "        \"StringLike\": {\n",
    "          \"ec2:ResourceTag/CSIVolumeSnapshotName\": \"*\"\n",
    "        }\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Action\": [\n",
    "        \"ec2:DeleteSnapshot\"\n",
    "      ],\n",
    "      \"Resource\": \"*\",\n",
    "      \"Condition\": {\n",
    "        \"StringLike\": {\n",
    "          \"ec2:ResourceTag/ebs.csi.aws.com/cluster\": \"true\"\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "try:\n",
    "    response = iam_client.create_policy(\n",
    "        PolicyName=amazon_eks_ebs_csi_driver_policy_name,\n",
    "        PolicyDocument=json.dumps(amazon_eks_ebs_csi_driver_policy)\n",
    "    )\n",
    "    amazon_eks_ebs_csi_driver_policy_arn = response['Policy']['Arn']\n",
    "except iam_client.exceptions.EntityAlreadyExistsException:\n",
    "    amazon_eks_ebs_csi_driver_policy_arn = f'arn:aws:iam::{aws_account_id}:policy/{amazon_eks_ebs_csi_driver_policy_name}'\n",
    "    response = iam_client.get_policy(PolicyArn=amazon_eks_ebs_csi_driver_policy_arn)\n",
    "\n",
    "    # TODO: If policy body is not the same, update. Code to update follows.\n",
    "    # iam_client.create_policy_version(\n",
    "    #     PolicyArn=policy_arn,\n",
    "    #     PolicyDocument=policy_document_json,\n",
    "    #     SetAsDefault=True\n",
    "    # )\n",
    "\n",
    "amazon_eks_ebs_csi_driver_policy_arn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e4e0ebf6-a8ef-42f7-bc4d-ba852964f0da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Role already exists. Updating the trust policy.\n"
     ]
    }
   ],
   "source": [
    "trust_policy = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\n",
    "                \"Federated\": f\"arn:aws:iam::{aws_account_id}:oidc-provider/oidc.eks.{REGION}.amazonaws.com/id/{oidc_id}\"\n",
    "            },\n",
    "            \"Action\": \"sts:AssumeRoleWithWebIdentity\",\n",
    "            \"Condition\": {\n",
    "                \"StringEquals\": {\n",
    "                    f\"oidc.eks.{REGION}.amazonaws.com/id/{oidc_id}:aud\": \"sts.amazonaws.com\",\n",
    "                    f\"oidc.eks.{REGION}.amazonaws.com/id/{oidc_id}:sub\": \"system:serviceaccount:kube-system:ebs-csi-controller-sa\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "try:\n",
    "    create_role_response = iam_client.create_role(\n",
    "        RoleName='AmazonEKS_EBS_CSI_DriverRole',\n",
    "        \n",
    "        AssumeRolePolicyDocument=json.dumps(trust_policy),\n",
    "        Description='Role for Amazon EKS EBS CSI Driver'\n",
    "    )\n",
    "    print(f\"Role created successfully: {create_role_response['Role']['Arn']}\")\n",
    "except iam_client.exceptions.EntityAlreadyExistsException:\n",
    "    print(\"Role already exists. Updating the trust policy.\")\n",
    "    iam_client.update_assume_role_policy(\n",
    "        RoleName='AmazonEKS_EBS_CSI_DriverRole',\n",
    "        PolicyDocument=json.dumps(trust_policy)\n",
    "    )\n",
    "\n",
    "attach_policy_response = iam_client.attach_role_policy(\n",
    "    RoleName='AmazonEKS_EBS_CSI_DriverRole',\n",
    "    PolicyArn='arn:aws:iam::aws:policy/service-role/AmazonEBSCSIDriverPolicy'\n",
    ")\n",
    "\n",
    "attach_policy_response = iam_client.attach_role_policy(\n",
    "    RoleName='AmazonEKS_EBS_CSI_DriverRole',\n",
    "    PolicyArn=amazon_eks_ebs_csi_driver_policy_arn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa839d9c-00b3-41f3-ba6b-f70575889176",
   "metadata": {},
   "source": [
    "# Install CSI Driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e2451fff-9fa9-4f93-b6c1-f3b26924ac3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "serviceaccount/ebs-csi-controller-sa created\n"
     ]
    }
   ],
   "source": [
    "!kubectl create sa ebs-csi-controller-sa -n kube-system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fc410c14-878b-4af9-b960-40503d451a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "serviceaccount/ebs-csi-controller-sa annotated\n"
     ]
    }
   ],
   "source": [
    "!kubectl annotate sa ebs-csi-controller-sa -n kube-system eks.amazonaws.com/role-arn=$ebs_csi_driver_role_arn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c15647b1-29f9-445e-a115-68db3fbec3d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "serviceaccount/ebs-csi-controller-sa annotated\n"
     ]
    }
   ],
   "source": [
    "!kubectl annotate sa ebs-csi-controller-sa -n kube-system meta.helm.sh/release-namespace=kube-system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "feef5937-e243-4ae6-84ee-5be572ad1c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "serviceaccount/ebs-csi-controller-sa annotated\n"
     ]
    }
   ],
   "source": [
    "!kubectl annotate sa ebs-csi-controller-sa -n kube-system meta.helm.sh/release-name=aws-ebs-csi-driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b0b85917-e62c-42aa-8f71-7a0001265efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "serviceaccount/ebs-csi-controller-sa labeled\n"
     ]
    }
   ],
   "source": [
    "!kubectl label sa ebs-csi-controller-sa -n kube-system app.kubernetes.io/managed-by=Helm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "65c7ca0d-34d7-4b6f-93cb-f39379235bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Release \"aws-ebs-csi-driver\" does not exist. Installing it now.\n",
      "NAME: aws-ebs-csi-driver\n",
      "LAST DEPLOYED: Thu May  8 01:11:47 2025\n",
      "NAMESPACE: kube-system\n",
      "STATUS: deployed\n",
      "REVISION: 1\n",
      "NOTES:\n",
      "To verify that aws-ebs-csi-driver has started, run:\n",
      "\n",
      "    kubectl get pod -n kube-system -l \"app.kubernetes.io/name=aws-ebs-csi-driver,app.kubernetes.io/instance=aws-ebs-csi-driver\"\n",
      "\n",
      "[Deprecation announcement] AWS Snow Family device support for the EBS CSI Driver\n",
      "\n",
      "Support for the EBS CSI Driver on [AWS Snow Family devices](https://aws.amazon.com/snowball/) is deprecated, effective immediately. No further Snow-specific bugfixes or feature requests will be merged. The existing functionality for Snow devices will be removed the 1.44 release of the EBS CSI Driver. This announcement does not affect the support of the EBS CSI Driver on other platforms, such as [Amazon EC2](https://aws.amazon.com/ec2/) or EC2 on [AWS Outposts](https://aws.amazon.com/outposts/). For any questions related to this announcement, please comment on this issue [#2365](https://github.com/kubernetes-sigs/aws-ebs-csi-driver/issues/2365) or open a new issue.\n"
     ]
    }
   ],
   "source": [
    "!helm upgrade --install aws-ebs-csi-driver \\\n",
    "    --namespace kube-system \\\n",
    "    aws-ebs-csi-driver/aws-ebs-csi-driver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f599762-1e35-459c-a195-464eb9bd7f1c",
   "metadata": {},
   "source": [
    "### == End of Code Specific to AWS =="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bcb2b6-3c4b-4d3e-9d6c-2daf8baadfee",
   "metadata": {},
   "source": [
    "# Install Nvidia Device Plugin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2715339c-3b4c-4203-92e0-15b51e2fd75d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "daemonset.apps/nvidia-device-plugin-daemonset created\n"
     ]
    }
   ],
   "source": [
    "!kubectl apply -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/refs/heads/main/deployments/static/nvidia-device-plugin.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5a0d46-7a29-4c21-80d0-1ba0f5f9b5fd",
   "metadata": {},
   "source": [
    "# Apply Helm Charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3b606826-23b1-423b-8a87-b078d28afda9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'vol-08de6efda65d040f1'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "volume_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e803a8bf-61c4-422f-8ed9-8c9f04a46916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME: storage\n",
      "LAST DEPLOYED: Thu May  8 01:11:57 2025\n",
      "NAMESPACE: default\n",
      "STATUS: deployed\n",
      "REVISION: 1\n",
      "TEST SUITE: None\n"
     ]
    }
   ],
   "source": [
    "!helm install storage /helm/storage/ --set volumeID=$volume_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "66755be9-07cc-4d24-a0f3-bae41cecd91a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: INSTALLATION FAILED: cannot re-use a name that is still in use\n"
     ]
    }
   ],
   "source": [
    "!helm install ingress-nginx ingress-nginx/ingress-nginx --namespace ingress-nginx --create-namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b905deee-a92e-4bb3-b23c-789c090924a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !helm upgrade --install lb /helm/lb-ingress/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "17474e33-1f9b-448f-9819-f2b432063da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !helm delete lb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1dfa6262-ddc4-45ff-95ab-90d75fb6c46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Need to wait before applying the ingress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "373f30d2-a911-47ac-99ea-9f402c13db23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Release \"fastapi\" has been upgraded. Happy Helming!\n",
      "NAME: fastapi\n",
      "LAST DEPLOYED: Thu May  8 01:48:55 2025\n",
      "NAMESPACE: default\n",
      "STATUS: deployed\n",
      "REVISION: 3\n",
      "TEST SUITE: None\n"
     ]
    }
   ],
   "source": [
    "!helm upgrade --install fastapi /helm/eberron-agent-server/ --set awsAccountId=$aws_account_id --set region=$REGION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "74e4bd90-d8bd-421f-b0cb-9b90ecb046e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Release \"frontend\" has been upgraded. Happy Helming!\n",
      "NAME: frontend\n",
      "LAST DEPLOYED: Thu May  8 01:48:56 2025\n",
      "NAMESPACE: default\n",
      "STATUS: deployed\n",
      "REVISION: 2\n",
      "TEST SUITE: None\n"
     ]
    }
   ],
   "source": [
    "!helm upgrade --install frontend /helm/eberron-agent-frontend/ --set awsAccountId=$aws_account_id --set region=$REGION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "f0e0d633-e0aa-43d3-96ee-71ebe1163431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Release \"ingress\" has been upgraded. Happy Helming!\n",
      "NAME: ingress\n",
      "LAST DEPLOYED: Thu May  8 02:05:30 2025\n",
      "NAMESPACE: default\n",
      "STATUS: deployed\n",
      "REVISION: 5\n",
      "TEST SUITE: None\n"
     ]
    }
   ],
   "source": [
    "!helm upgrade --install ingress /helm/eberron-agent-ingress/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97f5d85-1064-4d39-b205-71fddb658689",
   "metadata": {},
   "source": [
    "# Get URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "6ec37f9a-6ffd-4e4e-b9a2-f0955ae0423a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fastapi-677965765c-mbglm'"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Update get_one_running_pod to use staring with names.\n",
    "wait_until(get_one_running_pod, {'prefix': 'fastapi-'}, lambda x: x is not None, timeout=8 * 60)\n",
    "pod_name = get_one_running_pod(prefix='fastapi-')\n",
    "assert pod_name is not None\n",
    "pod_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3cee78c8-55c3-43d9-bd54-258451b2520d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output = !kubectl get service lb --no-headers\n",
    "# fields = re.split(r'\\s+', output[0])\n",
    "# external_url = fields[3]\n",
    "# port = fields[4].split('/')[0].split(':')[0]\n",
    "# external_url, port"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "1ba7411d-d1ea-46b9-b6d9-eafe4f8fc546",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ac724a96eb1dd4d2999b5a46f12e9fca-957283017.ca-central-1.elb.amazonaws.com',\n",
       " '80')"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "output = !kubectl get service ingress-nginx-controller --namespace ingress-nginx --no-headers\n",
    "fields = re.split(r'\\s+', output[0])\n",
    "external_url = fields[3]\n",
    "port = fields[4].split('/')[0].split(':')[0]\n",
    "external_url, port"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "8bf520ca-59d0-427f-8251-bc940cad8e46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://ac724a96eb1dd4d2999b5a46f12e9fca-957283017.ca-central-1.elb.amazonaws.com:80'"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = f'http://{external_url}:{port}'\n",
    "url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c7f019-64b1-4b2d-a343-b536fca16633",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "438b3f0b-0cc8-4aad-b796-5e7bb32fd110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Defaulted container \"kubyterlab-llm\" out of: kubyterlab-llm, init-container (init)\n",
    "# True\n",
    "# 1\n",
    "!kubectl exec $pod_name bash -- python3 -c 'import torch; print(torch.cuda.is_available()); print(torch.cuda.device_count())'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "6bfde574-66a3-4a21-b26d-0cab8eac0893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory.total [MiB], memory.used [MiB], memory.free [MiB]\n",
      "15360 MiB, 3583 MiB, 11513 MiB\n"
     ]
    }
   ],
   "source": [
    "!kubectl exec $pod_name bash -- bash -c 'nvidia-smi --query-gpu=memory.total,memory.used,memory.free --format=csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "a3c0a9bf-9d43-4149-ac2c-229059865aaf",
   "metadata": {},
   "outputs": [
    {
     "ename": "ChunkedEncodingError",
     "evalue": "Response ended prematurely",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mProtocolError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/requests/models.py:820\u001b[39m, in \u001b[36mResponse.iter_content.<locals>.generate\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    819\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m820\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m.raw.stream(chunk_size, decode_content=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    821\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/urllib3/response.py:1063\u001b[39m, in \u001b[36mHTTPResponse.stream\u001b[39m\u001b[34m(self, amt, decode_content)\u001b[39m\n\u001b[32m   1062\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.chunked \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.supports_chunked_reads():\n\u001b[32m-> \u001b[39m\u001b[32m1063\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m.read_chunked(amt, decode_content=decode_content)\n\u001b[32m   1064\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/urllib3/response.py:1219\u001b[39m, in \u001b[36mHTTPResponse.read_chunked\u001b[39m\u001b[34m(self, amt, decode_content)\u001b[39m\n\u001b[32m   1218\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1219\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_update_chunk_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1220\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.chunk_left == \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/urllib3/response.py:1149\u001b[39m, in \u001b[36mHTTPResponse._update_chunk_length\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1147\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1148\u001b[39m     \u001b[38;5;66;03m# Truncated at start of next chunk\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1149\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ProtocolError(\u001b[33m\"\u001b[39m\u001b[33mResponse ended prematurely\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mProtocolError\u001b[39m: Response ended prematurely",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mChunkedEncodingError\u001b[39m                      Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[148]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      5\u001b[39m response = requests.post(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/respond\u001b[39m\u001b[33m\"\u001b[39m, \n\u001b[32m      6\u001b[39m                          headers={\u001b[33m\"\u001b[39m\u001b[33mContent-Type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mapplication/json\u001b[39m\u001b[33m\"\u001b[39m, \n\u001b[32m      7\u001b[39m                                   \u001b[33m\"\u001b[39m\u001b[33mAccept\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mtext/event-stream\u001b[39m\u001b[33m\"\u001b[39m}, \n\u001b[32m      8\u001b[39m                          json={\u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mWhat are the languages in Eberron?\u001b[39m\u001b[33m\"\u001b[39m}, \n\u001b[32m      9\u001b[39m                          stream=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     10\u001b[39m md = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmd\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclear_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/requests/models.py:822\u001b[39m, in \u001b[36mResponse.iter_content.<locals>.generate\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    820\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m.raw.stream(chunk_size, decode_content=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    821\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m822\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m DecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    824\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ContentDecodingError(e)\n",
      "\u001b[31mChunkedEncodingError\u001b[39m: Response ended prematurely"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "from IPython.display import clear_output, display, HTML, Markdown\n",
    "\n",
    "response = requests.post(f\"{url}/respond\", \n",
    "                         headers={\"Content-Type\": \"application/json\", \n",
    "                                  \"Accept\": \"text/event-stream\"}, \n",
    "                         json={\"content\": \"What are the languages in Eberron?\"}, \n",
    "                         stream=True)\n",
    "md = \"\"\n",
    "for chunk in response:\n",
    "    md += chunk.decode(\"utf-8\")\n",
    "    clear_output()\n",
    "    display(Markdown(md.replace('\\\\n', '\\n')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4ae1ff-9361-4709-8343-2b2310999403",
   "metadata": {},
   "source": [
    "# == End of Procedure =="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54d53ff-124d-409d-95f3-6985f7382654",
   "metadata": {},
   "source": [
    "## == Do Not Continue: rest of the code is only for reference for troubleshooting =="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bd4f0a-fa1d-4578-89d6-84b9c30c2179",
   "metadata": {},
   "source": [
    "# Troubleshoot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2af3512-4723-4393-8d3a-56bdc6fa64f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# curl -X POST -H \"Content-Type: application/json\" -H \"Accept: text/event-stream\" -d '{\"key\":\"value\"}' http://your-api-endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f10cb1-1b0f-4212-a689-052928a4924d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that for the following to work, you need to attach the policy AmazonSSMManagedInstanceCore, in the relevant cell above, during the deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "44b4b8fb-d7fb-4c9c-87ba-777bf78f1f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = !aws ssm send-command \\\n",
    "    --document-name \"AWS-RunShellScript\" \\\n",
    "    --targets \"Key=instanceIds,Values=i-02f3c1f8a118eb352\" \\\n",
    "    --parameters 'commands=[\"nvidia-smi --query-gpu=memory.total,memory.used,memory.free --format=csv\"]' \\\n",
    "    --region $REGION\n",
    "command_id = json.loads(''.join(output))['Command']['CommandId']\n",
    "command_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a45b62bf-644f-41ce-a4bf-f745311de116",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = !aws ssm list-command-invocations \\\n",
    "    --command-id $command_id \\\n",
    "    --details\n",
    "output = json.loads(''.join(output))['CommandInvocations'][0]['CommandPlugins'][0]['Output']\n",
    "for line in output.split('\\n'):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0480a8c3-9aad-4a76-9f62-20de23589501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAST SEEN   TYPE      REASON                    OBJECT                                             MESSAGE\n",
      "24m         Normal    Starting                  node/ip-10-0-1-102.ca-central-1.compute.internal   \n",
      "23m         Normal    Scheduled                 pod/fastapi-677965765c-mbglm                       Successfully assigned default/fastapi-677965765c-mbglm to ip-10-0-1-28.ca-central-1.compute.internal\n",
      "24m         Normal    Starting                  node/ip-10-0-1-120.ca-central-1.compute.internal   \n",
      "25m         Normal    Starting                  node/ip-10-0-1-28.ca-central-1.compute.internal    \n",
      "23m         Normal    Scheduled                 pod/frontend-7dc87ddddd-x4zjg                      Successfully assigned default/frontend-7dc87ddddd-x4zjg to ip-10-0-1-120.ca-central-1.compute.internal\n",
      "25m         Normal    NodeHasSufficientPID      node/ip-10-0-1-28.ca-central-1.compute.internal    Node ip-10-0-1-28.ca-central-1.compute.internal status is now: NodeHasSufficientPID\n",
      "25m         Normal    NodeAllocatableEnforced   node/ip-10-0-1-28.ca-central-1.compute.internal    Updated Node Allocatable limit across pods\n",
      "25m         Normal    NodeHasNoDiskPressure     node/ip-10-0-1-28.ca-central-1.compute.internal    Node ip-10-0-1-28.ca-central-1.compute.internal status is now: NodeHasNoDiskPressure\n",
      "25m         Normal    NodeHasSufficientMemory   node/ip-10-0-1-28.ca-central-1.compute.internal    Node ip-10-0-1-28.ca-central-1.compute.internal status is now: NodeHasSufficientMemory\n",
      "25m         Warning   InvalidDiskCapacity       node/ip-10-0-1-28.ca-central-1.compute.internal    invalid capacity 0 on image filesystem\n",
      "25m         Warning   CgroupV1                  node/ip-10-0-1-28.ca-central-1.compute.internal    Cgroup v1 support is in maintenance mode, please migrate to Cgroup v2.\n",
      "25m         Normal    Starting                  node/ip-10-0-1-28.ca-central-1.compute.internal    Starting kubelet.\n",
      "25m         Normal    Synced                    node/ip-10-0-1-28.ca-central-1.compute.internal    Node synced successfully\n",
      "25m         Normal    RegisteredNode            node/ip-10-0-1-28.ca-central-1.compute.internal    Node ip-10-0-1-28.ca-central-1.compute.internal event: Registered Node ip-10-0-1-28.ca-central-1.compute.internal in Controller\n",
      "25m         Normal    NodeReady                 node/ip-10-0-1-28.ca-central-1.compute.internal    Node ip-10-0-1-28.ca-central-1.compute.internal status is now: NodeReady\n",
      "25m         Normal    Starting                  node/ip-10-0-1-102.ca-central-1.compute.internal   Starting kubelet.\n",
      "25m         Normal    NodeAllocatableEnforced   node/ip-10-0-1-102.ca-central-1.compute.internal   Updated Node Allocatable limit across pods\n",
      "25m         Warning   InvalidDiskCapacity       node/ip-10-0-1-102.ca-central-1.compute.internal   invalid capacity 0 on image filesystem\n",
      "25m         Warning   CgroupV1                  node/ip-10-0-1-102.ca-central-1.compute.internal   Cgroup v1 support is in maintenance mode, please migrate to Cgroup v2.\n",
      "25m         Normal    NodeHasSufficientMemory   node/ip-10-0-1-102.ca-central-1.compute.internal   Node ip-10-0-1-102.ca-central-1.compute.internal status is now: NodeHasSufficientMemory\n",
      "25m         Normal    NodeHasNoDiskPressure     node/ip-10-0-1-102.ca-central-1.compute.internal   Node ip-10-0-1-102.ca-central-1.compute.internal status is now: NodeHasNoDiskPressure\n",
      "25m         Normal    NodeHasSufficientPID      node/ip-10-0-1-102.ca-central-1.compute.internal   Node ip-10-0-1-102.ca-central-1.compute.internal status is now: NodeHasSufficientPID\n",
      "25m         Normal    RegisteredNode            node/ip-10-0-1-102.ca-central-1.compute.internal   Node ip-10-0-1-102.ca-central-1.compute.internal event: Registered Node ip-10-0-1-102.ca-central-1.compute.internal in Controller\n",
      "25m         Normal    Synced                    node/ip-10-0-1-102.ca-central-1.compute.internal   Node synced successfully\n",
      "25m         Normal    NodeHasSufficientPID      node/ip-10-0-1-120.ca-central-1.compute.internal   Node ip-10-0-1-120.ca-central-1.compute.internal status is now: NodeHasSufficientPID\n",
      "25m         Normal    NodeAllocatableEnforced   node/ip-10-0-1-120.ca-central-1.compute.internal   Updated Node Allocatable limit across pods\n",
      "25m         Normal    Starting                  node/ip-10-0-1-120.ca-central-1.compute.internal   Starting kubelet.\n",
      "25m         Warning   InvalidDiskCapacity       node/ip-10-0-1-120.ca-central-1.compute.internal   invalid capacity 0 on image filesystem\n",
      "25m         Warning   CgroupV1                  node/ip-10-0-1-120.ca-central-1.compute.internal   Cgroup v1 support is in maintenance mode, please migrate to Cgroup v2.\n",
      "25m         Normal    NodeHasSufficientMemory   node/ip-10-0-1-120.ca-central-1.compute.internal   Node ip-10-0-1-120.ca-central-1.compute.internal status is now: NodeHasSufficientMemory\n",
      "25m         Normal    NodeHasNoDiskPressure     node/ip-10-0-1-120.ca-central-1.compute.internal   Node ip-10-0-1-120.ca-central-1.compute.internal status is now: NodeHasNoDiskPressure\n",
      "25m         Normal    Synced                    node/ip-10-0-1-120.ca-central-1.compute.internal   Node synced successfully\n",
      "24m         Normal    RegisteredNode            node/ip-10-0-1-120.ca-central-1.compute.internal   Node ip-10-0-1-120.ca-central-1.compute.internal event: Registered Node ip-10-0-1-120.ca-central-1.compute.internal in Controller\n",
      "24m         Normal    NodeReady                 node/ip-10-0-1-102.ca-central-1.compute.internal   Node ip-10-0-1-102.ca-central-1.compute.internal status is now: NodeReady\n",
      "24m         Normal    NodeReady                 node/ip-10-0-1-120.ca-central-1.compute.internal   Node ip-10-0-1-120.ca-central-1.compute.internal status is now: NodeReady\n",
      "23m         Warning   ProvisioningFailed        persistentvolumeclaim/pvc-llm                      storageclass.storage.k8s.io \"manual\" not found\n",
      "23m         Normal    SuccessfulCreate          replicaset/fastapi-677965765c                      Created pod: fastapi-677965765c-mbglm\n",
      "23m         Normal    ScalingReplicaSet         deployment/fastapi                                 Scaled up replica set fastapi-677965765c to 1\n",
      "23m         Normal    ScalingReplicaSet         deployment/frontend                                Scaled up replica set frontend-7dc87ddddd to 1\n",
      "23m         Normal    SuccessfulCreate          replicaset/frontend-7dc87ddddd                     Created pod: frontend-7dc87ddddd-x4zjg\n",
      "23m         Normal    Pulling                   pod/frontend-7dc87ddddd-x4zjg                      Pulling image \"275678099358.dkr.ecr.ca-central-1.amazonaws.com/multi-agent/eberron-agent-frontend:25.02\"\n",
      "23m         Normal    SuccessfulAttachVolume    pod/fastapi-677965765c-mbglm                       AttachVolume.Attach succeeded for volume \"pv-llm\"\n",
      "23m         Normal    Pulling                   pod/fastapi-677965765c-mbglm                       Pulling image \"275678099358.dkr.ecr.ca-central-1.amazonaws.com/multi-agent/eberron-agent-server:25.02\"\n",
      "23m         Normal    Pulled                    pod/frontend-7dc87ddddd-x4zjg                      Successfully pulled image \"275678099358.dkr.ecr.ca-central-1.amazonaws.com/multi-agent/eberron-agent-frontend:25.02\" in 10.544s (10.544s including waiting). Image size: 292894546 bytes.\n",
      "23m         Normal    Created                   pod/frontend-7dc87ddddd-x4zjg                      Created container: frontend\n",
      "23m         Normal    Started                   pod/frontend-7dc87ddddd-x4zjg                      Started container frontend\n",
      "17m         Normal    Pulled                    pod/fastapi-677965765c-mbglm                       Successfully pulled image \"275678099358.dkr.ecr.ca-central-1.amazonaws.com/multi-agent/eberron-agent-server:25.02\" in 5m36.792s (5m36.792s including waiting). Image size: 14071664304 bytes.\n",
      "17m         Normal    Started                   pod/fastapi-677965765c-mbglm                       Started container fastapi\n",
      "17m         Normal    Created                   pod/fastapi-677965765c-mbglm                       Created container: fastapi\n",
      "86s         Normal    Sync                      ingress/ingress                                    Scheduled for sync\n"
     ]
    }
   ],
   "source": [
    "!kubectl get events --sort-by='.lastTimestamp'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d39f876b-ea67-4dd5-a9ee-92af8f218ce9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'instance_id' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[94], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43minstance_id\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'instance_id' is not defined"
     ]
    }
   ],
   "source": [
    "instance_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a8992595-dd14-4067-8607-3596e7fa746c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                  READY   STATUS    RESTARTS   AGE\n",
      "ebs-csi-controller-76c8cd8ccf-ntct4   5/5     Running   0          24m\n",
      "ebs-csi-controller-76c8cd8ccf-wc45h   5/5     Running   0          24m\n",
      "ebs-csi-node-2dblk                    3/3     Running   0          24m\n",
      "ebs-csi-node-kvqq9                    3/3     Running   0          24m\n",
      "ebs-csi-node-n8kfp                    3/3     Running   0          24m\n"
     ]
    }
   ],
   "source": [
    "!kubectl get pod -n kube-system -l \"app.kubernetes.io/name=aws-ebs-csi-driver,app.kubernetes.io/instance=aws-ebs-csi-driver\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "3b9460f8-e4d4-4247-908f-22706f2d876d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                          GPU\n",
      "ip-10-0-1-102.ca-central-1.compute.internal   <none>\n",
      "ip-10-0-1-120.ca-central-1.compute.internal   <none>\n",
      "ip-10-0-1-28.ca-central-1.compute.internal    1\n"
     ]
    }
   ],
   "source": [
    "# NAME                                          GPU\n",
    "# ip-10-0-1-129.ca-central-1.compute.internal   <none>\n",
    "# ip-10-0-1-223.ca-central-1.compute.internal   <none>\n",
    "# ip-10-0-1-8.ca-central-1.compute.internal     1\n",
    "!kubectl get nodes \"-o=custom-columns=NAME:.metadata.name,GPU:.status.allocatable.nvidia\\.com/gpu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "5d4e2db7-eec8-484d-af50-ff23a339c8f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                          TAINTS\n",
      "ip-10-0-1-102.ca-central-1.compute.internal   <none>\n",
      "ip-10-0-1-120.ca-central-1.compute.internal   <none>\n",
      "ip-10-0-1-28.ca-central-1.compute.internal    [map[effect:NoSchedule key:nvidia.com/gpu value:true]]\n"
     ]
    }
   ],
   "source": [
    "# NAME                                          TAINTS\n",
    "# ip-10-0-1-107.ca-central-1.compute.internal   <none>\n",
    "# ip-10-0-1-175.ca-central-1.compute.internal   [map[effect:NoSchedule key:nvidia.com/gpu value:true]]\n",
    "# ip-10-0-1-90.ca-central-1.compute.internal    <none>\n",
    "!kubectl get nodes -o custom-columns=NAME:.metadata.name,TAINTS:.spec.taints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "35f0a916-0089-4030-97aa-a074fe340403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME         TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)   AGE\n",
      "fastapi      ClusterIP   172.20.5.77      <none>        80/TCP    24m\n",
      "frontend     ClusterIP   172.20.206.206   <none>        80/TCP    24m\n",
      "kubernetes   ClusterIP   172.20.0.1       <none>        443/TCP   29m\n"
     ]
    }
   ],
   "source": [
    "# NAME                     TYPE           CLUSTER-IP     EXTERNAL-IP                                                                  PORT(S)          AGE\n",
    "# kubernetes               ClusterIP      172.20.0.1     <none>                                                                       443/TCP          37m\n",
    "# kubyterlab-llm-service   LoadBalancer   172.20.129.8   a4740e3e56bfe40ac81121bd46071903-1377611187.ca-central-1.elb.amazonaws.com   8888:31434/TCP   13s\n",
    "!kubectl get service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3cf151d7-7c1f-4bf8-963b-b510e1affaca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                        READY   STATUS    RESTARTS   AGE\n",
      "fastapi-677965765c-mbglm    1/1     Running   0          24m\n",
      "frontend-7dc87ddddd-x4zjg   1/1     Running   0          24m\n"
     ]
    }
   ],
   "source": [
    "!kubectl get pod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "fde9f7f4-b4cc-4834-9ccc-e69f70ffbbad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME       READY   UP-TO-DATE   AVAILABLE   AGE\n",
      "fastapi    1/1     1            1           24m\n",
      "frontend   1/1     1            1           24m\n"
     ]
    }
   ],
   "source": [
    "# NAME               READY   UP-TO-DATE   AVAILABLE   AGE\n",
    "# kubyterlab-llm-pod   1/1     1            1           31m\n",
    "!kubectl get deployments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5663728c-8285-4265-bb0f-eef82a989578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME      CLASS   HOSTS       ADDRESS                                                                     PORTS   AGE\n",
      "ingress   nginx   localhost   ac724a96eb1dd4d2999b5a46f12e9fca-957283017.ca-central-1.elb.amazonaws.com   80      3m7s\n"
     ]
    }
   ],
   "source": [
    "!kubectl get ingress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "54de0206-648b-48ef-a2f0-ccca684a3293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME              STATUS   AGE\n",
      "default           Active   29m\n",
      "ingress-nginx     Active   24m\n",
      "kube-node-lease   Active   29m\n",
      "kube-public       Active   29m\n",
      "kube-system       Active   29m\n"
     ]
    }
   ],
   "source": [
    "!kubectl get namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5a81be83-efd5-4afe-8e91-45f1bfa25703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME      CLASS   HOSTS       ADDRESS                                                                     PORTS   AGE     LABELS\n",
      "ingress   nginx   localhost   ac724a96eb1dd4d2999b5a46f12e9fca-957283017.ca-central-1.elb.amazonaws.com   80      3m14s   app.kubernetes.io/managed-by=Helm\n"
     ]
    }
   ],
   "source": [
    "!kubectl get ingress --show-labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3f118af3-dd86-43fd-b6b3-ead78a96dbb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                 TYPE           CLUSTER-IP       EXTERNAL-IP                                                                 PORT(S)                      AGE\n",
      "ingress-nginx-controller             LoadBalancer   172.20.107.114   ac724a96eb1dd4d2999b5a46f12e9fca-957283017.ca-central-1.elb.amazonaws.com   80:31295/TCP,443:31123/TCP   24m\n",
      "ingress-nginx-controller-admission   ClusterIP      172.20.35.192    <none>                                                                      443/TCP                      24m\n"
     ]
    }
   ],
   "source": [
    "!kubectl get svc -n ingress-nginx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c6834900-1ce4-4684-ae72-878f87519318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                          STATUS   ROLES    AGE   VERSION\n",
      "ip-10-0-1-102.ca-central-1.compute.internal   Ready    <none>   26m   v1.31.7-eks-473151a\n",
      "ip-10-0-1-120.ca-central-1.compute.internal   Ready    <none>   26m   v1.31.7-eks-473151a\n",
      "ip-10-0-1-28.ca-central-1.compute.internal    Ready    <none>   27m   v1.31.7-eks-473151a\n"
     ]
    }
   ],
   "source": [
    "!kubectl get nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c1f58371-742b-41ae-a44e-f9d9dcb6f977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.0.1.28 - - [08/May/2025:01:34:19 +0000] \"GET /static/js/textarea.BxksmrAF.js HTTP/1.1\" 200 2285 \"-\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36\" 581 0.003 [default-frontend-80] [] 10.0.1.179:8501 2285 0.003 200 ae1902649a082cb195e06e479d713426\n",
      "10.0.1.28 - - [08/May/2025:01:34:56 +0000] \"GET /static/media/SourceCodePro-Regular.CBOlD63d.woff2 HTTP/1.1\" 200 74052 \"http://ac724a96eb1dd4d2999b5a46f12e9fca-957283017.ca-central-1.elb.amazonaws.com/static/css/index.mUTQuMqR.css\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36\" 721 0.001 [default-frontend-80] [] 10.0.1.179:8501 74052 0.001 200 46aa8024df1d07b533a3d7ad39871051\n",
      "2025/05/08 01:35:04 [error] 98#98: *9349 connect() failed (111: Connection refused) while connecting to upstream, client: 10.0.1.28, server: _, request: \"POST /respond HTTP/1.1\", upstream: \"http://10.0.1.154:8000/respond\", host: \"ac724a96eb1dd4d2999b5a46f12e9fca-957283017.ca-central-1.elb.amazonaws.com\"\n",
      "2025/05/08 01:35:04 [error] 98#98: *9349 connect() failed (111: Connection refused) while connecting to upstream, client: 10.0.1.28, server: _, request: \"POST /respond HTTP/1.1\", upstream: \"http://10.0.1.154:8000/respond\", host: \"ac724a96eb1dd4d2999b5a46f12e9fca-957283017.ca-central-1.elb.amazonaws.com\"\n",
      "2025/05/08 01:35:04 [error] 98#98: *9349 connect() failed (111: Connection refused) while connecting to upstream, client: 10.0.1.28, server: _, request: \"POST /respond HTTP/1.1\", upstream: \"http://10.0.1.154:8000/respond\", host: \"ac724a96eb1dd4d2999b5a46f12e9fca-957283017.ca-central-1.elb.amazonaws.com\"\n",
      "10.0.1.28 - - [08/May/2025:01:35:04 +0000] \"POST /respond HTTP/1.1\" 502 150 \"-\" \"python-requests/2.32.3\" 327 0.002 [default-fastapi-80] [] 10.0.1.154:8000, 10.0.1.154:8000, 10.0.1.154:8000 0, 0, 0 0.001, 0.000, 0.000 502, 502, 502 2bf6a4748467fc55dc2d015e836f3499\n",
      "2025/05/08 01:35:06 [error] 98#98: *9371 connect() failed (111: Connection refused) while connecting to upstream, client: 10.0.1.120, server: _, request: \"POST /respond HTTP/1.1\", upstream: \"http://10.0.1.154:8000/respond\", host: \"ac724a96eb1dd4d2999b5a46f12e9fca-957283017.ca-central-1.elb.amazonaws.com\"\n",
      "2025/05/08 01:35:06 [error] 98#98: *9371 connect() failed (111: Connection refused) while connecting to upstream, client: 10.0.1.120, server: _, request: \"POST /respond HTTP/1.1\", upstream: \"http://10.0.1.154:8000/respond\", host: \"ac724a96eb1dd4d2999b5a46f12e9fca-957283017.ca-central-1.elb.amazonaws.com\"\n",
      "2025/05/08 01:35:06 [error] 98#98: *9371 connect() failed (111: Connection refused) while connecting to upstream, client: 10.0.1.120, server: _, request: \"POST /respond HTTP/1.1\", upstream: \"http://10.0.1.154:8000/respond\", host: \"ac724a96eb1dd4d2999b5a46f12e9fca-957283017.ca-central-1.elb.amazonaws.com\"\n",
      "10.0.1.120 - - [08/May/2025:01:35:06 +0000] \"POST /respond HTTP/1.1\" 502 150 \"-\" \"python-requests/2.32.3\" 327 0.001 [default-fastapi-80] [] 10.0.1.154:8000, 10.0.1.154:8000, 10.0.1.154:8000 0, 0, 0 0.000, 0.000, 0.001 502, 502, 502 5408f57420bd2195e26bc7996ae7edfa\n"
     ]
    }
   ],
   "source": [
    "!kubectl logs -l app.kubernetes.io/name=ingress-nginx -n ingress-nginx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "191dac73-b376-4f60-9792-a12df4721e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME              \tNAMESPACE    \tREVISION\tUPDATED                                \tSTATUS  \tCHART                    \tAPP VERSION                \n",
      "aws-ebs-csi-driver\tkube-system  \t1       \t2025-05-08 01:11:47.527936341 +0000 UTC\tdeployed\taws-ebs-csi-driver-2.43.0\t1.43.0                     \n",
      "fastapi           \tdefault      \t2       \t2025-05-08 01:35:31.255953683 +0000 UTC\tdeployed\tfastapi-0.1.0            \t{{ .Values.image.version }}\n",
      "frontend          \tdefault      \t1       \t2025-05-08 01:12:21.610688314 +0000 UTC\tdeployed\tfrontend-0.1.0           \t{{ .Values.image.version }}\n",
      "ingress           \tdefault      \t2       \t2025-05-08 01:33:48.995036113 +0000 UTC\tdeployed\tingress-0.1.0            \t0.1.0                      \n",
      "ingress-nginx     \tingress-nginx\t1       \t2025-05-08 01:12:01.671272824 +0000 UTC\tdeployed\tingress-nginx-4.12.2     \t1.12.2                     \n",
      "storage           \tdefault      \t1       \t2025-05-08 01:11:57.26943578 +0000 UTC \tdeployed\tstorage-0.1.0            \t                           \n"
     ]
    }
   ],
   "source": [
    "!helm list -A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8b31af91-2cff-4bd4-811d-a405ccbe7939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER-SUPPLIED VALUES:\n",
      "null\n"
     ]
    }
   ],
   "source": [
    "!helm get values ingress-nginx -n ingress-nginx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "2621d2b0-aa9a-43e1-a7ef-1b07c1be73c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME    CONTROLLER             PARAMETERS   AGE\n",
      "nginx   k8s.io/ingress-nginx   <none>       51m\n"
     ]
    }
   ],
   "source": [
    "!kubectl get ingressclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "0d727659-1be2-45c8-97db-48133f64865f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:             ingress\n",
      "Labels:           app.kubernetes.io/managed-by=Helm\n",
      "Namespace:        default\n",
      "Address:          ac724a96eb1dd4d2999b5a46f12e9fca-957283017.ca-central-1.elb.amazonaws.com\n",
      "Ingress Class:    nginx\n",
      "Default backend:  <default>\n",
      "Rules:\n",
      "  Host        Path  Backends\n",
      "  ----        ----  --------\n",
      "  localhost   \n",
      "              /respond   fastapi:80 (10.0.1.154:8000)\n",
      "              /          frontend:80 (10.0.1.179:8501)\n",
      "Annotations:  meta.helm.sh/release-name: ingress\n",
      "              meta.helm.sh/release-namespace: default\n",
      "              nginx.ingress.kubernetes.io/cors-allow-methods: POST,GET,PUT,DELETE,OPTIONS\n",
      "              nginx.ingress.kubernetes.io/enable-methods: POST,GET,PUT,DELETE,OPTIONS\n",
      "              nginx.ingress.kubernetes.io/proxy-read-timeout: 3600\n",
      "              nginx.ingress.kubernetes.io/proxy-send-timeout: 3600\n",
      "Events:\n",
      "  Type    Reason  Age                  From                      Message\n",
      "  ----    ------  ----                 ----                      -------\n",
      "  Normal  Sync    2m35s (x3 over 27m)  nginx-ingress-controller  Scheduled for sync\n"
     ]
    }
   ],
   "source": [
    "!kubectl describe ingress ingress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "787d64b3-aded-45b9-bcb5-cfcddb81ef33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'frontend-7dc87ddddd-x4zjg'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wait_until(get_one_running_pod, {'prefix': 'frontend-'}, lambda x: x is                                       not None, timeout=8 * 60)\n",
    "frontend_pod_name = get_one_running_pod(prefix='frontend-')\n",
    "assert frontend_pod_name is not None\n",
    "frontend_pod_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "2ca977a7-493f-428d-a15f-d142beb64492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
      "\n",
      "\n",
      "  You can now view your Streamlit app in your browser.\n",
      "\n",
      "  Local URL: http://localhost:8501\n",
      "  Network URL: http://10.0.1.179:8501\n",
      "  External URL: http://35.183.177.21:8501\n",
      "\n",
      "2025-05-08 01:34:55.877 Uncaught app execution\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/site-packages/urllib3/connection.py\", line 198, in _new_conn\n",
      "    sock = connection.create_connection(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/urllib3/util/connection.py\", line 85, in create_connection\n",
      "    raise err\n",
      "  File \"/usr/local/lib/python3.11/site-packages/urllib3/util/connection.py\", line 73, in create_connection\n",
      "    sock.connect(sa)\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 787, in urlopen\n",
      "    response = self._make_request(\n",
      "               ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 493, in _make_request\n",
      "    conn.request(\n",
      "  File \"/usr/local/lib/python3.11/site-packages/urllib3/connection.py\", line 445, in request\n",
      "    self.endheaders()\n",
      "  File \"/usr/local/lib/python3.11/http/client.py\", line 1298, in endheaders\n",
      "    self._send_output(message_body, encode_chunked=encode_chunked)\n",
      "  File \"/usr/local/lib/python3.11/http/client.py\", line 1058, in _send_output\n",
      "    self.send(msg)\n",
      "  File \"/usr/local/lib/python3.11/http/client.py\", line 996, in send\n",
      "    self.connect()\n",
      "  File \"/usr/local/lib/python3.11/site-packages/urllib3/connection.py\", line 276, in connect\n",
      "    self.sock = self._new_conn()\n",
      "                ^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/urllib3/connection.py\", line 213, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7fe584e8e010>: Failed to establish a new connection: [Errno 111] Connection refused\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/site-packages/requests/adapters.py\", line 667, in send\n",
      "    resp = conn.urlopen(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 841, in urlopen\n",
      "    retries = retries.increment(\n",
      "              ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/urllib3/util/retry.py\", line 519, in increment\n",
      "    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='fastapi', port=80): Max retries exceeded with url: /respond (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fe584e8e010>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/site-packages/streamlit/runtime/scriptrunner/exec_code.py\", line 121, in exec_func_with_error_handling\n",
      "    result = func()\n",
      "             ^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/streamlit/runtime/scriptrunner/script_runner.py\", line 591, in code_to_exec\n",
      "    exec(code, module.__dict__)\n",
      "  File \"/app/streamlit_app.py\", line 44, in <module>\n",
      "    response = st.write_stream(stream)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/streamlit/runtime/metrics_util.py\", line 410, in wrapped_func\n",
      "    result = non_optional_func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/streamlit/elements/write.py\", line 189, in write_stream\n",
      "    for chunk in stream:  # type: ignore\n",
      "  File \"/app/streamlit_app.py\", line 11, in responder\n",
      "    response = requests.post(f\"{url}/respond\",\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/requests/api.py\", line 115, in post\n",
      "    return request(\"post\", url, data=data, json=json, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/requests/api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/requests/sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/requests/sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/requests/adapters.py\", line 700, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPConnectionPool(host='fastapi', port=80): Max retries exceeded with url: /respond (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fe584e8e010>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "2025-05-08 01:40:51.090 Uncaught app execution\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/site-packages/urllib3/connection.py\", line 198, in _new_conn\n",
      "    sock = connection.create_connection(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/urllib3/util/connection.py\", line 85, in create_connection\n",
      "    raise err\n",
      "  File \"/usr/local/lib/python3.11/site-packages/urllib3/util/connection.py\", line 73, in create_connection\n",
      "    sock.connect(sa)\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 787, in urlopen\n",
      "    response = self._make_request(\n",
      "               ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 493, in _make_request\n",
      "    conn.request(\n",
      "  File \"/usr/local/lib/python3.11/site-packages/urllib3/connection.py\", line 445, in request\n",
      "    self.endheaders()\n",
      "  File \"/usr/local/lib/python3.11/http/client.py\", line 1298, in endheaders\n",
      "    self._send_output(message_body, encode_chunked=encode_chunked)\n",
      "  File \"/usr/local/lib/python3.11/http/client.py\", line 1058, in _send_output\n",
      "    self.send(msg)\n",
      "  File \"/usr/local/lib/python3.11/http/client.py\", line 996, in send\n",
      "    self.connect()\n",
      "  File \"/usr/local/lib/python3.11/site-packages/urllib3/connection.py\", line 276, in connect\n",
      "    self.sock = self._new_conn()\n",
      "                ^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/urllib3/connection.py\", line 213, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7fe561936110>: Failed to establish a new connection: [Errno 111] Connection refused\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/site-packages/requests/adapters.py\", line 667, in send\n",
      "    resp = conn.urlopen(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 841, in urlopen\n",
      "    retries = retries.increment(\n",
      "              ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/urllib3/util/retry.py\", line 519, in increment\n",
      "    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='fastapi', port=80): Max retries exceeded with url: /respond (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fe561936110>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/site-packages/streamlit/runtime/scriptrunner/exec_code.py\", line 121, in exec_func_with_error_handling\n",
      "    result = func()\n",
      "             ^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/streamlit/runtime/scriptrunner/script_runner.py\", line 591, in code_to_exec\n",
      "    exec(code, module.__dict__)\n",
      "  File \"/app/streamlit_app.py\", line 44, in <module>\n",
      "    response = st.write_stream(stream)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/streamlit/runtime/metrics_util.py\", line 410, in wrapped_func\n",
      "    result = non_optional_func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/streamlit/elements/write.py\", line 189, in write_stream\n",
      "    for chunk in stream:  # type: ignore\n",
      "  File \"/app/streamlit_app.py\", line 11, in responder\n",
      "    response = requests.post(f\"{url}/respond\",\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/requests/api.py\", line 115, in post\n",
      "    return request(\"post\", url, data=data, json=json, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/requests/api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/requests/sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/requests/sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/requests/adapters.py\", line 700, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPConnectionPool(host='fastapi', port=80): Max retries exceeded with url: /respond (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fe561936110>: Failed to establish a new connection: [Errno 111] Connection refused'))\n"
     ]
    }
   ],
   "source": [
    "!kubectl logs $frontend_pod_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b715f925-0cee-4bf1-9368-f39e45740e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pod \"frontend-7dc87ddddd-6x8q8\" deleted\n"
     ]
    }
   ],
   "source": [
    "!kubectl delete pod $frontend_pod_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e459b1a6-a8cd-408f-9e24-5b9a810940b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                          STATUS   ROLES    AGE   VERSION\n",
      "ip-10-0-1-118.ca-central-1.compute.internal   Ready    <none>   93m   v1.31.5-eks-5d632ec\n",
      "ip-10-0-1-19.ca-central-1.compute.internal    Ready    <none>   92m   v1.31.5-eks-5d632ec\n",
      "ip-10-0-1-35.ca-central-1.compute.internal    Ready    <none>   92m   v1.31.5-eks-5d632ec\n"
     ]
    }
   ],
   "source": [
    "!kubectl get nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "f63db4e1-29e3-479e-8db9-13b6b2a1c8fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ingress-nginx-controller-86578cc48d-cz9zf'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = !kubectl get pods -n ingress-nginx --no-headers\n",
    "fields = re.split(r'\\s+', output[0])\n",
    "ingress_controller_pod = fields[0]\n",
    "ingress_controller_pod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a6b04770-c2d7-400f-aa14-4ffed17a9258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Host frontend.default.svc.cluster.local:80 was resolved.\n",
      "* IPv6: (none)\n",
      "* IPv4: 172.20.206.206\n",
      "*   Trying 172.20.206.206:80...\n",
      "* Connected to frontend.default.svc.cluster.local (172.20.206.206) port 80\n",
      "* using HTTP/1.x\n",
      "> GET / HTTP/1.1\n",
      "> Host: frontend.default.svc.cluster.local\n",
      "> User-Agent: curl/8.12.1\n",
      "> Accept: */*\n",
      "> \n",
      "* Request completely sent off\n",
      "< HTTP/1.1 200 OK\n",
      "< Server: TornadoServer/6.4.2\n",
      "< Content-Type: text/html\n",
      "< Date: Thu, 08 May 2025 01:39:09 GMT\n",
      "< Accept-Ranges: bytes\n",
      "< Etag: \"dbab17a21eaa5481e5ea74ed9b2a58f9cddc1fa1c62651ba7aaaf6016c1781d9d3404eda0c9aa965c79e88d417929d1dfdda09325d0e845b3b42b7b0b6a642cd\"\n",
      "< Last-Modified: Thu, 27 Feb 2025 03:24:13 GMT\n",
      "< Cache-Control: no-cache\n",
      "< Content-Length: 1837\n",
      "< Vary: Accept-Encoding\n",
      "< \n",
      "<!--\n",
      " Copyright (c) Streamlit Inc. (2018-2022) Snowflake Inc. (2022-2025)\n",
      "\n",
      " Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      " you may not use this file except in compliance with the License.\n",
      " You may obtain a copy of the License at\n",
      "\n",
      "     http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      " Unless required by applicable law or agreed to in writing, software\n",
      " distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      " WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      " See the License for the specific language governing permissions and\n",
      " limitations under the License.\n",
      "-->\n",
      "\n",
      "<!DOCTYPE html>\n",
      "<html lang=\"en\">\n",
      "  <head>\n",
      "    <meta charset=\"UTF-8\" />\n",
      "    <meta\n",
      "      name=\"viewport\"\n",
      "      content=\"width=device-width, initial-scale=1, shrink-to-fit=no\"\n",
      "    />\n",
      "    <link rel=\"shortcut icon\" href=\"./favicon.png\" />\n",
      "    <link\n",
      "      rel=\"preload\"\n",
      "      href=\"./static/media/SourceSansPro-Regular.DZLUzqI4.woff2\"\n",
      "      as=\"font\"\n",
      "      type=\"font/woff2\"\n",
      "      crossorigin\n",
      "    />\n",
      "    <link\n",
      "      rel=\"preload\"\n",
      "      href=\"./static/media/SourceSansPro-SemiBold.sKQIyTMz.woff2\"\n",
      "      as=\"font\"\n",
      "      type=\"font/woff2\"\n",
      "      crossorigin\n",
      "    />\n",
      "    <link\n",
      "      rel=\"preload\"\n",
      "      href=\"./static/media/SourceSansPro-Bold.-6c9oR8J.woff2\"\n",
      "      as=\"font\"\n",
      "      type=\"font/woff2\"\n",
      "      crossorigin\n",
      "    />\n",
      "\n",
      "    <title>Streamlit</title>\n",
      "\n",
      "    <!-- initialize window.prerenderReady to false and then set to true in React app when app is ready for indexing -->\n",
      "    <script>\n",
      "      window.prerenderReady = false\n",
      "    </script>\n",
      "    <script type=\"module\" crossorigin src=\"./static/js/index.tKq1MI69.js\"></script>\n",
      "    <link rel=\"stylesheet\" crossorigin href=\"./static/css/index.mUTQuMqR.css\">\n",
      "  </head>\n",
      "  <body>\n",
      "    <noscript>You need to enable JavaScript to run this app.</noscript>\n",
      "    <div id=\"root\"></div>\n",
      "  </body>\n",
      "</html>\n",
      "* Connection #0 to host frontend.default.svc.cluster.local left intact\n"
     ]
    }
   ],
   "source": [
    "!kubectl exec -it $ingress_controller_pod -n ingress-nginx -- curl -v frontend.default.svc.cluster.local:80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "79853d58-a743-441b-971c-218dea063d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Host frontend.default.svc.cluster.local:80 was resolved.\n",
      "* IPv6: (none)\n",
      "* IPv4: 172.20.41.240\n",
      "*   Trying 172.20.41.240:80...\n",
      "* Connected to frontend.default.svc.cluster.local (172.20.41.240) port 80\n",
      "* using HTTP/1.x\n",
      "> GET / HTTP/1.1\n",
      "> Host: frontend.default.svc.cluster.local\n",
      "> User-Agent: curl/8.11.1\n",
      "> Accept: */*\n",
      "> \n",
      "* Request completely sent off\n",
      "< HTTP/1.1 200 OK\n",
      "< Server: TornadoServer/6.4.2\n",
      "< Content-Type: text/html\n",
      "< Date: Tue, 04 Mar 2025 04:12:03 GMT\n",
      "< Accept-Ranges: bytes\n",
      "< Etag: \"dbab17a21eaa5481e5ea74ed9b2a58f9cddc1fa1c62651ba7aaaf6016c1781d9d3404eda0c9aa965c79e88d417929d1dfdda09325d0e845b3b42b7b0b6a642cd\"\n",
      "< Last-Modified: Thu, 27 Feb 2025 03:24:13 GMT\n",
      "< Cache-Control: no-cache\n",
      "< Content-Length: 1837\n",
      "< Vary: Accept-Encoding\n",
      "< \n",
      "<!--\n",
      " Copyright (c) Streamlit Inc. (2018-2022) Snowflake Inc. (2022-2025)\n",
      "\n",
      " Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      " you may not use this file except in compliance with the License.\n",
      " You may obtain a copy of the License at\n",
      "\n",
      "     http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      " Unless required by applicable law or agreed to in writing, software\n",
      " distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      " WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      " See the License for the specific language governing permissions and\n",
      " limitations under the License.\n",
      "-->\n",
      "\n",
      "<!DOCTYPE html>\n",
      "<html lang=\"en\">\n",
      "  <head>\n",
      "    <meta charset=\"UTF-8\" />\n",
      "    <meta\n",
      "      name=\"viewport\"\n",
      "      content=\"width=device-width, initial-scale=1, shrink-to-fit=no\"\n",
      "    />\n",
      "    <link rel=\"shortcut icon\" href=\"./favicon.png\" />\n",
      "    <link\n",
      "      rel=\"preload\"\n",
      "      href=\"./static/media/SourceSansPro-Regular.DZLUzqI4.woff2\"\n",
      "      as=\"font\"\n",
      "      type=\"font/woff2\"\n",
      "      crossorigin\n",
      "    />\n",
      "    <link\n",
      "      rel=\"preload\"\n",
      "      href=\"./static/media/SourceSansPro-SemiBold.sKQIyTMz.woff2\"\n",
      "      as=\"font\"\n",
      "      type=\"font/woff2\"\n",
      "      crossorigin\n",
      "    />\n",
      "    <link\n",
      "      rel=\"preload\"\n",
      "      href=\"./static/media/SourceSansPro-Bold.-6c9oR8J.woff2\"\n",
      "      as=\"font\"\n",
      "      type=\"font/woff2\"\n",
      "      crossorigin\n",
      "    />\n",
      "\n",
      "    <title>Streamlit</title>\n",
      "\n",
      "    <!-- initialize window.prerenderReady to false and then set to true in React app when app is ready for indexing -->\n",
      "    <script>\n",
      "      window.prerenderReady = false\n",
      "    </script>\n",
      "    <script type=\"module\" crossorigin src=\"./static/js/index.tKq1MI69.js\"></script>\n",
      "    <link rel=\"stylesheet\" crossorigin href=\"./static/css/index.mUTQuMqR.css\">\n",
      "  </head>\n",
      "  <body>\n",
      "    <noscript>You need to enable JavaScript to run this app.</noscript>\n",
      "    <div id=\"root\"></div>\n",
      "  </body>\n",
      "</html>\n",
      "* Connection #0 to host frontend.default.svc.cluster.local left intact\n"
     ]
    }
   ],
   "source": [
    "!kubectl exec -it $ingress_controller_pod -n ingress-nginx -- curl -v frontend.default.svc.cluster.local:80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "048615e9-b5c6-4cc8-abfc-0446ed55c1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Host fastapi.default.svc.cluster.local:80 was resolved.\n",
      "* IPv6: (none)\n",
      "* IPv4: 172.20.116.75\n",
      "*   Trying 172.20.116.75:80...\n",
      "* Connected to fastapi.default.svc.cluster.local (172.20.116.75) port 80\n",
      "* using HTTP/1.x\n",
      "> GET / HTTP/1.1\n",
      "> Host: fastapi.default.svc.cluster.local\n",
      "> User-Agent: curl/8.11.1\n",
      "> Accept: */*\n",
      "> \n",
      "* Request completely sent off\n",
      "< HTTP/1.1 200 OK\n",
      "< date: Tue, 04 Mar 2025 04:12:03 GMT\n",
      "< server: uvicorn\n",
      "< content-length: 27\n",
      "< content-type: application/json\n",
      "< \n",
      "* Connection #0 to host fastapi.default.svc.cluster.local left intact\n",
      "{\"message\":\"Hello, world!\"}"
     ]
    }
   ],
   "source": [
    "!kubectl exec -it $ingress_controller_pod -n ingress-nginx -- curl -v fastapi.default.svc.cluster.local:80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "28175614-e54a-4ec6-b84a-0280737ba5dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Host fastapi.default.svc.cluster.local:8000 was resolved.\n",
      "* IPv6: (none)\n",
      "* IPv4: 172.20.5.77\n",
      "*   Trying 172.20.5.77:8000...\n",
      "^C\n",
      "command terminated with exit code 130\n"
     ]
    }
   ],
   "source": [
    "!kubectl exec -it $ingress_controller_pod -n ingress-nginx -- curl -v -X POST fastapi.default.svc.cluster.local:8000/respond --no-buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8c6d2510-2c09-43d3-bb80-db1227c96a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error: pod, type/name or --filename must be specified\n"
     ]
    }
   ],
   "source": [
    "!kubectl exec -it $ingress_controller_pod -n ingress-nginx -- curl -v -X POST fastapi.default.svc.cluster.local:8000/respond --no-buffer -H \"Content-Type: application/json\" -d '{\"content\": \"What are the languages in Eberron?\"}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "0d63949c-c95c-40c5-8629-04ffce5c061c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*   Trying 10.0.1.49:8000...\n",
      "* connect to 10.0.1.49 port 8000 from 10.0.1.140 port 58704 failed: Host is unreachable\n",
      "* Failed to connect to 10.0.1.49 port 8000 after 3060 ms: Could not connect to server\n",
      "* closing connection #0\n",
      "curl: (7) Failed to connect to 10.0.1.49 port 8000 after 3060 ms: Could not connect to server\n",
      "command terminated with exit code 7\n"
     ]
    }
   ],
   "source": [
    "!kubectl exec -it $ingress_controller_pod -n ingress-nginx -- curl -v 10.0.1.49:8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "72ff6e74-4202-45f1-af0a-9d5b6299a9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kubectl exec $ingress_controller_pod -n ingress-nginx -- cat /etc/nginx/nginx.conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "8d9430ed-d98f-4214-b59b-99edfdd606fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Could not resolve proxy: POST\n",
      "* shutting down connection #0\n",
      "curl: (5) Could not resolve proxy: POST\n",
      "command terminated with exit code 5\n"
     ]
    }
   ],
   "source": [
    "!kubectl exec -it $ingress_controller_pod -n ingress-nginx -- curl -v -x POST 10.0.1.49:8000/respond --no-buffer -H \"Content-Type: application/json\" -d '{\"content\": \"What are the languages in Eberron?\"}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "092efbc0-177c-4386-8e51-433562dc3ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server:\t\t172.20.0.10\n",
      "Address:\t172.20.0.10:53\n",
      "\n",
      "\n",
      "Name:\tfastapi.default.svc.cluster.local\n",
      "Address: 172.20.5.77\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!kubectl exec -it $ingress_controller_pod -n ingress-nginx -- nslookup fastapi.default.svc.cluster.local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "ac7d6a8b-d3a4-4659-9a44-10d21c7c43ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server:\t\t172.20.0.10\n",
      "Address:\t172.20.0.10:53\n",
      "\n",
      "\n",
      "Name:\tfastapi.default.svc.cluster.local\n",
      "Address: 172.20.5.77\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!kubectl exec -it $ingress_controller_pod -n ingress-nginx -- nslookup fastapi.default.svc.cluster.local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "176374ad-2ad0-4ca4-a19d-41e3c688265c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME      TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)   AGE\n",
      "fastapi   ClusterIP   172.20.5.77   <none>        80/TCP    38m\n"
     ]
    }
   ],
   "source": [
    "!kubectl get svc fastapi -n default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "4f2502d1-940c-4257-b1a1-5ef1b1519931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*   Trying 172.20.5.77:8000...\n",
      "^C\n",
      "command terminated with exit code 130\n"
     ]
    }
   ],
   "source": [
    "!kubectl exec -it $ingress_controller_pod -n ingress-nginx -- curl -v 172.20.5.77:8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "41168437-7564-4b3f-9a0d-e1739ebd0483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*   Trying 172.20.5.77:8000...\n",
      "^C\n",
      "command terminated with exit code 130\n"
     ]
    }
   ],
   "source": [
    "!kubectl exec -it $pod_name -n default -- curl -v fastapi:8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "665a516c-3332-4c8e-8f13-6c3b98f6fa69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*   Trying 172.20.5.77:8000...\n",
      "^C\n",
      "command terminated with exit code 130\n"
     ]
    }
   ],
   "source": [
    "!kubectl exec -it $pod_name -n default -- curl -v -X POST fastapi:8000/respond --no-buffer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837840e5-85b5-4b9a-a937-9a319a1b1b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl exec -it $pod_name -n default -- curl -v -X POST fastapi:8000/respond --no-buffer -H \"Content-Type: application/json\" -d '{\"content\": \"What are the languages in Eberron?\"}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b476e154-946e-45f4-bd87-ca22a910e610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Host fastapi.default.svc.cluster.local:8000 was resolved.\n",
      "* IPv6: (none)\n",
      "* IPv4: 172.20.5.77\n",
      "*   Trying 172.20.5.77:8000...\n",
      "^C\n",
      "command terminated with exit code 130\n"
     ]
    }
   ],
   "source": [
    "!kubectl exec -it $ingress_controller_pod -n ingress-nginx -- curl -v fastapi.default.svc.cluster.local:8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "ee4164d1-4e5e-45c3-9072-917d7d4f222c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME      ENDPOINTS         AGE\n",
      "fastapi   10.0.1.154:8000   39m\n"
     ]
    }
   ],
   "source": [
    "!kubectl get endpoints fastapi -n default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e319c578-74f9-4274-af11-1c0bc114fe43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kubectl describe deployment kubyterlab-llm-pod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a9b485d7-ab30-4ad8-8c23-0c83a5f50f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kubectl describe pod frontend-75669c8cd8-5kkmk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "3d2ac1bd-3d25-4fa8-b2dd-fb7fa10e42d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                        READY   STATUS    RESTARTS   AGE\n",
      "fastapi-677965765c-mbglm    1/1     Running   0          39m\n",
      "frontend-7dc87ddddd-x4zjg   1/1     Running   0          39m\n"
     ]
    }
   ],
   "source": [
    "!kubectl get pods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "0a2da721-c219-436c-8ba5-11d905c507d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME      STATUS   VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   VOLUMEATTRIBUTESCLASS   AGE\n",
      "pvc-llm   Bound    pv-llm   500Gi      RWO            manual         <unset>                 39m\n"
     ]
    }
   ],
   "source": [
    "!kubectl get pvc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "23b517a1-9d50-47a0-bcc8-e9a0f787c216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME     CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM             STORAGECLASS   VOLUMEATTRIBUTESCLASS   REASON   AGE\n",
      "pv-llm   500Gi      RWO            Retain           Bound    default/pvc-llm   manual         <unset>                          39m\n"
     ]
    }
   ],
   "source": [
    "!kubectl get pv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "6a28c8a8-575e-4182-b448-4c5498d74666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========\n",
      "== CUDA ==\n",
      "==========\n",
      "\n",
      "CUDA Version 12.6.1\n",
      "\n",
      "Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n",
      "\n",
      "This container image and its contents are governed by the NVIDIA Deep Learning Container License.\n",
      "By pulling and using the container, you accept the terms and conditions of this license:\n",
      "https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license\n",
      "\n",
      "A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.\n",
      "\n",
      "2025-05-08 01:18:05.863234: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-08 01:18:05.879067: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746667085.899643       1 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746667085.905844       1 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-05-08 01:18:05.925961: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/app/models.py:41: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=embedding_model_path,\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [37:09<00:00, 743.17s/it]\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "INFO:     Started server process [1]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n",
      "INFO:     10.0.1.136:36620 - \"POST /respond HTTP/1.1\" 200 OK\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "INFO:     10.0.1.179:42950 - \"POST /respond HTTP/1.1\" 200 OK\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "!kubectl logs $pod_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "6c6f80b2-a78a-45e6-bd1e-0fc3bd307b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:             fastapi-677965765c-mbglm\n",
      "Namespace:        default\n",
      "Priority:         0\n",
      "Service Account:  default\n",
      "Node:             ip-10-0-1-28.ca-central-1.compute.internal/10.0.1.28\n",
      "Start Time:       Thu, 08 May 2025 01:12:17 +0000\n",
      "Labels:           app=fastapi\n",
      "                  pod-template-hash=677965765c\n",
      "Annotations:      <none>\n",
      "Status:           Running\n",
      "IP:               10.0.1.154\n",
      "IPs:\n",
      "  IP:           10.0.1.154\n",
      "Controlled By:  ReplicaSet/fastapi-677965765c\n",
      "Containers:\n",
      "  fastapi:\n",
      "    Container ID:   containerd://e18da5611e5b0ef5d29cbacda0d10bf2f66a705ded18ef7547b658612b212c76\n",
      "    Image:          275678099358.dkr.ecr.ca-central-1.amazonaws.com/multi-agent/eberron-agent-server:25.02\n",
      "    Image ID:       275678099358.dkr.ecr.ca-central-1.amazonaws.com/multi-agent/eberron-agent-server@sha256:0a7ec392240e2f7a4ba9ce05b279df68209e085074bbe4fb8f465e047156b2ff\n",
      "    Port:           8000/TCP\n",
      "    Host Port:      0/TCP\n",
      "    State:          Running\n",
      "      Started:      Thu, 08 May 2025 01:18:01 +0000\n",
      "    Ready:          True\n",
      "    Restart Count:  0\n",
      "    Environment:\n",
      "      ARTEFACT_VERSION:        04\n",
      "      ARTEFACT_ROOT_FOLDER:    /jupyterlab/artefacts\n",
      "      HF_HOME:                 /jupyterlab/models/hf\n",
      "      MODEL_NAME:              Mistral-7B-Instruct-v0.3\n",
      "      MODEL_ORG:               mistralai\n",
      "      COMMIT_HASH:             e0bc86c23ce5aae1db576c8cca6f06f1f73af2db\n",
      "      TOKENIZERS_PARALLELISM:  true\n",
      "      TRANSFORMERS_OFFLINE:    1\n",
      "      HF_DATASETS_OFFLINE:     1\n",
      "    Mounts:\n",
      "      /jupyterlab from pvc-llm (rw)\n",
      "      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-h96lp (ro)\n",
      "Conditions:\n",
      "  Type                        Status\n",
      "  PodReadyToStartContainers   True \n",
      "  Initialized                 True \n",
      "  Ready                       True \n",
      "  ContainersReady             True \n",
      "  PodScheduled                True \n",
      "Volumes:\n",
      "  pvc-llm:\n",
      "    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)\n",
      "    ClaimName:  pvc-llm\n",
      "    ReadOnly:   false\n",
      "  kube-api-access-h96lp:\n",
      "    Type:                    Projected (a volume that contains injected data from multiple sources)\n",
      "    TokenExpirationSeconds:  3607\n",
      "    ConfigMapName:           kube-root-ca.crt\n",
      "    ConfigMapOptional:       <nil>\n",
      "    DownwardAPI:             true\n",
      "QoS Class:                   BestEffort\n",
      "Node-Selectors:              <none>\n",
      "Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n",
      "                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\n",
      "                             nvidia.com/gpu:NoSchedule op=Exists\n",
      "Events:\n",
      "  Type    Reason                  Age   From                     Message\n",
      "  ----    ------                  ----  ----                     -------\n",
      "  Normal  Scheduled               39m   default-scheduler        Successfully assigned default/fastapi-677965765c-mbglm to ip-10-0-1-28.ca-central-1.compute.internal\n",
      "  Normal  SuccessfulAttachVolume  39m   attachdetach-controller  AttachVolume.Attach succeeded for volume \"pv-llm\"\n",
      "  Normal  Pulling                 39m   kubelet                  Pulling image \"275678099358.dkr.ecr.ca-central-1.amazonaws.com/multi-agent/eberron-agent-server:25.02\"\n",
      "  Normal  Pulled                  34m   kubelet                  Successfully pulled image \"275678099358.dkr.ecr.ca-central-1.amazonaws.com/multi-agent/eberron-agent-server:25.02\" in 5m36.792s (5m36.792s including waiting). Image size: 14071664304 bytes.\n",
      "  Normal  Created                 34m   kubelet                  Created container: fastapi\n",
      "  Normal  Started                 34m   kubelet                  Started container fastapi\n"
     ]
    }
   ],
   "source": [
    "!kubectl describe pod $pod_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad2f7fa-c856-4866-96bb-54eefb245b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attachdetach-controller  AttachVolume.Attach failed for volume \"pv-llm\" : rpc error: code = Internal desc = Could not attach volume \"volume_id\" to node \"i-055aa2853ecdeac3b\": could not attach volume \"volume_id\" to node \"i-055aa2853ecdeac3b\": operation error EC2: AttachVolume, https response error StatusCode: 400, RequestID: a7250de3-c285-4963-84eb-5c46b28c2b96, api error InvalidParameterValue: The volume ID 'volume_id' is malformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "dc6fbb74-4125-4ad3-96b4-f7117b82e1c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA-SMI version  : 550.163.01\n",
      "NVML version        : 550.163\n",
      "DRIVER version      : 550.163.01\n",
      "CUDA Version        : 12.6\n"
     ]
    }
   ],
   "source": [
    "!kubectl exec $pod_name bash -- nvidia-smi --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "e4308b32-7f70-4861-81f6-7bc0702cbc87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu May  8 02:07:19 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.163.01             Driver Version: 550.163.01     CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       On  |   00000000:00:1E.0 Off |                    0 |\n",
      "| N/A   33C    P0             34W /   70W |    4271MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!kubectl exec $pod_name bash -- nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "5a68c2d5-dc22-4529-9e8f-a7935f360340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: Tesla T4 (UUID: GPU-6c4efb2a-8a04-0601-a79b-cdb539a7fd40)\n"
     ]
    }
   ],
   "source": [
    "!kubectl exec $pod_name bash -- nvidia-smi -L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "8d229310-97eb-4788-bf0e-22203dffe078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2024 NVIDIA Corporation\n",
      "Built on Wed_Aug_14_10:10:22_PDT_2024\n",
      "Cuda compilation tools, release 12.6, V12.6.68\n",
      "Build cuda_12.6.r12.6/compiler.34714021_0\n"
     ]
    }
   ],
   "source": [
    "!kubectl exec $pod_name bash -- nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "722a4aa7-a0c3-462e-9e37-60230261fcd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:             fastapi-677965765c-mbglm\n",
      "Namespace:        default\n",
      "Priority:         0\n",
      "Service Account:  default\n",
      "Node:             ip-10-0-1-28.ca-central-1.compute.internal/10.0.1.28\n",
      "Start Time:       Thu, 08 May 2025 01:12:17 +0000\n",
      "Labels:           app=fastapi\n",
      "                  pod-template-hash=677965765c\n",
      "Annotations:      <none>\n",
      "Status:           Running\n",
      "IP:               10.0.1.154\n",
      "IPs:\n",
      "  IP:           10.0.1.154\n",
      "Controlled By:  ReplicaSet/fastapi-677965765c\n",
      "Containers:\n",
      "  fastapi:\n",
      "    Container ID:   containerd://e18da5611e5b0ef5d29cbacda0d10bf2f66a705ded18ef7547b658612b212c76\n",
      "    Image:          275678099358.dkr.ecr.ca-central-1.amazonaws.com/multi-agent/eberron-agent-server:25.02\n",
      "    Image ID:       275678099358.dkr.ecr.ca-central-1.amazonaws.com/multi-agent/eberron-agent-server@sha256:0a7ec392240e2f7a4ba9ce05b279df68209e085074bbe4fb8f465e047156b2ff\n",
      "    Port:           8000/TCP\n",
      "    Host Port:      0/TCP\n",
      "    State:          Running\n",
      "      Started:      Thu, 08 May 2025 01:18:01 +0000\n",
      "    Ready:          True\n",
      "    Restart Count:  0\n",
      "    Environment:\n",
      "      ARTEFACT_VERSION:        04\n",
      "      ARTEFACT_ROOT_FOLDER:    /jupyterlab/artefacts\n",
      "      HF_HOME:                 /jupyterlab/models/hf\n",
      "      MODEL_NAME:              Mistral-7B-Instruct-v0.3\n",
      "      MODEL_ORG:               mistralai\n",
      "      COMMIT_HASH:             e0bc86c23ce5aae1db576c8cca6f06f1f73af2db\n",
      "      TOKENIZERS_PARALLELISM:  true\n",
      "      TRANSFORMERS_OFFLINE:    1\n",
      "      HF_DATASETS_OFFLINE:     1\n",
      "    Mounts:\n",
      "      /jupyterlab from pvc-llm (rw)\n",
      "      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-h96lp (ro)\n",
      "Conditions:\n",
      "  Type                        Status\n",
      "  PodReadyToStartContainers   True \n",
      "  Initialized                 True \n",
      "  Ready                       True \n",
      "  ContainersReady             True \n",
      "  PodScheduled                True \n",
      "Volumes:\n",
      "  pvc-llm:\n",
      "    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)\n",
      "    ClaimName:  pvc-llm\n",
      "    ReadOnly:   false\n",
      "  kube-api-access-h96lp:\n",
      "    Type:                    Projected (a volume that contains injected data from multiple sources)\n",
      "    TokenExpirationSeconds:  3607\n",
      "    ConfigMapName:           kube-root-ca.crt\n",
      "    ConfigMapOptional:       <nil>\n",
      "    DownwardAPI:             true\n",
      "QoS Class:                   BestEffort\n",
      "Node-Selectors:              <none>\n",
      "Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n",
      "                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\n",
      "                             nvidia.com/gpu:NoSchedule op=Exists\n",
      "Events:\n",
      "  Type    Reason                  Age   From                     Message\n",
      "  ----    ------                  ----  ----                     -------\n",
      "  Normal  Scheduled               36m   default-scheduler        Successfully assigned default/fastapi-677965765c-mbglm to ip-10-0-1-28.ca-central-1.compute.internal\n",
      "  Normal  SuccessfulAttachVolume  36m   attachdetach-controller  AttachVolume.Attach succeeded for volume \"pv-llm\"\n",
      "  Normal  Pulling                 36m   kubelet                  Pulling image \"275678099358.dkr.ecr.ca-central-1.amazonaws.com/multi-agent/eberron-agent-server:25.02\"\n",
      "  Normal  Pulled                  30m   kubelet                  Successfully pulled image \"275678099358.dkr.ecr.ca-central-1.amazonaws.com/multi-agent/eberron-agent-server:25.02\" in 5m36.792s (5m36.792s including waiting). Image size: 14071664304 bytes.\n",
      "  Normal  Created                 30m   kubelet                  Created container: fastapi\n",
      "  Normal  Started                 30m   kubelet                  Started container fastapi\n"
     ]
    }
   ],
   "source": [
    "!kubectl describe pod $pod_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "116fd849-a569-472e-93d6-23624e93c7e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                   READY   STATUS             RESTARTS         AGE\n",
      "aws-node-dc9qc                         2/2     Running            0                37m\n",
      "aws-node-kj654                         2/2     Running            0                37m\n",
      "aws-node-wrs2x                         2/2     Running            0                38m\n",
      "coredns-57d9dcc947-452jx               1/1     Running            0                40m\n",
      "coredns-57d9dcc947-hnt5k               1/1     Running            0                40m\n",
      "ebs-csi-controller-76c8cd8ccf-ntct4    5/5     Running            0                36m\n",
      "ebs-csi-controller-76c8cd8ccf-wc45h    5/5     Running            0                36m\n",
      "ebs-csi-node-2dblk                     3/3     Running            0                36m\n",
      "ebs-csi-node-kvqq9                     3/3     Running            0                36m\n",
      "ebs-csi-node-n8kfp                     3/3     Running            0                36m\n",
      "kube-proxy-kfn29                       1/1     Running            0                37m\n",
      "kube-proxy-nxd6f                       1/1     Running            0                37m\n",
      "kube-proxy-st5wn                       1/1     Running            0                38m\n",
      "nvidia-device-plugin-daemonset-qqbbv   0/1     CrashLoopBackOff   11 (4m59s ago)   36m\n",
      "nvidia-device-plugin-daemonset-vm5qt   0/1     CrashLoopBackOff   12 (18s ago)     36m\n",
      "nvidia-device-plugin-daemonset-zzgxx   1/1     Running            0                36m\n"
     ]
    }
   ],
   "source": [
    "!kubectl get pod -n kube-system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "90495808-55bb-4d5b-84e8-c00ca25250b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:          pvc-llm\n",
      "Namespace:     default\n",
      "StorageClass:  manual\n",
      "Status:        Bound\n",
      "Volume:        pv-llm\n",
      "Labels:        <none>\n",
      "Annotations:   pv.kubernetes.io/bind-completed: yes\n",
      "               pv.kubernetes.io/bound-by-controller: yes\n",
      "Finalizers:    [kubernetes.io/pvc-protection]\n",
      "Capacity:      10Gi\n",
      "Access Modes:  RWO\n",
      "VolumeMode:    Filesystem\n",
      "Used By:       kubyterlab-llm-pod-67f5cf95dc-s2mrn\n",
      "Events:        <none>\n"
     ]
    }
   ],
   "source": [
    "!kubectl describe pvc pvc-llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "da047557-f797-4492-b7f6-1cdcb16a3283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:            pv-llm\n",
      "Labels:          <none>\n",
      "Annotations:     pv.kubernetes.io/bound-by-controller: yes\n",
      "Finalizers:      [kubernetes.io/pv-protection]\n",
      "StorageClass:    manual\n",
      "Status:          Bound\n",
      "Claim:           default/pvc-llm\n",
      "Reclaim Policy:  Retain\n",
      "Access Modes:    RWO\n",
      "VolumeMode:      Filesystem\n",
      "Capacity:        10Gi\n",
      "Node Affinity:   <none>\n",
      "Message:         \n",
      "Source:\n",
      "    Type:       AWSElasticBlockStore (a Persistent Disk resource in AWS)\n",
      "    VolumeID:   vol-04ce08a2c05b8e1da\n",
      "    FSType:     ext4\n",
      "    Partition:  0\n",
      "    ReadOnly:   false\n",
      "Events:         <none>\n"
     ]
    }
   ],
   "source": [
    "!kubectl describe pv pv-llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "80827487-9836-450d-806d-5f56b00ab821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:                   kubyterlab-llm-pod\n",
      "Namespace:              default\n",
      "CreationTimestamp:      Mon, 02 Dec 2024 20:05:56 +0000\n",
      "Labels:                 <none>\n",
      "Annotations:            deployment.kubernetes.io/revision: 1\n",
      "Selector:               app=kubyterlab-llm\n",
      "Replicas:               1 desired | 1 updated | 1 total | 0 available | 1 unavailable\n",
      "StrategyType:           RollingUpdate\n",
      "MinReadySeconds:        0\n",
      "RollingUpdateStrategy:  25% max unavailable, 25% max surge\n",
      "Pod Template:\n",
      "  Labels:  app=kubyterlab-llm\n",
      "  Containers:\n",
      "   kubyterlab-llm:\n",
      "    Image:      kubyterlab:24.10\n",
      "    Port:       8888/TCP\n",
      "    Host Port:  0/TCP\n",
      "    Limits:\n",
      "      cpu:             1\n",
      "      memory:          8Gi\n",
      "      nvidia.com/gpu:  1\n",
      "    Requests:\n",
      "      cpu:             1\n",
      "      memory:          8Gi\n",
      "      nvidia.com/gpu:  1\n",
      "    Environment:\n",
      "      JUPYTERLAB_SETTINGS_DIR:  /jupyterlab/config\n",
      "      MISTRAL_MODEL:            /models/mistral\n",
      "    Mounts:\n",
      "      /corpus from pv-llm (rw)\n",
      "      /jupyterlab from pv-llm (rw)\n",
      "      /models from pv-llm (rw)\n",
      "  Volumes:\n",
      "   pv-llm:\n",
      "    Type:          PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)\n",
      "    ClaimName:     pvc-llm\n",
      "    ReadOnly:      false\n",
      "  Node-Selectors:  <none>\n",
      "  Tolerations:     nvidia.com/gpu:NoSchedule op=Exists\n",
      "Conditions:\n",
      "  Type           Status  Reason\n",
      "  ----           ------  ------\n",
      "  Available      False   MinimumReplicasUnavailable\n",
      "  Progressing    True    ReplicaSetUpdated\n",
      "OldReplicaSets:  <none>\n",
      "NewReplicaSet:   kubyterlab-llm-pod-67f5cf95dc (1/1 replicas created)\n",
      "Events:\n",
      "  Type    Reason             Age    From                   Message\n",
      "  ----    ------             ----   ----                   -------\n",
      "  Normal  ScalingReplicaSet  5m46s  deployment-controller  Scaled up replica set kubyterlab-llm-pod-67f5cf95dc to 1\n"
     ]
    }
   ],
   "source": [
    "!kubectl describe deployment kubyterlab-llm-pod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ce402031-be81-4578-8877-15bcd40ad8e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                          STATUS   ROLES    AGE   VERSION\n",
      "ip-10-0-1-100.ca-central-1.compute.internal   Ready    <none>   47m   v1.30.7-eks-59bf375\n",
      "ip-10-0-1-25.ca-central-1.compute.internal    Ready    <none>   47m   v1.30.7-eks-59bf375\n",
      "ip-10-0-1-50.ca-central-1.compute.internal    Ready    <none>   48m   v1.30.7-eks-59bf375\n"
     ]
    }
   ],
   "source": [
    "!kubectl get nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d0bb4314-1002-45c7-b0c2-4d706b5d533f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME               DATA   AGE\n",
      "kube-root-ca.crt   1      50m\n"
     ]
    }
   ],
   "source": [
    "!kubectl get configmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6d65a766-6f17-413c-a7f5-9f8e07fd0a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                     READY   STATUS    RESTARTS   AGE\n",
      "pod/ebs-csi-controller-59b6797bf-vfvs2   5/5     Running   0          47m\n",
      "pod/ebs-csi-controller-59b6797bf-xt7fv   5/5     Running   0          47m\n",
      "pod/ebs-csi-node-dwzz7                   3/3     Running   0          47m\n",
      "pod/ebs-csi-node-gplqp                   3/3     Running   0          47m\n",
      "pod/ebs-csi-node-l5nh6                   3/3     Running   0          47m\n",
      "\n",
      "NAME                          DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE\n",
      "daemonset.apps/ebs-csi-node   3         3         3       3            3           kubernetes.io/os=linux   47m\n",
      "\n",
      "NAME                                 READY   UP-TO-DATE   AVAILABLE   AGE\n",
      "deployment.apps/ebs-csi-controller   2/2     2            2           47m\n",
      "\n",
      "NAME                                           DESIRED   CURRENT   READY   AGE\n",
      "replicaset.apps/ebs-csi-controller-59b6797bf   2         2         2       47m\n"
     ]
    }
   ],
   "source": [
    "!kubectl get all -l app.kubernetes.io/name=aws-ebs-csi-driver -n kube-system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3bf4b6bf-4a67-48e4-ba16-8207e90b7fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"OpenIDConnectProviderList\": [\n",
      "        {\n",
      "            \"Arn\": \"arn:aws:iam::275678099358:oidc-provider/oidc.eks.ca-central-1.amazonaws.com/id/8D7619520212428BD59C46B20BF19338\"\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!aws iam list-open-id-connect-providers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "65c00bf0-9326-417c-964f-530ab62c85da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 pods, using pod/ebs-csi-controller-59b6797bf-vfvs2\n",
      "I0113 21:28:06.936951       1 main.go:153] \"Initializing metadata\"\n",
      "I0113 21:28:06.943481       1 metadata.go:48] \"Retrieved metadata from IMDS\"\n",
      "I0113 21:28:06.944585       1 driver.go:69] \"Driver Information\" Driver=\"ebs.csi.aws.com\" Version=\"v1.38.1\"\n",
      "I0113 22:01:16.236299       1 controller.go:410] \"ControllerPublishVolume: attaching\" volumeID=\"vol-0ed5d3cc8cb5a989e\" nodeID=\"i-0fd8ede93d6ac59a4\"\n",
      "I0113 22:01:17.882498       1 controller.go:419] \"ControllerPublishVolume: attached\" volumeID=\"vol-0ed5d3cc8cb5a989e\" nodeID=\"i-0fd8ede93d6ac59a4\" devicePath=\"/dev/xvdaa\"\n"
     ]
    }
   ],
   "source": [
    "!kubectl logs deployment/ebs-csi-controller -n kube-system -c ebs-plugin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8ab7f84b-398b-41fb-9c35-30f40c179f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 pods, using pod/ebs-csi-node-l5nh6\n",
      "I0113 21:27:55.828513       1 main.go:153] \"Initializing metadata\"\n",
      "I0113 21:27:55.832956       1 metadata.go:48] \"Retrieved metadata from IMDS\"\n",
      "I0113 21:27:55.833450       1 driver.go:69] \"Driver Information\" Driver=\"ebs.csi.aws.com\" Version=\"v1.38.1\"\n",
      "E0113 21:27:56.896438       1 node.go:856] \"Unexpected failure when attempting to remove node taint(s)\" err=\"isAllocatableSet: driver not found on node ip-10-0-1-50.ca-central-1.compute.internal\"\n",
      "I0113 21:27:57.410789       1 node.go:936] \"CSINode Allocatable value is set\" nodeName=\"ip-10-0-1-50.ca-central-1.compute.internal\" count=24\n",
      "I0113 22:01:18.817732       1 node.go:204] \"NodeStageVolume: invalid partition config, will ignore.\" partition=\"0\"\n",
      "I0113 22:01:18.980008       1 mount_linux.go:295] Detected OS without systemd\n"
     ]
    }
   ],
   "source": [
    "!kubectl logs daemonset/ebs-csi-node -n kube-system -c ebs-plugin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f2fd1fcf-f18f-4b19-abeb-1c207ddd58de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 pods, using pod/ebs-csi-controller-59b6797bf-vfvs2\n",
      "W0113 21:28:09.566495       1 feature_gate.go:354] Setting GA feature gate Topology=true. It will be removed in a future release.\n",
      "I0113 21:28:09.566789       1 feature_gate.go:387] feature gates: {map[Topology:true]}\n",
      "I0113 21:28:09.566840       1 csi-provisioner.go:154] Version: v5.1.0\n",
      "I0113 21:28:09.566851       1 csi-provisioner.go:177] Building kube configs for running in cluster...\n",
      "I0113 21:28:09.568886       1 common.go:143] \"Probing CSI driver for readiness\"\n",
      "I0113 21:28:09.598446       1 csi-provisioner.go:230] Detected CSI driver ebs.csi.aws.com\n",
      "I0113 21:28:09.598619       1 csi-provisioner.go:240] Supports migration from in-tree plugin: kubernetes.io/aws-ebs\n",
      "I0113 21:28:09.600672       1 common.go:143] \"Probing CSI driver for readiness\"\n",
      "I0113 21:28:09.603385       1 csi-provisioner.go:299] CSI driver supports PUBLISH_UNPUBLISH_VOLUME, watching VolumeAttachments\n",
      "I0113 21:28:09.604750       1 controller.go:744] \"Using saving PVs to API server in background\"\n",
      "I0113 21:28:09.605162       1 leaderelection.go:254] attempting to acquire leader lease kube-system/ebs-csi-aws-com...\n",
      "I0113 21:28:09.627768       1 leaderelection.go:268] successfully acquired lease kube-system/ebs-csi-aws-com\n",
      "I0113 21:28:09.629736       1 leader_election.go:184] \"became leader, starting\"\n",
      "I0113 21:28:09.629845       1 envvar.go:172] \"Feature gate default state\" feature=\"WatchListClient\" enabled=false\n",
      "I0113 21:28:09.629863       1 envvar.go:172] \"Feature gate default state\" feature=\"InformerResourceVersion\" enabled=false\n",
      "I0113 21:28:09.640053       1 reflector.go:368] Caches populated for *v1.StorageClass from k8s.io/client-go@v0.31.0/tools/cache/reflector.go:243\n",
      "I0113 21:28:09.651560       1 reflector.go:368] Caches populated for *v1.PersistentVolumeClaim from k8s.io/client-go@v0.31.0/tools/cache/reflector.go:243\n",
      "I0113 21:28:09.656122       1 reflector.go:368] Caches populated for *v1.Node from k8s.io/client-go@v0.31.0/tools/cache/reflector.go:243\n",
      "I0113 21:28:09.656502       1 reflector.go:368] Caches populated for *v1.VolumeAttachment from k8s.io/client-go@v0.31.0/tools/cache/reflector.go:243\n",
      "I0113 21:28:09.657442       1 reflector.go:368] Caches populated for *v1.CSINode from k8s.io/client-go@v0.31.0/tools/cache/reflector.go:243\n",
      "I0113 21:28:09.729875       1 controller.go:824] \"Starting provisioner controller\" component=\"ebs.csi.aws.com_ebs-csi-controller-59b6797bf-vfvs2_1c292ac0-a9d9-48ff-8621-9ba1f75e01b8\"\n",
      "I0113 21:28:09.729957       1 volume_store.go:98] \"Starting save volume queue\"\n",
      "I0113 21:28:09.734566       1 reflector.go:368] Caches populated for *v1.StorageClass from k8s.io/client-go@v0.31.0/tools/cache/reflector.go:243\n",
      "I0113 21:28:09.734567       1 reflector.go:368] Caches populated for *v1.PersistentVolume from k8s.io/client-go@v0.31.0/tools/cache/reflector.go:243\n",
      "I0113 21:28:09.831005       1 controller.go:873] \"Started provisioner controller\" component=\"ebs.csi.aws.com_ebs-csi-controller-59b6797bf-vfvs2_1c292ac0-a9d9-48ff-8621-9ba1f75e01b8\"\n"
     ]
    }
   ],
   "source": [
    "!kubectl logs deployment/ebs-csi-controller -n kube-system -c csi-provisioner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "49134a19-5818-4030-8a7a-6c9e04d6157c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 pods, using pod/ebs-csi-controller-59b6797bf-vfvs2\n",
      "I0113 21:28:12.211572       1 main.go:109] \"Version\" version=\"v4.7.0\"\n",
      "I0113 21:28:12.215865       1 common.go:143] \"Probing CSI driver for readiness\"\n",
      "I0113 21:28:12.238251       1 main.go:169] \"CSI driver name\" driver=\"ebs.csi.aws.com\"\n",
      "I0113 21:28:12.240448       1 common.go:143] \"Probing CSI driver for readiness\"\n",
      "I0113 21:28:12.242734       1 main.go:249] \"CSI driver supports ControllerPublishUnpublish, using real CSI handler\" driver=\"ebs.csi.aws.com\"\n",
      "I0113 21:28:12.243624       1 leaderelection.go:254] attempting to acquire leader lease kube-system/external-attacher-leader-ebs-csi-aws-com...\n",
      "I0113 21:28:12.265555       1 leaderelection.go:268] successfully acquired lease kube-system/external-attacher-leader-ebs-csi-aws-com\n",
      "I0113 21:28:12.266787       1 leader_election.go:184] \"became leader, starting\"\n",
      "I0113 21:28:12.266811       1 controller.go:129] \"Starting CSI attacher\"\n",
      "I0113 21:28:12.267257       1 envvar.go:172] \"Feature gate default state\" feature=\"InformerResourceVersion\" enabled=false\n",
      "I0113 21:28:12.267536       1 envvar.go:172] \"Feature gate default state\" feature=\"WatchListClient\" enabled=false\n",
      "I0113 21:28:12.271734       1 reflector.go:368] Caches populated for *v1.PersistentVolume from k8s.io/client-go@v0.31.0/tools/cache/reflector.go:243\n",
      "I0113 21:28:12.272191       1 reflector.go:368] Caches populated for *v1.CSINode from k8s.io/client-go@v0.31.0/tools/cache/reflector.go:243\n",
      "I0113 21:28:12.272546       1 reflector.go:368] Caches populated for *v1.VolumeAttachment from k8s.io/client-go@v0.31.0/tools/cache/reflector.go:243\n",
      "I0113 22:01:16.209140       1 csi_handler.go:261] \"Attaching\" VolumeAttachment=\"csi-ed0e5d922afcde1b934ffafc0f0d3a9543d0760d7dfa4ce99585f3fc2c2922b0\"\n",
      "I0113 22:01:17.882946       1 csi_handler.go:273] \"Attached\" VolumeAttachment=\"csi-ed0e5d922afcde1b934ffafc0f0d3a9543d0760d7dfa4ce99585f3fc2c2922b0\"\n"
     ]
    }
   ],
   "source": [
    "!kubectl logs deployment/ebs-csi-controller -n kube-system -c csi-attacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6fdb0f08-aba7-4d63-8e01-2b25d1b06b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"eks.amazonaws.com/role-arn\":\"\",\"meta.helm.sh/release-name\":\"aws-ebs-csi-driver\",\"meta.helm.sh/release-namespace\":\"kube-system\"}"
     ]
    }
   ],
   "source": [
    "!kubectl get sa ebs-csi-controller-sa -n kube-system -o jsonpath='{.metadata.annotations}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a85d88b-946c-40e7-8b59-0e69a4567d5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
