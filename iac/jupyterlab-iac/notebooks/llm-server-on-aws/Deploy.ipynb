{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d02ca0b4-1c4a-4b8e-a7a9-8fe3c9ba40b1",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38534870-6679-40ae-bcf1-f778377e5998",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from time import sleep\n",
    "from time import time as unixtime\n",
    "from typing import Callable, List\n",
    "import random\n",
    "from math import ceil\n",
    "import yaml\n",
    "import string\n",
    "import json\n",
    "from urllib.parse import urlparse\n",
    "from time import sleep\n",
    "import re\n",
    "from operator import itemgetter\n",
    "import subprocess\n",
    "from tempfile import NamedTemporaryFile\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "import boto3\n",
    "import kubernetes\n",
    "from kubernetes.client.rest import ApiException\n",
    "\n",
    "from pyhelm3 import Client as HelmClient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a98f1d3-e2b6-4c0a-ad3d-c07f5416d684",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv('/.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08774840-c227-48d4-844c-165c6f5dc8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import wait_until\n",
    "from helper.ec2 import (get_vpcs_ids, get_internet_gateway_ids_attached_to_vpc, \n",
    "                        get_route_table_ids_for_vpc, route_to_gateway_exists, \n",
    "                        get_subnet_ids_in_vpc, get_security_group_ids)\n",
    "from helper.k8s import (get_one_running_pod, get_jupyter_token_from_pod)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1800b74-d019-445d-942e-7223e770f332",
   "metadata": {},
   "source": [
    "# Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3b9f759-4820-40ad-8916-e935d6e57232",
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = 'ca-central-1'\n",
    "CLUSTER_NAME = 'kubyterlab-llm'\n",
    "EBS_VOLUME_SIZE = 500 # GiB\n",
    "TAGS = {'cluster': CLUSTER_NAME, 'purpose': 'llm'}  # Do not change the keys, they are hardcoded throughout.\n",
    "CLUSTER_TAGS = {'cluster': CLUSTER_NAME}\n",
    "VOLUME_FILTERS = [\n",
    "    {'Name': f'tag:purpose', \n",
    "     'Values': ['kubyterlab-llm', 'llm']}]\n",
    "K8S_VERSION = os.environ['K8S_VERSION']  # '1.30'\n",
    "K8S_VERSION = '.'.join(K8S_VERSION.split('.')[:2]) if len(K8S_VERSION.split('.')) > 2 else K8S_VERSION\n",
    "INSTANCE_TYPES = {'gpu': ['g4dn.2xlarge'], 'default': ['t3.medium']}\n",
    "ALLOWED_PORTS = [80, 443, 22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d859c1f-c31c-43b5-9a3a-fa9ed0ae7006",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_all_vpcs_available(response: dict) -> True:\n",
    "    if not 'Vpcs' in response:\n",
    "        raise ValueError\n",
    "    return all([vpc.get('State', '') == 'available' for vpc in response['Vpcs']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70ff5d92-c3be-4255-9d40-afea872ff121",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_cluster_active(response: dict) -> bool:\n",
    "    status = response['cluster']['status']\n",
    "    clear_output(wait=True)\n",
    "    display(status)\n",
    "    return status == 'ACTIVE'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8983294-0e11-457c-abb6-2ec996243d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_node_group_active(response: dict) -> bool:\n",
    "    status = response['nodegroup']['status']\n",
    "    clear_output(wait=True)\n",
    "    display(status)\n",
    "    return status in ['ACTIVE', 'CREATE_FAILED']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9938e8cf-fc3a-42f8-819b-7748181fffc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_snapshot_completed(response: dict) -> bool:\n",
    "    state = response['Snapshots'][0]['State']\n",
    "    clear_output(wait=True)\n",
    "    display(state)\n",
    "    return state.lower() == 'completed'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f79a307-fbfa-4d1e-9d75-9f77405a7dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_volume_available(response: dict) -> bool:\n",
    "    state = response['Volumes'][0]['State']\n",
    "    clear_output(wait=True)\n",
    "    display(state)\n",
    "    return state.lower() == 'available'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73ff831-37b6-4fbf-9aad-1068cfb8efe7",
   "metadata": {},
   "source": [
    "# Instantiate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97c98c25-b95b-40da-be00-f65f5952d5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = boto3.Session(region_name=REGION)\n",
    "eks_client = session.client('eks')\n",
    "ec2_client = session.client('ec2')\n",
    "iam_client = session.client('iam')\n",
    "\n",
    "aws_account_id = boto3.client('sts').get_caller_identity().get('Account')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbf7c2f-2e86-48c9-b369-1047bd8fea6c",
   "metadata": {},
   "source": [
    "# Create or Restore Volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "025247fd-7e39-46c3-ba9c-8bf58a3dcc19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vol-021e0e2f73fe34170']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    response = ec2_client.describe_volumes(Filters=VOLUME_FILTERS)\n",
    "    volumes = response.get('Volumes', [])\n",
    "except RuntimeError:\n",
    "    volumes = []\n",
    "volume_ids = [volume['VolumeId'] for volume in volumes]\n",
    "volume_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "728edb7d-b012-4778-8f98-34f00f520042",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(volume_ids) <= 1  # TODO: Get the latest one if more than one.\n",
    "if volume_ids:\n",
    "    volume_id = volume_ids[0]\n",
    "    availability_zone = volumes[0]['AvailabilityZone']\n",
    "else:\n",
    "    volume_id = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf65cde1-f0ad-4b9a-9c91-49c20d09b2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not volume_id:\n",
    "    response = ec2_client.describe_snapshots(Filters=VOLUME_FILTERS)\n",
    "    snapshots = response.get('Snapshots', [])\n",
    "    if snapshots:\n",
    "        sorted_snapshots = sorted(snapshots, key=itemgetter('StartTime'), reverse=True)\n",
    "        snapshot_id = sorted_snapshots[0]['SnapshotId']\n",
    "        print(snapshot_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5d85302-63d3-4ef2-a07e-bd59dc564768",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('vol-021e0e2f73fe34170', 'ca-central-1a')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not volume_id:\n",
    "    response = ec2_client.describe_availability_zones()\n",
    "    availability_zones = response['AvailabilityZones']\n",
    "    availability_zone = availability_zones[0]['ZoneName']\n",
    "    # availability_zone = f'{REGION}a'\n",
    "    if snapshots:\n",
    "        # TODO: Change this to the latest snapshot!!\n",
    "        response = ec2_client.create_volume(\n",
    "            SnapshotId=snapshot_id,\n",
    "            Size=EBS_VOLUME_SIZE,\n",
    "            AvailabilityZone=availability_zone,\n",
    "            VolumeType='gp3',\n",
    "            TagSpecifications=[\n",
    "                {\n",
    "                    'ResourceType': 'volume',\n",
    "                    'Tags': [{'Key': 'purpose', 'Value': 'llm'}]\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "    else:\n",
    "        response = ec2_client.create_volume(\n",
    "            Size=EBS_VOLUME_SIZE,\n",
    "            AvailabilityZone=availability_zones[0]['ZoneName'],\n",
    "            VolumeType='gp3',\n",
    "            TagSpecifications=[\n",
    "                {\n",
    "                    'ResourceType': 'volume',\n",
    "                    'Tags': [{'Key': 'purpose', 'Value': 'llm'}]\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "    wait_until(ec2_client.describe_volumes, {'VolumeIds': [response['VolumeId']]}, is_volume_available)\n",
    "    volume_id = response['VolumeId']\n",
    "volume_id, availability_zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e815d5e-1d1e-4f94-ac53-9fa53edbdd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Change the parts below to use Terraform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbda3f9d-a108-48f4-8411-f7dd2e0e1f43",
   "metadata": {},
   "source": [
    "# Create VPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "429b8e60-e14a-460b-a448-45bc392e6c12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('vpc-0ba31ec9327f8e890', 'igw-0aab310379559d7cf', 'rtb-0fc1da438a5ff2898')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vpc_ids = get_vpcs_ids(ec2_client, TAGS)\n",
    "vpc_exists = len(vpc_ids) > 0\n",
    "\n",
    "if len(vpc_ids) > 1:\n",
    "    raise RuntimeError\n",
    "elif vpc_exists:\n",
    "    vpc_id = vpc_ids[0]\n",
    "\n",
    "# Create VPC\n",
    "if not vpc_exists:\n",
    "    print('Creating VPC...')\n",
    "    vpc_response = ec2_client.create_vpc(\n",
    "        CidrBlock='10.0.0.0/16',\n",
    "        TagSpecifications=[\n",
    "            {\n",
    "                'ResourceType': 'vpc',\n",
    "                'Tags': [{'Key': tag, 'Value': TAGS[tag] }for tag in TAGS]\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    vpc_id = vpc_response['Vpc']['VpcId']\n",
    "wait_until(ec2_client.describe_vpcs, {'VpcIds': [vpc_id]}, check_all_vpcs_available)\n",
    "\n",
    "# Create Internet Gateway\n",
    "igw_ids = get_internet_gateway_ids_attached_to_vpc(ec2_client, vpc_id)\n",
    "igw_exists = len(igw_ids) > 0\n",
    "\n",
    "if len(igw_ids) > 1:\n",
    "    raise RuntimeError\n",
    "elif igw_exists:\n",
    "    igw_id = igw_ids[0]\n",
    "\n",
    "\n",
    "if not igw_exists:\n",
    "    print('Creating Internet Gateway...')\n",
    "    # Create an Internet Gateway\n",
    "    igw_response = ec2_client.create_internet_gateway()\n",
    "    igw_id = igw_response['InternetGateway']['InternetGatewayId']\n",
    "    \n",
    "    # Attach Internet Gateway to VPC\n",
    "    ec2_client.attach_internet_gateway(\n",
    "        InternetGatewayId=igw_id,\n",
    "        VpcId=vpc_id\n",
    "    )\n",
    "\n",
    "# Create a route table\n",
    "route_table_ids = get_route_table_ids_for_vpc(ec2_client, vpc_id)\n",
    "route_table_exists = len(route_table_ids) > 0\n",
    "\n",
    "if not route_table_exists:\n",
    "    print('Creating a route table...')\n",
    "    # Create a route table\n",
    "    route_table_response = ec2_client.create_route_table(VpcId=vpc_id)\n",
    "    route_table_id = route_table_response['RouteTable']['RouteTableId']\n",
    "\n",
    "is_route_created = False\n",
    "for route_table_id in route_table_ids:\n",
    "    if route_to_gateway_exists(ec2_client, route_table_id, igw_id):\n",
    "        is_route_created = True\n",
    "        break\n",
    "\n",
    "if not is_route_created:\n",
    "    # Create a route to the Internet Gateway\n",
    "    ec2_client.create_route(\n",
    "        RouteTableId=route_table_id,\n",
    "        DestinationCidrBlock='0.0.0.0/0',\n",
    "        GatewayId=igw_id\n",
    "    )\n",
    "\n",
    "vpc_id, igw_id, route_table_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbfdb24-1435-47bb-8293-b87b9d4075b7",
   "metadata": {},
   "source": [
    "# Create Subnets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b32166a-ce50-45f6-aac3-3f809836f103",
   "metadata": {},
   "outputs": [],
   "source": [
    "subnet_ids = get_subnet_ids_in_vpc(ec2_client, vpc_id)\n",
    "min_subnet_count = 2\n",
    "current_subnet_count = len(subnet_ids)\n",
    "\n",
    "# if current_subnet_count < min_subnet_count:\n",
    "    # response = ec2_client.describe_availability_zones()\n",
    "    \n",
    "    # availability_zones = [az['ZoneName'] for az in response['AvailabilityZones']]\n",
    "    # random.shuffle(availability_zones)\n",
    "    # TODO: Following logic is still necessary for situations with larger subnets. Change to something reasonable.\n",
    "    # availability_zones *= ceil(min_subnet_count / len(availability_zones))\n",
    "    \n",
    "\n",
    "while current_subnet_count < min_subnet_count:\n",
    "    i = current_subnet_count + 1\n",
    "    print(f'Creating subnet #{i}...')\n",
    "    subnet_response = ec2_client.create_subnet(\n",
    "        VpcId=vpc_id,\n",
    "        CidrBlock=f'10.0.{i}.0/24',\n",
    "        # AvailabilityZone=availability_zone,\n",
    "        AvailabilityZone=availability_zones[current_subnet_count]['ZoneName'],\n",
    "        TagSpecifications=[\n",
    "            {\n",
    "                'ResourceType': 'subnet',\n",
    "                'Tags': [{'Key': tag, 'Value': TAGS[tag]} for tag in TAGS]\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    subnet_id = subnet_response['Subnet']['SubnetId']\n",
    "    subnet_ids.append(subnet_id)\n",
    "    current_subnet_count = len(subnet_ids)\n",
    "\n",
    "# check if any subnet is public, add the table to first subnet if not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c837c746-05e1-432f-8619-e3c0581650f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "subnets_response = ec2_client.describe_subnets(SubnetIds=subnet_ids)\n",
    "assert len(subnets_response) == len(subnet_ids) == min_subnet_count\n",
    "is_any_subnet_open_to_public = False\n",
    "for subnet in subnets_response['Subnets']:\n",
    "    if subnets_response.get('MapPublicIpOnLaunch', False):\n",
    "        # I could also look at the route tables, and see if they are assigned to any subnet. ec2_client.describe_route_tables(RouteTableIds=[route_table_id]) RouteTables.SubnetId (optional parameter)\n",
    "        public_subnet_id = subnet['SubnetId']\n",
    "        is_any_subnet_open_to_public = True\n",
    "\n",
    "if not is_any_subnet_open_to_public:\n",
    "    for subnet in subnets_response['Subnets']:\n",
    "        if subnet['CidrBlock'] == '10.0.1.0/24':\n",
    "            public_subnet_id = subnet['SubnetId']\n",
    "\n",
    "            # Associate the public subnet with the route table\n",
    "            ec2_client.associate_route_table(\n",
    "                RouteTableId=route_table_id,\n",
    "                SubnetId=public_subnet_id\n",
    "            )\n",
    "            \n",
    "            # Modify the public subnet to auto-assign public IPs\n",
    "            ec2_client.modify_subnet_attribute(\n",
    "                SubnetId=public_subnet_id,\n",
    "                MapPublicIpOnLaunch={\"Value\": True}\n",
    "            )\n",
    "            break\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95af95c-f3a4-46d3-b5f8-b7d953eda3a9",
   "metadata": {},
   "source": [
    "# Create Security Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bf11870e-3dee-4564-9c82-c78a0262ce77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sg-0ac7721dab43cc0fc'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "security_group_ids = get_security_group_ids(ec2_client, TAGS)\n",
    "\n",
    "security_group_exists = len(security_group_ids) > 0\n",
    "\n",
    "if len(security_group_ids) > 1:\n",
    "    raise RuntimeError\n",
    "elif security_group_exists:\n",
    "    security_group_id = security_group_ids[0]\n",
    "\n",
    "# The security group gets torn down when deleted, so there is no need to check the rules and rewrite all of them.\n",
    "\n",
    "if not security_group_exists:\n",
    "    print('Creating Security Group...')\n",
    "    response = ec2_client.create_security_group(\n",
    "        GroupName=f'eks-cluster-sg-{CLUSTER_NAME}',\n",
    "        TagSpecifications=[\n",
    "            {\n",
    "                'ResourceType': 'security-group', \n",
    "                'Tags': [{'Key': tag, 'Value': TAGS[tag]} for tag in TAGS]\n",
    "            }\n",
    "        ],\n",
    "        Description=f'Security group for EKS cluster: {CLUSTER_NAME}',\n",
    "        VpcId=vpc_id\n",
    "    )\n",
    "    security_group_id = response['GroupId']\n",
    "\n",
    "    # TODO: Can I make the CidrIp more restrictive for the next deployment? Load Balancer needs to have a static IP, probably through the Kubernetes YAML?\n",
    "    for port in ALLOWED_PORTS:\n",
    "        ec2_client.authorize_security_group_ingress(\n",
    "            GroupId=security_group_id,\n",
    "            IpPermissions=[\n",
    "                {\n",
    "                    'IpProtocol': 'tcp',\n",
    "                    'FromPort': port,\n",
    "                    'ToPort': port,\n",
    "                    'IpRanges': [{'CidrIp': '0.0.0.0/0'}]\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "    \n",
    "security_group_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6a6e50f1-8540-4dcf-9aa6-46c0fd668aea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('vol-021e0e2f73fe34170',\n",
       " 'vpc-0ba31ec9327f8e890',\n",
       " 'igw-0aab310379559d7cf',\n",
       " 'rtb-0fc1da438a5ff2898',\n",
       " 'subnet-0374c1250c2c5914e',\n",
       " ['subnet-09561967a87d79ac3', 'subnet-0374c1250c2c5914e'],\n",
       " 'sg-0ac7721dab43cc0fc')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "volume_id, vpc_id, igw_id, route_table_id, public_subnet_id, subnet_ids, security_group_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5e3e6a-1650-4a59-91b7-bff33b937a52",
   "metadata": {},
   "source": [
    "# Create Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e5e5fec8-eea5-4ef7-806c-4bdea1f61845",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ACTIVE'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tear_down_and_rebuild = False\n",
    "response = eks_client.list_clusters()\n",
    "clusters = response['clusters']\n",
    "if CLUSTER_NAME not in clusters:\n",
    "    eks_client.create_cluster(name=CLUSTER_NAME, \n",
    "                              version=K8S_VERSION, \n",
    "                              roleArn=f'arn:aws:iam::{aws_account_id}:role/EKS_Cluster_Role', \n",
    "                              resourcesVpcConfig={'subnetIds': subnet_ids,\n",
    "                                                  'securityGroupIds': [security_group_id],\n",
    "                                                  'endpointPublicAccess': True,\n",
    "                                                  'endpointPrivateAccess': False\n",
    "                              },\n",
    "                              tags=TAGS,\n",
    "                             )\n",
    "else:\n",
    "    if tear_down_and_rebuild:\n",
    "        pass\n",
    "        # TODO: Create a new cluster with a temp name.\n",
    "        # Wait until the cluster has finished forming\n",
    "        # Delete the old cluster\n",
    "        # Wait until deletion is complete\n",
    "        # Rename the new cluster\n",
    "        \n",
    "wait_until(eks_client.describe_cluster, {'name': CLUSTER_NAME}, is_cluster_active, timeout=7 * 60)\n",
    "response = eks_client.describe_cluster(name=CLUSTER_NAME)\n",
    "assert response['cluster']['status'] == 'ACTIVE'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7a812b-4a62-4b3b-a88e-90b0db82a194",
   "metadata": {},
   "source": [
    "# Create IAM Role for Node Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "49bcf925-9e7a-47af-9d1a-d160959148cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Role EKS_Cluster_Role already exists. Arn: arn:aws:iam::275678099358:role/EKS_Cluster_Role.\n",
      "Attached policy AmazonEKSWorkerNodePolicy to role EKS_Cluster_Role.\n",
      "Attached policy AmazonEC2ContainerRegistryReadOnly to role EKS_Cluster_Role.\n",
      "Attached policy AmazonEKS_CNI_Policy to role EKS_Cluster_Role.\n",
      "Attached policy AmazonEKSClusterPolicy to role EKS_Cluster_Role.\n"
     ]
    }
   ],
   "source": [
    "# Define the role name\n",
    "role_name = 'EKS_Cluster_Role'  # WARNING: I get a weird error if I call this role anything else. The call looks for this particular name, and I do not know how to override it.\n",
    "\n",
    "# Create the trust policy for the role\n",
    "trust_policy = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\n",
    "                \"Service\": [\n",
    "                    \"eks.amazonaws.com\",\n",
    "                    \"ec2.amazonaws.com\"\n",
    "                ]\n",
    "            },\n",
    "            \"Action\": \"sts:AssumeRole\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create the IAM role\n",
    "try:\n",
    "    response = iam_client.create_role(\n",
    "        RoleName=role_name,\n",
    "        AssumeRolePolicyDocument=json.dumps(trust_policy),\n",
    "        Description='Role for EKS Node Group'\n",
    "    )\n",
    "    node_role_arn = response['Role']['Arn']\n",
    "    print(f\"Created role: {node_role_arn}\")\n",
    "except iam_client.exceptions.EntityAlreadyExistsException:\n",
    "    response = iam_client.get_role(RoleName=role_name)\n",
    "    node_role_arn = response['Role']['Arn']\n",
    "    print(f\"Role {role_name} already exists. Arn: {node_role_arn}.\")\n",
    "    # TODO: Check if trust_policy is correct.\n",
    "\n",
    "# Attach necessary policies\n",
    "policies = [\n",
    "    'AmazonEKSWorkerNodePolicy',\n",
    "    'AmazonEC2ContainerRegistryReadOnly',\n",
    "    'AmazonEKS_CNI_Policy',\n",
    "    'AmazonEKSClusterPolicy',\n",
    "    # 'AmazonSSMManagedInstanceCore',\n",
    "]\n",
    "# Policy AmazonSSMManagedInstanceCore is not necessary, I used it for debugging, to connect to the node and run commands. \n",
    "\n",
    "for policy in policies:\n",
    "    try:\n",
    "        iam_client.attach_role_policy(\n",
    "            RoleName=role_name,\n",
    "            PolicyArn=f'arn:aws:iam::aws:policy/{policy}'\n",
    "        )\n",
    "        print(f\"Attached policy {policy} to role {role_name}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error attaching policy {policy}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8c8bd2-f80d-4a36-b648-4895429988ab",
   "metadata": {},
   "source": [
    "# Add Node Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "11c771d0-9929-4c01-8ae6-e7c0845760d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: This part may also need a wait."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "75cfa5fb-e965-4e51-bee6-8414c32a7689",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ACTIVE'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_groups = eks_client.list_nodegroups(clusterName=CLUSTER_NAME)\n",
    "# TODO: Make this static and put to the top, with different `diskSize` and `scalingConfig`\n",
    "node_group_name = 'gpu'\n",
    "ami_type = 'AL2_x86_64_GPU'\n",
    "if node_group_name in node_groups['nodegroups']:\n",
    "    response = eks_client.describe_nodegroup(clusterName=CLUSTER_NAME, nodegroupName=node_group_name)\n",
    "    status = response['nodegroup']['status']\n",
    "    if status == 'CREATE_FAILED':\n",
    "        print('Node group exists, but failed to create. Deleting...')\n",
    "        failed_node_group_info = response\n",
    "\n",
    "        eks_client.delete_nodegroup(clusterName=CLUSTER_NAME, nodegroupName=node_group_name)\n",
    "    \n",
    "        wait_until(eks_client.list_nodegroups, {'clusterName': CLUSTER_NAME}, lambda x: node_group_name not in x['nodegroups'])\n",
    "        node_groups = eks_client.list_nodegroups(clusterName=CLUSTER_NAME)\n",
    "    else:\n",
    "        print('Node group exists.')\n",
    "\n",
    "\n",
    "\n",
    "if not node_groups['nodegroups']:\n",
    "    print('Creating node group...')\n",
    "    response = eks_client.create_nodegroup(\n",
    "        clusterName=CLUSTER_NAME,\n",
    "        nodegroupName=node_group_name,\n",
    "        scalingConfig={\n",
    "            'desiredSize': 1,\n",
    "            'minSize': 1,\n",
    "            'maxSize': 1\n",
    "        },\n",
    "        diskSize=50,  # Size in GiB\n",
    "        subnets=[public_subnet_id],\n",
    "        nodeRole=node_role_arn,\n",
    "        amiType=ami_type,\n",
    "        instanceTypes=INSTANCE_TYPES[node_group_name],\n",
    "        labels={\n",
    "            'gpu-memory': 'true'\n",
    "        },\n",
    "        taints=[\n",
    "            {\n",
    "                'key': 'nvidia.com/gpu',\n",
    "                'value': 'true',\n",
    "                'effect': 'NO_SCHEDULE'\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "wait_until(eks_client.describe_nodegroup, {'clusterName': CLUSTER_NAME, 'nodegroupName': node_group_name}, is_node_group_active)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a69858b4-e5f2-4951-aa3d-69cd404c0e27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ACTIVE'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_groups = eks_client.list_nodegroups(clusterName=CLUSTER_NAME)\n",
    "# TODO: Make this static and put to the top, with different `diskSize` and `scalingConfig`\n",
    "node_group_name = 'default'\n",
    "ami_type = 'AL2_x86_64'\n",
    "if node_group_name in node_groups['nodegroups']:\n",
    "    response = eks_client.describe_nodegroup(clusterName=CLUSTER_NAME, nodegroupName=node_group_name)\n",
    "    status = response['nodegroup']['status']\n",
    "    if status == 'CREATE_FAILED':\n",
    "        print('Node group exists, but failed to create. Deleting...')\n",
    "        failed_node_group_info = response\n",
    "\n",
    "        eks_client.delete_nodegroup(clusterName=CLUSTER_NAME, nodegroupName=node_group_name)\n",
    "    \n",
    "        wait_until(eks_client.list_nodegroups, {'clusterName': CLUSTER_NAME}, lambda x: node_group_name not in x['nodegroups'])\n",
    "        node_groups = eks_client.list_nodegroups(clusterName=CLUSTER_NAME)\n",
    "    else:\n",
    "        print('Node group exists.')\n",
    "\n",
    "\n",
    "# TODO: See if lower disksize will work.\n",
    "if node_group_name not in node_groups['nodegroups']:\n",
    "    print('Creating node group...')\n",
    "    response = eks_client.create_nodegroup(\n",
    "        clusterName=CLUSTER_NAME,\n",
    "        nodegroupName=node_group_name,\n",
    "        scalingConfig={\n",
    "            'desiredSize': 2,\n",
    "            'minSize': 2,\n",
    "            'maxSize': 2\n",
    "        },\n",
    "        diskSize=1000,  # Size in GiB\n",
    "        subnets=[public_subnet_id],\n",
    "        nodeRole=node_role_arn,\n",
    "        amiType=ami_type,\n",
    "        instanceTypes=INSTANCE_TYPES[node_group_name],\n",
    "    )\n",
    "wait_until(eks_client.describe_nodegroup, {'clusterName': CLUSTER_NAME, 'nodegroupName': node_group_name}, is_node_group_active)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9df50a-ede3-4b60-af87-f7295796c94c",
   "metadata": {},
   "source": [
    "# Initialize Kubernetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "77edf8ec-0ec3-42d2-a8a6-f8dbf606364f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added new context arn:aws:eks:ca-central-1:275678099358:cluster/kubyterlab-llm to /root/.kube/config\n"
     ]
    }
   ],
   "source": [
    "!aws eks update-kubeconfig --name $CLUSTER_NAME --region $REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8ac935-b3bd-491f-9a4c-ec60bd3f3271",
   "metadata": {},
   "source": [
    "# Initialize Helm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9ec6ffe2-7458-4d1b-90dd-0b6fbbb0e921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"aws-ebs-csi-driver\" has been added to your repositories\n"
     ]
    }
   ],
   "source": [
    "!helm repo add aws-ebs-csi-driver https://kubernetes-sigs.github.io/aws-ebs-csi-driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1d3707f7-5505-4195-8666-748cda44c02a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"eks\" has been added to your repositories\n"
     ]
    }
   ],
   "source": [
    "!helm repo add eks https://aws.github.io/eks-charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5c979c04-3511-4ad9-a1f8-f2af1cdac0e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"ingress-nginx\" has been added to your repositories\n"
     ]
    }
   ],
   "source": [
    "!helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "79fbbaa4-e44b-4a43-bfd1-04d3a9dcf4fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hang tight while we grab the latest from your chart repositories...\n",
      "...Successfully got an update from the \"aws-ebs-csi-driver\" chart repository\n",
      "...Successfully got an update from the \"ingress-nginx\" chart repository\n",
      "...Successfully got an update from the \"eks\" chart repository\n",
      "Update Complete. ⎈Happy Helming!⎈\n"
     ]
    }
   ],
   "source": [
    "!helm repo update"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1566d62-4c89-4f87-84a1-5d465d8d89d4",
   "metadata": {},
   "source": [
    "# Create a Role to Install the CSI Driver and Give Permissions Using the OIDC Provider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "baafae37-f87f-47bb-a642-fc1c1c6cb5a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://oidc.eks.ca-central-1.amazonaws.com/id/26168D03C1C42A23884C0AE1BA33F5A7'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_response = eks_client.describe_cluster(name=CLUSTER_NAME)\n",
    "oidc_id = cluster_response['cluster']['identity']['oidc']['issuer'].split('/')[-1]\n",
    "oidc_url = f'https://oidc.eks.{REGION}.amazonaws.com/id/{oidc_id}'\n",
    "oidc_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "47f875e6-9a22-4615-98a9-eb43c520fd94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m2025-02-22 23:59:41 [ℹ]  IAM Open ID Connect provider is already associated with cluster \"kubyterlab-llm\" in \"ca-central-1\"\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!eksctl utils associate-iam-oidc-provider --cluster $CLUSTER_NAME --approve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "023b4e7f-f182-4474-a035-4027c0c75351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arn:aws:iam::275678099358:policy/AmazonEKS_EBS_CSI_Driver_Policy'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazon_eks_ebs_csi_driver_policy_name = 'AmazonEKS_EBS_CSI_Driver_Policy'\n",
    "amazon_eks_ebs_csi_driver_policy = {\n",
    "  \"Version\": \"2012-10-17\",\n",
    "  \"Statement\": [\n",
    "    {\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Action\": [\n",
    "        \"ec2:CreateSnapshot\",\n",
    "        \"ec2:AttachVolume\",\n",
    "        \"ec2:DetachVolume\",\n",
    "        \"ec2:ModifyVolume\",\n",
    "        \"ec2:DescribeAvailabilityZones\",\n",
    "        \"ec2:DescribeInstances\",\n",
    "        \"ec2:DescribeSnapshots\",\n",
    "        \"ec2:DescribeTags\",\n",
    "        \"ec2:DescribeVolumes\",\n",
    "        \"ec2:DescribeVolumesModifications\"\n",
    "      ],\n",
    "      \"Resource\": \"*\"\n",
    "    },\n",
    "    {\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Action\": [\n",
    "        \"ec2:CreateTags\"\n",
    "      ],\n",
    "      \"Resource\": [\n",
    "        \"arn:aws:ec2:*:*:volume/*\",\n",
    "        \"arn:aws:ec2:*:*:snapshot/*\"\n",
    "      ],\n",
    "      \"Condition\": {\n",
    "        \"StringEquals\": {\n",
    "          \"ec2:CreateAction\": [\n",
    "            \"CreateVolume\",\n",
    "            \"CreateSnapshot\"\n",
    "          ]\n",
    "        }\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Action\": [\n",
    "        \"ec2:DeleteTags\"\n",
    "      ],\n",
    "      \"Resource\": [\n",
    "        \"arn:aws:ec2:*:*:volume/*\",\n",
    "        \"arn:aws:ec2:*:*:snapshot/*\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Action\": [\n",
    "        \"ec2:CreateVolume\"\n",
    "      ],\n",
    "      \"Resource\": \"*\",\n",
    "      \"Condition\": {\n",
    "        \"StringLike\": {\n",
    "          \"aws:RequestTag/ebs.csi.aws.com/cluster\": \"true\"\n",
    "        }\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Action\": [\n",
    "        \"ec2:CreateVolume\"\n",
    "      ],\n",
    "      \"Resource\": \"*\",\n",
    "      \"Condition\": {\n",
    "        \"StringLike\": {\n",
    "          \"aws:RequestTag/CSIVolumeName\": \"*\"\n",
    "        }\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Action\": [\n",
    "        \"ec2:DeleteVolume\"\n",
    "      ],\n",
    "      \"Resource\": \"*\",\n",
    "      \"Condition\": {\n",
    "        \"StringLike\": {\n",
    "          \"ec2:ResourceTag/CSIVolumeName\": \"*\"\n",
    "        }\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Action\": [\n",
    "        \"ec2:DeleteVolume\"\n",
    "      ],\n",
    "      \"Resource\": \"*\",\n",
    "      \"Condition\": {\n",
    "        \"StringLike\": {\n",
    "          \"ec2:ResourceTag/ebs.csi.aws.com/cluster\": \"true\"\n",
    "        }\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Action\": [\n",
    "        \"ec2:DeleteSnapshot\"\n",
    "      ],\n",
    "      \"Resource\": \"*\",\n",
    "      \"Condition\": {\n",
    "        \"StringLike\": {\n",
    "          \"ec2:ResourceTag/CSIVolumeSnapshotName\": \"*\"\n",
    "        }\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Action\": [\n",
    "        \"ec2:DeleteSnapshot\"\n",
    "      ],\n",
    "      \"Resource\": \"*\",\n",
    "      \"Condition\": {\n",
    "        \"StringLike\": {\n",
    "          \"ec2:ResourceTag/ebs.csi.aws.com/cluster\": \"true\"\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "try:\n",
    "    response = iam_client.create_policy(\n",
    "        PolicyName=amazon_eks_ebs_csi_driver_policy_name,\n",
    "        PolicyDocument=json.dumps(amazon_eks_ebs_csi_driver_policy)\n",
    "    )\n",
    "    amazon_eks_ebs_csi_driver_policy_arn = response['Policy']['Arn']\n",
    "except iam_client.exceptions.EntityAlreadyExistsException:\n",
    "    amazon_eks_ebs_csi_driver_policy_arn = f'arn:aws:iam::{aws_account_id}:policy/{amazon_eks_ebs_csi_driver_policy_name}'\n",
    "    response = iam_client.get_policy(PolicyArn=amazon_eks_ebs_csi_driver_policy_arn)\n",
    "\n",
    "    # TODO: If policy body is not the same, update. Code to update follows.\n",
    "    # iam_client.create_policy_version(\n",
    "    #     PolicyArn=policy_arn,\n",
    "    #     PolicyDocument=policy_document_json,\n",
    "    #     SetAsDefault=True\n",
    "    # )\n",
    "\n",
    "amazon_eks_ebs_csi_driver_policy_arn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e4e0ebf6-a8ef-42f7-bc4d-ba852964f0da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Role already exists. Updating the trust policy.\n"
     ]
    }
   ],
   "source": [
    "trust_policy = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\n",
    "                \"Federated\": f\"arn:aws:iam::{aws_account_id}:oidc-provider/oidc.eks.{REGION}.amazonaws.com/id/{oidc_id}\"\n",
    "            },\n",
    "            \"Action\": \"sts:AssumeRoleWithWebIdentity\",\n",
    "            \"Condition\": {\n",
    "                \"StringEquals\": {\n",
    "                    f\"oidc.eks.{REGION}.amazonaws.com/id/{oidc_id}:aud\": \"sts.amazonaws.com\",\n",
    "                    f\"oidc.eks.{REGION}.amazonaws.com/id/{oidc_id}:sub\": \"system:serviceaccount:kube-system:ebs-csi-controller-sa\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "try:\n",
    "    create_role_response = iam_client.create_role(\n",
    "        RoleName='AmazonEKS_EBS_CSI_DriverRole',\n",
    "        \n",
    "        AssumeRolePolicyDocument=json.dumps(trust_policy),\n",
    "        Description='Role for Amazon EKS EBS CSI Driver'\n",
    "    )\n",
    "    print(f\"Role created successfully: {create_role_response['Role']['Arn']}\")\n",
    "except iam_client.exceptions.EntityAlreadyExistsException:\n",
    "    print(\"Role already exists. Updating the trust policy.\")\n",
    "    iam_client.update_assume_role_policy(\n",
    "        RoleName='AmazonEKS_EBS_CSI_DriverRole',\n",
    "        PolicyDocument=json.dumps(trust_policy)\n",
    "    )\n",
    "\n",
    "attach_policy_response = iam_client.attach_role_policy(\n",
    "    RoleName='AmazonEKS_EBS_CSI_DriverRole',\n",
    "    PolicyArn='arn:aws:iam::aws:policy/service-role/AmazonEBSCSIDriverPolicy'\n",
    ")\n",
    "\n",
    "attach_policy_response = iam_client.attach_role_policy(\n",
    "    RoleName='AmazonEKS_EBS_CSI_DriverRole',\n",
    "    PolicyArn=amazon_eks_ebs_csi_driver_policy_arn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa839d9c-00b3-41f3-ba6b-f70575889176",
   "metadata": {},
   "source": [
    "# Install CSI Driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e2451fff-9fa9-4f93-b6c1-f3b26924ac3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error: failed to create serviceaccount: serviceaccounts \"ebs-csi-controller-sa\" already exists\n"
     ]
    }
   ],
   "source": [
    "!kubectl create sa ebs-csi-controller-sa -n kube-system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fc410c14-878b-4af9-b960-40503d451a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "serviceaccount/ebs-csi-controller-sa annotated\n"
     ]
    }
   ],
   "source": [
    "!kubectl annotate sa ebs-csi-controller-sa -n kube-system eks.amazonaws.com/role-arn=$ebs_csi_driver_role_arn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c15647b1-29f9-445e-a115-68db3fbec3d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "serviceaccount/ebs-csi-controller-sa annotated\n"
     ]
    }
   ],
   "source": [
    "!kubectl annotate sa ebs-csi-controller-sa -n kube-system meta.helm.sh/release-namespace=kube-system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "feef5937-e243-4ae6-84ee-5be572ad1c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "serviceaccount/ebs-csi-controller-sa annotated\n"
     ]
    }
   ],
   "source": [
    "!kubectl annotate sa ebs-csi-controller-sa -n kube-system meta.helm.sh/release-name=aws-ebs-csi-driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b0b85917-e62c-42aa-8f71-7a0001265efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "serviceaccount/ebs-csi-controller-sa labeled\n"
     ]
    }
   ],
   "source": [
    "!kubectl label sa ebs-csi-controller-sa -n kube-system app.kubernetes.io/managed-by=Helm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "65c7ca0d-34d7-4b6f-93cb-f39379235bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Release \"aws-ebs-csi-driver\" does not exist. Installing it now.\n",
      "NAME: aws-ebs-csi-driver\n",
      "LAST DEPLOYED: Sat Feb 22 19:09:49 2025\n",
      "NAMESPACE: kube-system\n",
      "STATUS: deployed\n",
      "REVISION: 1\n",
      "NOTES:\n",
      "To verify that aws-ebs-csi-driver has started, run:\n",
      "\n",
      "    kubectl get pod -n kube-system -l \"app.kubernetes.io/name=aws-ebs-csi-driver,app.kubernetes.io/instance=aws-ebs-csi-driver\"\n",
      "\n",
      "[ACTION REQUIRED] Update to the EBS CSI Driver IAM Policy\n",
      "\n",
      "Due to an upcoming change in handling of IAM polices for the CreateVolume API when creating a volume from an EBS snapshot, a change to your EBS CSI Driver policy may be needed. For more information and remediation steps, see GitHub issue #2190 (https://github.com/kubernetes-sigs/aws-ebs-csi-driver/issues/2190). This change affects all versions of the EBS CSI Driver and action may be required even on clusters where the driver is not upgraded.\n"
     ]
    }
   ],
   "source": [
    "!helm upgrade --install aws-ebs-csi-driver \\\n",
    "    --namespace kube-system \\\n",
    "    aws-ebs-csi-driver/aws-ebs-csi-driver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f599762-1e35-459c-a195-464eb9bd7f1c",
   "metadata": {},
   "source": [
    "### == End of Code Specific to AWS =="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bcb2b6-3c4b-4d3e-9d6c-2daf8baadfee",
   "metadata": {},
   "source": [
    "# Install Nvidia Device Plugin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2715339c-3b4c-4203-92e0-15b51e2fd75d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "daemonset.apps/nvidia-device-plugin-daemonset created\n"
     ]
    }
   ],
   "source": [
    "!kubectl apply -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/refs/heads/main/deployments/static/nvidia-device-plugin.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5a0d46-7a29-4c21-80d0-1ba0f5f9b5fd",
   "metadata": {},
   "source": [
    "# Apply Helm Charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3b606826-23b1-423b-8a87-b078d28afda9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'vol-021e0e2f73fe34170'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "volume_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e803a8bf-61c4-422f-8ed9-8c9f04a46916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME: storage\n",
      "LAST DEPLOYED: Sat Feb 22 19:52:35 2025\n",
      "NAMESPACE: default\n",
      "STATUS: deployed\n",
      "REVISION: 1\n",
      "TEST SUITE: None\n"
     ]
    }
   ],
   "source": [
    "!helm install storage /helm/storage/ --set volumeID=$volume_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "66755be9-07cc-4d24-a0f3-bae41cecd91a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME: ingress-nginx\n",
      "LAST DEPLOYED: Sat Feb 22 19:52:40 2025\n",
      "NAMESPACE: ingress-nginx\n",
      "STATUS: deployed\n",
      "REVISION: 1\n",
      "TEST SUITE: None\n",
      "NOTES:\n",
      "The ingress-nginx controller has been installed.\n",
      "It may take a few minutes for the load balancer IP to be available.\n",
      "You can watch the status by running 'kubectl get service --namespace ingress-nginx ingress-nginx-controller --output wide --watch'\n",
      "\n",
      "An example Ingress that makes use of the controller:\n",
      "  apiVersion: networking.k8s.io/v1\n",
      "  kind: Ingress\n",
      "  metadata:\n",
      "    name: example\n",
      "    namespace: foo\n",
      "  spec:\n",
      "    ingressClassName: nginx\n",
      "    rules:\n",
      "      - host: www.example.com\n",
      "        http:\n",
      "          paths:\n",
      "            - pathType: Prefix\n",
      "              backend:\n",
      "                service:\n",
      "                  name: exampleService\n",
      "                  port:\n",
      "                    number: 80\n",
      "              path: /\n",
      "    # This section is only required if TLS is to be enabled for the Ingress\n",
      "    tls:\n",
      "      - hosts:\n",
      "        - www.example.com\n",
      "        secretName: example-tls\n",
      "\n",
      "If TLS is enabled for the Ingress, a Secret containing the certificate and key must also be provided:\n",
      "\n",
      "  apiVersion: v1\n",
      "  kind: Secret\n",
      "  metadata:\n",
      "    name: example-tls\n",
      "    namespace: foo\n",
      "  data:\n",
      "    tls.crt: <base64 encoded cert>\n",
      "    tls.key: <base64 encoded key>\n",
      "  type: kubernetes.io/tls\n"
     ]
    }
   ],
   "source": [
    "!helm install ingress-nginx ingress-nginx/ingress-nginx --namespace ingress-nginx --create-namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b905deee-a92e-4bb3-b23c-789c090924a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Release \"lb\" does not exist. Installing it now.\n",
      "NAME: lb\n",
      "LAST DEPLOYED: Sat Feb 22 19:53:00 2025\n",
      "NAMESPACE: default\n",
      "STATUS: deployed\n",
      "REVISION: 1\n",
      "TEST SUITE: None\n"
     ]
    }
   ],
   "source": [
    "# !helm upgrade --install lb /helm/lb-ingress/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "17474e33-1f9b-448f-9819-f2b432063da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: uninstall: Release not loaded: lb: release: not found\n"
     ]
    }
   ],
   "source": [
    "# !helm delete lb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "f0e0d633-e0aa-43d3-96ee-71ebe1163431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Release \"ingress\" has been upgraded. Happy Helming!\n",
      "NAME: ingress\n",
      "LAST DEPLOYED: Sun Feb 23 00:35:47 2025\n",
      "NAMESPACE: default\n",
      "STATUS: deployed\n",
      "REVISION: 18\n",
      "TEST SUITE: None\n"
     ]
    }
   ],
   "source": [
    "!helm upgrade --install ingress /helm/eberron-agent-ingress/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "373f30d2-a911-47ac-99ea-9f402c13db23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Release \"fastapi\" has been upgraded. Happy Helming!\n",
      "NAME: fastapi\n",
      "LAST DEPLOYED: Sat Feb 22 23:59:59 2025\n",
      "NAMESPACE: default\n",
      "STATUS: deployed\n",
      "REVISION: 2\n",
      "TEST SUITE: None\n"
     ]
    }
   ],
   "source": [
    "!helm upgrade --install fastapi /helm/eberron-agent-server/ --set awsAccountId=$aws_account_id --set region=$REGION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "74e4bd90-d8bd-421f-b0cb-9b90ecb046e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Release \"frontend\" has been upgraded. Happy Helming!\n",
      "NAME: frontend\n",
      "LAST DEPLOYED: Sun Feb 23 00:00:01 2025\n",
      "NAMESPACE: default\n",
      "STATUS: deployed\n",
      "REVISION: 2\n",
      "TEST SUITE: None\n"
     ]
    }
   ],
   "source": [
    "!helm upgrade --install frontend /helm/eberron-agent-frontend/ --set awsAccountId=$aws_account_id --set region=$REGION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97f5d85-1064-4d39-b205-71fddb658689",
   "metadata": {},
   "source": [
    "# Get URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6ec37f9a-6ffd-4e4e-b9a2-f0955ae0423a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fastapi-677965765c-qkfn4'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Update get_one_running_pod to use staring with names.\n",
    "wait_until(get_one_running_pod, {'prefix': 'fastapi-'}, lambda x: x is not None, timeout=8 * 60)\n",
    "pod_name = get_one_running_pod()\n",
    "assert pod_name is not None\n",
    "pod_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3cee78c8-55c3-43d9-bd54-258451b2520d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output = !kubectl get service lb --no-headers\n",
    "# fields = re.split(r'\\s+', output[0])\n",
    "# external_url = fields[3]\n",
    "# port = fields[4].split('/')[0].split(':')[0]\n",
    "# external_url, port"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1ba7411d-d1ea-46b9-b6d9-eafe4f8fc546",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('aa703202c6c1849ce9814d2c1fbe6a9c-697758537.ca-central-1.elb.amazonaws.com',\n",
       " '80')"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "output = !kubectl get service ingress-nginx-controller --namespace ingress-nginx --no-headers\n",
    "fields = re.split(r'\\s+', output[0])\n",
    "external_url = fields[3]\n",
    "port = fields[4].split('/')[0].split(':')[0]\n",
    "external_url, port"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8bf520ca-59d0-427f-8251-bc940cad8e46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://aa703202c6c1849ce9814d2c1fbe6a9c-697758537.ca-central-1.elb.amazonaws.com:80'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = f'http://{external_url}:{port}'\n",
    "url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c7f019-64b1-4b2d-a343-b536fca16633",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "438b3f0b-0cc8-4aad-b796-5e7bb32fd110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Defaulted container \"kubyterlab-llm\" out of: kubyterlab-llm, init-container (init)\n",
    "# True\n",
    "# 1\n",
    "!kubectl exec $pod_name bash -- python3 -c 'import torch; print(torch.cuda.is_available()); print(torch.cuda.device_count())'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6bfde574-66a3-4a21-b26d-0cab8eac0893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory.total [MiB], memory.used [MiB], memory.free [MiB]\n",
      "15360 MiB, 4087 MiB, 11009 MiB\n"
     ]
    }
   ],
   "source": [
    "!kubectl exec $pod_name bash -- bash -c 'nvidia-smi --query-gpu=memory.total,memory.used,memory.free --format=csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a3c0a9bf-9d43-4149-ac2c-229059865aaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The languages in Eberron include, but are not limited to:\n",
       "\n",
       "1. Abyssal: Common tongue of all fiends, also known as \"Khyber’s Speech.\"\n",
       "2. Argon: Spoken by barbarians of Argonnessen.\n",
       "3. Auran: Spoken by air-based creatures like elves.\n",
       "4. Common: The language of the Five Nations and the language of trade in Khorvaire, known by most of its people.\n",
       "5. Daan: Spoken by formians, lawful outsiders, and other daelkyr aberrations.\n",
       "6. Daelkyr: Spoken by the daelkyr, mind flayers, and other creatures of Xoriat.\n",
       "7. Draconic: Spoken by kobolds, troglodytes, lizardfolk, dragons, and others.\n",
       "8. Druidic: Spoken by druids (only).\n",
       "9. Dwarven: Spoken by dwarves.\n",
       "10. Elven: Spoken by elves and drow.\n",
       "11. Giant: Spoken by ogres, giants, and drow.\n",
       "12. Gnoll: Spoken by gnolls.\n",
       "13. Goblin: Spoken by goblins, hobgoblins, and bugbears.\n",
       "14. Halfling: Spoken by halflings.\n",
       "15. Ignan: Spoken by fire-based creatures.\n",
       "16. Infernal: Spoken by devils of Shavarath.\n",
       "17. Irial: Spoken by Ravids, positive energy users.\n",
       "18. Kythric: Spoken by Slaadi, chaotic outsiders.\n",
       "19. Mabran: Spoken by Nightshades, shadows, and draconic creatures of Mabar.\n",
       "20. Ore: Spoken by orcs.\n",
       "21. Quori: Spoken by the Inspired, kalashtar.\n",
       "22. Riedran: Spoken by the lower classes of Sarlona.\n",
       "23. Risian: Spoken by ice-based creatures.\n",
       "24. Sylvan: Spoken by dryads, eladrins, and creatures of Thelanis.\n",
       "25. Syranian: Spoken by angels of Syrania.\n",
       "26. Terran: Spoken by xorns and other earth-based creatures.\n",
       "27. Undercommon: Spoken by chokers and underground Daelkyr denizens.\n",
       "\n",
       "This list is not exhaustive, and there may be other languages in Eberron. The use of a language often reflects the culture and geography of the character or creature. For example, an orc from Droaam likely speaks Goblin instead of Giant."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "from IPython.display import clear_output, display, HTML, Markdown\n",
    "\n",
    "response = requests.post(f\"{url}/respond\", \n",
    "                         headers={\"Content-Type\": \"application/json\", \n",
    "                                  \"Accept\": \"text/event-stream\"}, \n",
    "                         json={\"content\": \"What are the languages in Eberron?\"}, \n",
    "                         stream=True)\n",
    "md = \"\"\n",
    "for chunk in response:\n",
    "    md += chunk.decode(\"utf-8\")\n",
    "    clear_output()\n",
    "    display(Markdown(md.replace('\\\\n', '\\n')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4ae1ff-9361-4709-8343-2b2310999403",
   "metadata": {},
   "source": [
    "# == End of Procedure =="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54d53ff-124d-409d-95f3-6985f7382654",
   "metadata": {},
   "source": [
    "## == Do Not Continue: rest of the code is only for reference for troubleshooting =="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bd4f0a-fa1d-4578-89d6-84b9c30c2179",
   "metadata": {},
   "source": [
    "# Troubleshoot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2af3512-4723-4393-8d3a-56bdc6fa64f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# curl -X POST -H \"Content-Type: application/json\" -H \"Accept: text/event-stream\" -d '{\"key\":\"value\"}' http://your-api-endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f10cb1-1b0f-4212-a689-052928a4924d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that for the following to work, you need to attach the policy AmazonSSMManagedInstanceCore, in the relevant cell above, during the deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "44b4b8fb-d7fb-4c9c-87ba-777bf78f1f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = !aws ssm send-command \\\n",
    "    --document-name \"AWS-RunShellScript\" \\\n",
    "    --targets \"Key=instanceIds,Values=i-02f3c1f8a118eb352\" \\\n",
    "    --parameters 'commands=[\"nvidia-smi --query-gpu=memory.total,memory.used,memory.free --format=csv\"]' \\\n",
    "    --region $REGION\n",
    "command_id = json.loads(''.join(output))['Command']['CommandId']\n",
    "command_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45b62bf-644f-41ce-a4bf-f745311de116",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = !aws ssm list-command-invocations \\\n",
    "    --command-id $command_id \\\n",
    "    --details\n",
    "output = json.loads(''.join(output))['CommandInvocations'][0]['CommandPlugins'][0]['Output']\n",
    "for line in output.split('\\n'):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0480a8c3-9aad-4a76-9f62-20de23589501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAST SEEN   TYPE      REASON                    OBJECT                                             MESSAGE\n",
      "59m         Normal    Starting                  node/ip-10-0-1-161.ca-central-1.compute.internal   \n",
      "58m         Normal    Starting                  node/ip-10-0-1-137.ca-central-1.compute.internal   \n",
      "58m         Normal    Starting                  node/ip-10-0-1-15.ca-central-1.compute.internal    \n",
      "59m         Warning   CgroupV1                  node/ip-10-0-1-161.ca-central-1.compute.internal   Cgroup v1 support is in maintenance mode, please migrate to Cgroup v2.\n",
      "59m         Normal    Starting                  node/ip-10-0-1-161.ca-central-1.compute.internal   Starting kubelet.\n",
      "59m         Normal    NodeAllocatableEnforced   node/ip-10-0-1-161.ca-central-1.compute.internal   Updated Node Allocatable limit across pods\n",
      "59m         Normal    NodeHasSufficientPID      node/ip-10-0-1-161.ca-central-1.compute.internal   Node ip-10-0-1-161.ca-central-1.compute.internal status is now: NodeHasSufficientPID\n",
      "59m         Normal    NodeHasNoDiskPressure     node/ip-10-0-1-161.ca-central-1.compute.internal   Node ip-10-0-1-161.ca-central-1.compute.internal status is now: NodeHasNoDiskPressure\n",
      "59m         Normal    NodeHasSufficientMemory   node/ip-10-0-1-161.ca-central-1.compute.internal   Node ip-10-0-1-161.ca-central-1.compute.internal status is now: NodeHasSufficientMemory\n",
      "59m         Warning   InvalidDiskCapacity       node/ip-10-0-1-161.ca-central-1.compute.internal   invalid capacity 0 on image filesystem\n",
      "59m         Normal    Synced                    node/ip-10-0-1-161.ca-central-1.compute.internal   Node synced successfully\n",
      "58m         Normal    RegisteredNode            node/ip-10-0-1-161.ca-central-1.compute.internal   Node ip-10-0-1-161.ca-central-1.compute.internal event: Registered Node ip-10-0-1-161.ca-central-1.compute.internal in Controller\n",
      "58m         Normal    NodeReady                 node/ip-10-0-1-161.ca-central-1.compute.internal   Node ip-10-0-1-161.ca-central-1.compute.internal status is now: NodeReady\n",
      "58m         Normal    NodeHasSufficientPID      node/ip-10-0-1-137.ca-central-1.compute.internal   Node ip-10-0-1-137.ca-central-1.compute.internal status is now: NodeHasSufficientPID\n",
      "58m         Normal    NodeAllocatableEnforced   node/ip-10-0-1-137.ca-central-1.compute.internal   Updated Node Allocatable limit across pods\n",
      "58m         Normal    Starting                  node/ip-10-0-1-137.ca-central-1.compute.internal   Starting kubelet.\n",
      "58m         Warning   InvalidDiskCapacity       node/ip-10-0-1-137.ca-central-1.compute.internal   invalid capacity 0 on image filesystem\n",
      "58m         Warning   CgroupV1                  node/ip-10-0-1-137.ca-central-1.compute.internal   Cgroup v1 support is in maintenance mode, please migrate to Cgroup v2.\n",
      "58m         Normal    NodeHasSufficientMemory   node/ip-10-0-1-137.ca-central-1.compute.internal   Node ip-10-0-1-137.ca-central-1.compute.internal status is now: NodeHasSufficientMemory\n",
      "58m         Normal    NodeHasNoDiskPressure     node/ip-10-0-1-137.ca-central-1.compute.internal   Node ip-10-0-1-137.ca-central-1.compute.internal status is now: NodeHasNoDiskPressure\n",
      "58m         Warning   CgroupV1                  node/ip-10-0-1-15.ca-central-1.compute.internal    Cgroup v1 support is in maintenance mode, please migrate to Cgroup v2.\n",
      "58m         Normal    Synced                    node/ip-10-0-1-137.ca-central-1.compute.internal   Node synced successfully\n",
      "58m         Normal    Starting                  node/ip-10-0-1-15.ca-central-1.compute.internal    Starting kubelet.\n",
      "58m         Warning   InvalidDiskCapacity       node/ip-10-0-1-15.ca-central-1.compute.internal    invalid capacity 0 on image filesystem\n",
      "58m         Normal    NodeHasSufficientMemory   node/ip-10-0-1-15.ca-central-1.compute.internal    Node ip-10-0-1-15.ca-central-1.compute.internal status is now: NodeHasSufficientMemory\n",
      "58m         Normal    NodeHasNoDiskPressure     node/ip-10-0-1-15.ca-central-1.compute.internal    Node ip-10-0-1-15.ca-central-1.compute.internal status is now: NodeHasNoDiskPressure\n",
      "58m         Normal    NodeHasSufficientPID      node/ip-10-0-1-15.ca-central-1.compute.internal    Node ip-10-0-1-15.ca-central-1.compute.internal status is now: NodeHasSufficientPID\n",
      "58m         Normal    NodeAllocatableEnforced   node/ip-10-0-1-15.ca-central-1.compute.internal    Updated Node Allocatable limit across pods\n",
      "58m         Normal    Synced                    node/ip-10-0-1-15.ca-central-1.compute.internal    Node synced successfully\n",
      "58m         Normal    RegisteredNode            node/ip-10-0-1-137.ca-central-1.compute.internal   Node ip-10-0-1-137.ca-central-1.compute.internal event: Registered Node ip-10-0-1-137.ca-central-1.compute.internal in Controller\n",
      "58m         Normal    RegisteredNode            node/ip-10-0-1-15.ca-central-1.compute.internal    Node ip-10-0-1-15.ca-central-1.compute.internal event: Registered Node ip-10-0-1-15.ca-central-1.compute.internal in Controller\n",
      "58m         Normal    NodeReady                 node/ip-10-0-1-137.ca-central-1.compute.internal   Node ip-10-0-1-137.ca-central-1.compute.internal status is now: NodeReady\n",
      "58m         Normal    NodeReady                 node/ip-10-0-1-15.ca-central-1.compute.internal    Node ip-10-0-1-15.ca-central-1.compute.internal status is now: NodeReady\n",
      "14m         Normal    EnsuringLoadBalancer      service/lb                                         Ensuring load balancer\n",
      "14m         Normal    EnsuredLoadBalancer       service/lb                                         Ensured load balancer\n",
      "14m         Normal    SuccessfulCreate          replicaset/fastapi-677965765c                      Created pod: fastapi-677965765c-qkfn4\n",
      "14m         Normal    ScalingReplicaSet         deployment/fastapi                                 Scaled up replica set fastapi-677965765c to 1\n",
      "14m         Normal    Scheduled                 pod/fastapi-677965765c-qkfn4                       Successfully assigned default/fastapi-677965765c-qkfn4 to ip-10-0-1-161.ca-central-1.compute.internal\n",
      "14m         Normal    SuccessfulAttachVolume    pod/fastapi-677965765c-qkfn4                       AttachVolume.Attach succeeded for volume \"pv-llm\"\n",
      "14m         Normal    Scheduled                 pod/frontend-74b7bcf65b-9gpm5                      Successfully assigned default/frontend-74b7bcf65b-9gpm5 to ip-10-0-1-15.ca-central-1.compute.internal\n",
      "14m         Normal    Pulling                   pod/frontend-74b7bcf65b-9gpm5                      Pulling image \"275678099358.dkr.ecr.ca-central-1.amazonaws.com/multi-agent/eberron-agent-frontend:25.02\"\n",
      "14m         Normal    ScalingReplicaSet         deployment/frontend                                Scaled up replica set frontend-74b7bcf65b to 1\n",
      "14m         Normal    SuccessfulCreate          replicaset/frontend-74b7bcf65b                     Created pod: frontend-74b7bcf65b-9gpm5\n",
      "14m         Normal    Pulling                   pod/fastapi-677965765c-qkfn4                       Pulling image \"275678099358.dkr.ecr.ca-central-1.amazonaws.com/multi-agent/eberron-agent-server:25.02\"\n",
      "14m         Normal    Started                   pod/frontend-74b7bcf65b-9gpm5                      Started container frontend\n",
      "14m         Normal    Pulled                    pod/frontend-74b7bcf65b-9gpm5                      Successfully pulled image \"275678099358.dkr.ecr.ca-central-1.amazonaws.com/multi-agent/eberron-agent-frontend:25.02\" in 21.701s (21.701s including waiting). Image size: 217536832 bytes.\n",
      "14m         Normal    Created                   pod/frontend-74b7bcf65b-9gpm5                      Created container frontend\n",
      "10m         Normal    Created                   pod/fastapi-677965765c-qkfn4                       Created container fastapi\n",
      "10m         Normal    Pulled                    pod/fastapi-677965765c-qkfn4                       Successfully pulled image \"275678099358.dkr.ecr.ca-central-1.amazonaws.com/multi-agent/eberron-agent-server:25.02\" in 4m12.592s (4m12.592s including waiting). Image size: 14071664304 bytes.\n",
      "10m         Normal    Started                   pod/fastapi-677965765c-qkfn4                       Started container fastapi\n",
      "54s         Normal    DeletingLoadBalancer      service/lb                                         Deleting load balancer\n",
      "32s         Normal    DeletedLoadBalancer       service/lb                                         Deleted load balancer\n"
     ]
    }
   ],
   "source": [
    "!kubectl get events --sort-by='.lastTimestamp'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39f876b-ea67-4dd5-a9ee-92af8f218ce9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a8992595-dd14-4067-8607-3596e7fa746c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                 READY   STATUS    RESTARTS   AGE\n",
      "ebs-csi-controller-fcb84d9bc-g9pdj   5/5     Running   0          4h52m\n",
      "ebs-csi-controller-fcb84d9bc-hp5lz   5/5     Running   0          4h52m\n",
      "ebs-csi-node-frpqr                   3/3     Running   0          4h52m\n",
      "ebs-csi-node-rh8fz                   3/3     Running   0          4h52m\n",
      "ebs-csi-node-zvvhd                   3/3     Running   0          4h52m\n"
     ]
    }
   ],
   "source": [
    "!kubectl get pod -n kube-system -l \"app.kubernetes.io/name=aws-ebs-csi-driver,app.kubernetes.io/instance=aws-ebs-csi-driver\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3b9460f8-e4d4-4247-908f-22706f2d876d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                          GPU\n",
      "ip-10-0-1-137.ca-central-1.compute.internal   <none>\n",
      "ip-10-0-1-15.ca-central-1.compute.internal    <none>\n",
      "ip-10-0-1-161.ca-central-1.compute.internal   1\n"
     ]
    }
   ],
   "source": [
    "# NAME                                          GPU\n",
    "# ip-10-0-1-129.ca-central-1.compute.internal   <none>\n",
    "# ip-10-0-1-223.ca-central-1.compute.internal   <none>\n",
    "# ip-10-0-1-8.ca-central-1.compute.internal     1z\n",
    "!kubectl get nodes \"-o=custom-columns=NAME:.metadata.name,GPU:.status.allocatable.nvidia\\.com/gpu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5d4e2db7-eec8-484d-af50-ff23a339c8f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                          TAINTS\n",
      "ip-10-0-1-137.ca-central-1.compute.internal   <none>\n",
      "ip-10-0-1-15.ca-central-1.compute.internal    <none>\n",
      "ip-10-0-1-161.ca-central-1.compute.internal   [map[effect:NoSchedule key:nvidia.com/gpu value:true]]\n"
     ]
    }
   ],
   "source": [
    "# NAME                                          TAINTS\n",
    "# ip-10-0-1-107.ca-central-1.compute.internal   <none>\n",
    "# ip-10-0-1-175.ca-central-1.compute.internal   [map[effect:NoSchedule key:nvidia.com/gpu value:true]]\n",
    "# ip-10-0-1-90.ca-central-1.compute.internal    <none>\n",
    "!kubectl get nodes -o custom-columns=NAME:.metadata.name,TAINTS:.spec.taints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "35f0a916-0089-4030-97aa-a074fe340403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME         TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE\n",
      "fastapi      ClusterIP   172.20.157.106   <none>        8000/TCP   4h8m\n",
      "frontend     ClusterIP   172.20.163.190   <none>        4200/TCP   4h8m\n",
      "kubernetes   ClusterIP   172.20.0.1       <none>        443/TCP    4h55m\n"
     ]
    }
   ],
   "source": [
    "# NAME                     TYPE           CLUSTER-IP     EXTERNAL-IP                                                                  PORT(S)          AGE\n",
    "# kubernetes               ClusterIP      172.20.0.1     <none>                                                                       443/TCP          37m\n",
    "# kubyterlab-llm-service   LoadBalancer   172.20.129.8   a4740e3e56bfe40ac81121bd46071903-1377611187.ca-central-1.elb.amazonaws.com   8888:31434/TCP   13s\n",
    "!kubectl get service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fde9f7f4-b4cc-4834-9ccc-e69f70ffbbad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME       READY   UP-TO-DATE   AVAILABLE   AGE\n",
      "fastapi    1/1     1            1           4h8m\n",
      "frontend   1/1     1            1           4h8m\n"
     ]
    }
   ],
   "source": [
    "# NAME               READY   UP-TO-DATE   AVAILABLE   AGE\n",
    "# kubyterlab-llm-pod   1/1     1            1           31m\n",
    "!kubectl get deployments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5663728c-8285-4265-bb0f-eef82a989578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME      CLASS   HOSTS   ADDRESS                                                                     PORTS   AGE\n",
      "ingress   nginx   *       aa703202c6c1849ce9814d2c1fbe6a9c-697758537.ca-central-1.elb.amazonaws.com   80      3h24m\n"
     ]
    }
   ],
   "source": [
    "!kubectl get ingress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "54de0206-648b-48ef-a2f0-ccca684a3293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME              STATUS   AGE\n",
      "default           Active   4h55m\n",
      "ingress-nginx     Active   4h9m\n",
      "kube-node-lease   Active   4h55m\n",
      "kube-public       Active   4h55m\n",
      "kube-system       Active   4h55m\n"
     ]
    }
   ],
   "source": [
    "!kubectl get namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5a81be83-efd5-4afe-8e91-45f1bfa25703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME      CLASS   HOSTS   ADDRESS                                                                     PORTS   AGE     LABELS\n",
      "ingress   nginx   *       aa703202c6c1849ce9814d2c1fbe6a9c-697758537.ca-central-1.elb.amazonaws.com   80      3h24m   app.kubernetes.io/managed-by=Helm\n"
     ]
    }
   ],
   "source": [
    "!kubectl get ingress --show-labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3f118af3-dd86-43fd-b6b3-ead78a96dbb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                 TYPE           CLUSTER-IP       EXTERNAL-IP                                                                 PORT(S)                      AGE\n",
      "ingress-nginx-controller             LoadBalancer   172.20.108.128   aa703202c6c1849ce9814d2c1fbe6a9c-697758537.ca-central-1.elb.amazonaws.com   80:32121/TCP,443:30845/TCP   4h9m\n",
      "ingress-nginx-controller-admission   ClusterIP      172.20.2.80      <none>                                                                      443/TCP                      4h9m\n"
     ]
    }
   ],
   "source": [
    "!kubectl get svc -n ingress-nginx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c1f58371-742b-41ae-a44e-f9d9dcb6f977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.0.1.161 - - [23/Feb/2025:00:01:16 +0000] \"{\\x22params\\x22:[],\\x22id\\x22:\\x22echo\\x22,\\x22method\\x22:\\x22echo\\x22}\" 400 150 \"-\" \"-\" 0 0.014 [] [] - - - - d417d63eb079e0181b7c9bf24f4ae483\n",
      "I0223 00:01:41.207939       7 main.go:107] \"successfully validated configuration, accepting\" ingress=\"default/ingress\"\n",
      "I0223 00:01:41.216036       7 controller.go:196] \"Configuration changes detected, backend reload required\"\n",
      "I0223 00:01:41.216226       7 event.go:377] Event(v1.ObjectReference{Kind:\"Ingress\", Namespace:\"default\", Name:\"ingress\", UID:\"32b88182-7182-4606-90af-267f226b32e2\", APIVersion:\"networking.k8s.io/v1\", ResourceVersion:\"64151\", FieldPath:\"\"}): type: 'Normal' reason: 'Sync' Scheduled for sync\n",
      "I0223 00:01:41.255519       7 controller.go:216] \"Backend successfully reloaded\"\n",
      "I0223 00:01:41.255960       7 event.go:377] Event(v1.ObjectReference{Kind:\"Pod\", Namespace:\"ingress-nginx\", Name:\"ingress-nginx-controller-7657f6db5f-s8bmd\", UID:\"bf3c1154-788c-4528-8a34-b580223c8e50\", APIVersion:\"v1\", ResourceVersion:\"10377\", FieldPath:\"\"}): type: 'Normal' reason: 'RELOAD' NGINX reload triggered due to a change in configuration\n",
      "10.0.1.161 - - [23/Feb/2025:00:01:45 +0000] \"POST /respond HTTP/1.1\" 405 31 \"-\" \"python-requests/2.32.3\" 327 0.002 [default-fastapi-8000] [] 10.0.1.49:8000 31 0.002 405 b781d6650e79ad2c45c8e002390de507\n",
      "10.0.1.137 - - [23/Feb/2025:00:01:47 +0000] \"{\\x22params\\x22:[],\\x22id\\x22:\\x22echo\\x22,\\x22method\\x22:\\x22echo\\x22}\" 400 150 \"-\" \"-\" 0 0.014 [] [] - - - - d696c9eaafb5269ea5d94b90b1286428\n",
      "10.0.1.137 - - [23/Feb/2025:00:01:49 +0000] \"POST /respond HTTP/1.1\" 405 31 \"-\" \"python-requests/2.32.3\" 327 0.002 [default-fastapi-8000] [] 10.0.1.49:8000 31 0.002 405 73d65c0d84b3b377ceb873c532fd085e\n",
      "10.0.1.15 - - [23/Feb/2025:00:02:18 +0000] \"{\\x22params\\x22:[],\\x22id\\x22:\\x22echo\\x22,\\x22method\\x22:\\x22echo\\x22}\" 400 150 \"-\" \"-\" 0 0.012 [] [] - - - - af4ad9778eedcea48748a5596862dc02\n"
     ]
    }
   ],
   "source": [
    "!kubectl logs -l app.kubernetes.io/name=ingress-nginx -n ingress-nginx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "191dac73-b376-4f60-9792-a12df4721e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME              \tNAMESPACE    \tREVISION\tUPDATED                                \tSTATUS  \tCHART                    \tAPP VERSION\n",
      "aws-ebs-csi-driver\tkube-system  \t1       \t2025-02-22 19:09:49.193714285 +0000 UTC\tdeployed\taws-ebs-csi-driver-2.40.0\t1.40.0     \n",
      "fastapi           \tdefault      \t2       \t2025-02-22 23:59:59.878444676 +0000 UTC\tdeployed\tfastapi-0.1.0            \t1.0        \n",
      "frontend          \tdefault      \t2       \t2025-02-23 00:00:01.859164008 +0000 UTC\tdeployed\tfrontend-0.1.0           \t1.0        \n",
      "ingress           \tdefault      \t10      \t2025-02-23 00:01:41.147457177 +0000 UTC\tdeployed\tingress-0.1.0            \t1.0.0      \n",
      "ingress-nginx     \tingress-nginx\t1       \t2025-02-22 19:52:40.900849212 +0000 UTC\tdeployed\tingress-nginx-4.12.0     \t1.12.0     \n",
      "storage           \tdefault      \t1       \t2025-02-22 19:52:35.81714641 +0000 UTC \tdeployed\tstorage-0.1.0            \t           \n"
     ]
    }
   ],
   "source": [
    "!helm list -A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8b31af91-2cff-4bd4-811d-a405ccbe7939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER-SUPPLIED VALUES:\n",
      "null\n"
     ]
    }
   ],
   "source": [
    "!helm get values ingress-nginx -n ingress-nginx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2621d2b0-aa9a-43e1-a7ef-1b07c1be73c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME    CONTROLLER             PARAMETERS   AGE\n",
      "nginx   k8s.io/ingress-nginx   <none>       4h9m\n"
     ]
    }
   ],
   "source": [
    "!kubectl get ingressclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b313ba9a-6ab6-4b07-a524-7804bcfffc19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME      CLASS   HOSTS   ADDRESS                                                                     PORTS   AGE\n",
      "ingress   nginx   *       aa703202c6c1849ce9814d2c1fbe6a9c-697758537.ca-central-1.elb.amazonaws.com   80      3h25m\n"
     ]
    }
   ],
   "source": [
    "!kubectl get ingress "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0d727659-1be2-45c8-97db-48133f64865f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:             ingress\n",
      "Labels:           app.kubernetes.io/managed-by=Helm\n",
      "Namespace:        default\n",
      "Address:          aa703202c6c1849ce9814d2c1fbe6a9c-697758537.ca-central-1.elb.amazonaws.com\n",
      "Ingress Class:    nginx\n",
      "Default backend:  <default>\n",
      "Rules:\n",
      "  Host        Path  Backends\n",
      "  ----        ----  --------\n",
      "  *           \n",
      "              /respond   fastapi:8000 (10.0.1.49:8000)\n",
      "              /          frontend:4200 (10.0.1.208:4200)\n",
      "Annotations:  meta.helm.sh/release-name: ingress\n",
      "              meta.helm.sh/release-namespace: default\n",
      "              nginx.ingress.kubernetes.io/cors-allow-methods: POST,GET,PUT,DELETE\n",
      "              nginx.ingress.kubernetes.io/enable-methods: POST,GET,PUT,DELETE\n",
      "              nginx.ingress.kubernetes.io/proxy-read-timeout: 3600\n",
      "              nginx.ingress.kubernetes.io/proxy-send-timeout: 3600\n",
      "              nginx.ingress.kubernetes.io/rewrite-target: /\n",
      "Events:\n",
      "  Type    Reason  Age                  From                      Message\n",
      "  ----    ------  ----                 ----                      -------\n",
      "  Normal  Sync    57s (x4 over 3h18m)  nginx-ingress-controller  Scheduled for sync\n"
     ]
    }
   ],
   "source": [
    "!kubectl describe ingress ingress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca977a7-493f-428d-a15f-d142beb64492",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f63db4e1-29e3-479e-8db9-13b6b2a1c8fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ingress-nginx-controller-7657f6db5f-s8bmd'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = !kubectl get pods -n ingress-nginx --no-headers\n",
    "fields = re.split(r'\\s+', output[0])\n",
    "ingress_controller_pod = fields[0]\n",
    "ingress_controller_pod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "79853d58-a743-441b-971c-218dea063d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Host frontend.default.svc.cluster.local:4200 was resolved.\n",
      "* IPv6: (none)\n",
      "* IPv4: 172.20.163.190\n",
      "*   Trying 172.20.163.190:4200...\n",
      "* connect to 172.20.163.190 port 4200 from 10.0.1.58 port 41446 failed: Connection refused\n",
      "* Failed to connect to frontend.default.svc.cluster.local port 4200 after 4 ms: Could not connect to server\n",
      "* closing connection #0\n",
      "curl: (7) Failed to connect to frontend.default.svc.cluster.local port 4200 after 4 ms: Could not connect to server\n",
      "command terminated with exit code 7\n"
     ]
    }
   ],
   "source": [
    "!kubectl exec -it $ingress_controller_pod -n ingress-nginx -- curl -v frontend.default.svc.cluster.local:4200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "048615e9-b5c6-4cc8-abfc-0446ed55c1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Host fastapi.default.svc.cluster.local:8000 was resolved.\n",
      "* IPv6: (none)\n",
      "* IPv4: 172.20.157.106\n",
      "*   Trying 172.20.157.106:8000...\n",
      "* Connected to fastapi.default.svc.cluster.local (172.20.157.106) port 8000\n",
      "* using HTTP/1.x\n",
      "> GET / HTTP/1.1\n",
      "> Host: fastapi.default.svc.cluster.local:8000\n",
      "> User-Agent: curl/8.11.1\n",
      "> Accept: */*\n",
      "> \n",
      "* Request completely sent off\n",
      "< HTTP/1.1 200 OK\n",
      "< date: Sun, 23 Feb 2025 00:02:47 GMT\n",
      "< server: uvicorn\n",
      "< content-length: 27\n",
      "< content-type: application/json\n",
      "< \n",
      "* Connection #0 to host fastapi.default.svc.cluster.local left intact\n",
      "{\"message\":\"Hello, world!\"}"
     ]
    }
   ],
   "source": [
    "!kubectl exec -it $ingress_controller_pod -n ingress-nginx -- curl -v fastapi.default.svc.cluster.local:8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "28175614-e54a-4ec6-b84a-0280737ba5dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Host fastapi.default.svc.cluster.local:8000 was resolved.\n",
      "* IPv6: (none)\n",
      "* IPv4: 172.20.157.106\n",
      "*   Trying 172.20.157.106:8000...\n",
      "* Connected to fastapi.default.svc.cluster.local (172.20.157.106) port 8000\n",
      "* using HTTP/1.x\n",
      "> POST /respond HTTP/1.1\n",
      "> Host: fastapi.default.svc.cluster.local:8000\n",
      "> User-Agent: curl/8.11.1\n",
      "> Accept: */*\n",
      "> \n",
      "* Request completely sent off\n",
      "< HTTP/1.1 422 Unprocessable Entity\n",
      "< date: Sun, 23 Feb 2025 00:02:53 GMT\n",
      "< server: uvicorn\n",
      "< content-length: 82\n",
      "< content-type: application/json\n",
      "< \n",
      "{\"detail\":[{\"type\":\"missing\",\"loc\":[\"body\"],\"msg\":\"Field required\",\"input\":null}]}* Connection #0 to host fastapi.default.svc.cluster.local left intact\n"
     ]
    }
   ],
   "source": [
    "!kubectl exec -it $ingress_controller_pod -n ingress-nginx -- curl -v -X POST fastapi.default.svc.cluster.local:8000/respond --no-buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "8c6d2510-2c09-43d3-bb80-db1227c96a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error: pod, type/name or --filename must be specified\n"
     ]
    }
   ],
   "source": [
    "!kubectl exec -it $ingress_controller_pod -n ingress-nginx -- curl -v -X POST fastapi.default.svc.cluster.local:8000/respond --no-buffer -H \"Content-Type: application/json\" -d '{\"content\": \"What are the languages in Eberron?\"}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "0d63949c-c95c-40c5-8629-04ffce5c061c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*   Trying 10.0.1.49:8000...\n",
      "* Connected to 10.0.1.49 (10.0.1.49) port 8000\n",
      "* using HTTP/1.x\n",
      "> GET / HTTP/1.1\n",
      "> Host: 10.0.1.49:8000\n",
      "> User-Agent: curl/8.11.1\n",
      "> Accept: */*\n",
      "> \n",
      "* Request completely sent off\n",
      "< HTTP/1.1 200 OK\n",
      "< date: Sat, 22 Feb 2025 21:27:40 GMT\n",
      "< server: uvicorn\n",
      "< content-length: 27\n",
      "< content-type: application/json\n",
      "< \n",
      "* Connection #0 to host 10.0.1.49 left intact\n",
      "{\"message\":\"Hello, world!\"}"
     ]
    }
   ],
   "source": [
    "!kubectl exec -it $ingress_controller_pod -n ingress-nginx -- curl -v 10.0.1.49:8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "72ff6e74-4202-45f1-af0a-9d5b6299a9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kubectl exec $ingress_controller_pod -n ingress-nginx -- cat /etc/nginx/nginx.conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "8d9430ed-d98f-4214-b59b-99edfdd606fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Could not resolve proxy: POST\n",
      "* shutting down connection #0\n",
      "curl: (5) Could not resolve proxy: POST\n",
      "command terminated with exit code 5\n"
     ]
    }
   ],
   "source": [
    "!kubectl exec -it $ingress_controller_pod -n ingress-nginx -- curl -v -x POST 10.0.1.49:8000/respond --no-buffer -H \"Content-Type: application/json\" -d '{\"content\": \"What are the languages in Eberron?\"}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "092efbc0-177c-4386-8e51-433562dc3ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server:\t\t172.20.0.10\n",
      "Address:\t172.20.0.10:53\n",
      "\n",
      "\n",
      "Name:\tfastapi.default.svc.cluster.local\n",
      "Address: 172.20.157.106\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!kubectl exec -it $ingress_controller_pod -n ingress-nginx -- nslookup fastapi.default.svc.cluster.local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "ac7d6a8b-d3a4-4659-9a44-10d21c7c43ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server:\t\t172.20.0.10\n",
      "Address:\t172.20.0.10:53\n",
      "\n",
      "\n",
      "Name:\tfastapi.default.svc.cluster.local\n",
      "Address: 172.20.157.106\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!kubectl exec -it $ingress_controller_pod -n ingress-nginx -- nslookup fastapi.default.svc.cluster.local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "176374ad-2ad0-4ca4-a19d-41e3c688265c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME      TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE\n",
      "fastapi   ClusterIP   172.20.157.106   <none>        8000/TCP   82m\n"
     ]
    }
   ],
   "source": [
    "!kubectl get svc fastapi -n default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "4f2502d1-940c-4257-b1a1-5ef1b1519931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*   Trying 172.20.157.106:8000...\n",
      "* Connected to 172.20.157.106 (172.20.157.106) port 8000\n",
      "* using HTTP/1.x\n",
      "> GET / HTTP/1.1\n",
      "> Host: 172.20.157.106:8000\n",
      "> User-Agent: curl/8.11.1\n",
      "> Accept: */*\n",
      "> \n",
      "* Request completely sent off\n",
      "< HTTP/1.1 200 OK\n",
      "< date: Sat, 22 Feb 2025 21:16:29 GMT\n",
      "< server: uvicorn\n",
      "< content-length: 27\n",
      "< content-type: application/json\n",
      "< \n",
      "* Connection #0 to host 172.20.157.106 left intact\n",
      "{\"message\":\"Hello, world!\"}"
     ]
    }
   ],
   "source": [
    "!kubectl exec -it $ingress_controller_pod -n ingress-nginx -- curl -v 172.20.157.106:8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "41168437-7564-4b3f-9a0d-e1739ebd0483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*   Trying 172.20.157.106:8000...\n",
      "* Connected to fastapi (172.20.157.106) port 8000 (#0)\n",
      "> GET / HTTP/1.1\n",
      "> Host: fastapi:8000\n",
      "> User-Agent: curl/7.81.0\n",
      "> Accept: */*\n",
      "> \n",
      "* Mark bundle as not supporting multiuse\n",
      "< HTTP/1.1 200 OK\n",
      "< date: Sat, 22 Feb 2025 21:25:46 GMT\n",
      "< server: uvicorn\n",
      "< content-length: 27\n",
      "< content-type: application/json\n",
      "< \n",
      "* Connection #0 to host fastapi left intact\n",
      "{\"message\":\"Hello, world!\"}"
     ]
    }
   ],
   "source": [
    "!kubectl exec -it $pod_name -n default -- curl -v fastapi:8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "665a516c-3332-4c8e-8f13-6c3b98f6fa69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*   Trying 172.20.157.106:8000...\n",
      "* Connected to fastapi (172.20.157.106) port 8000 (#0)\n",
      "> POST /respond HTTP/1.1\n",
      "> Host: fastapi:8000\n",
      "> User-Agent: curl/7.81.0\n",
      "> Accept: */*\n",
      "> \n",
      "* Mark bundle as not supporting multiuse\n",
      "< HTTP/1.1 422 Unprocessable Entity\n",
      "< date: Sat, 22 Feb 2025 21:26:46 GMT\n",
      "< server: uvicorn\n",
      "< content-length: 82\n",
      "< content-type: application/json\n",
      "< \n",
      "{\"detail\":[{\"type\":\"missing\",\"loc\":[\"body\"],\"msg\":\"Field required\",\"input\":null}]}* Connection #0 to host fastapi left intact\n"
     ]
    }
   ],
   "source": [
    "!kubectl exec -it $pod_name -n default -- curl -v -X POST fastapi:8000/respond --no-buffer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837840e5-85b5-4b9a-a937-9a319a1b1b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl exec -it $pod_name -n default -- curl -v -X POST fastapi:8000/respond --no-buffer -H \"Content-Type: application/json\" -d '{\"content\": \"What are the languages in Eberron?\"}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "b476e154-946e-45f4-bd87-ca22a910e610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Host fastapi.default.svc.cluster.local:8000 was resolved.\n",
      "* IPv6: (none)\n",
      "* IPv4: 172.20.157.106\n",
      "*   Trying 172.20.157.106:8000...\n",
      "* Connected to fastapi.default.svc.cluster.local (172.20.157.106) port 8000\n",
      "* using HTTP/1.x\n",
      "> GET / HTTP/1.1\n",
      "> Host: fastapi.default.svc.cluster.local:8000\n",
      "> User-Agent: curl/8.11.1\n",
      "> Accept: */*\n",
      "> \n",
      "* Request completely sent off\n",
      "< HTTP/1.1 200 OK\n",
      "< date: Sat, 22 Feb 2025 21:19:01 GMT\n",
      "< server: uvicorn\n",
      "< content-length: 27\n",
      "< content-type: application/json\n",
      "< \n",
      "* Connection #0 to host fastapi.default.svc.cluster.local left intact\n",
      "{\"message\":\"Hello, world!\"}"
     ]
    }
   ],
   "source": [
    "!kubectl exec -it $ingress_controller_pod -n ingress-nginx -- curl -v fastapi.default.svc.cluster.local:8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "ee4164d1-4e5e-45c3-9072-917d7d4f222c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME      ENDPOINTS        AGE\n",
      "fastapi   10.0.1.49:8000   77m\n"
     ]
    }
   ],
   "source": [
    "!kubectl get endpoints fastapi -n default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e319c578-74f9-4274-af11-1c0bc114fe43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kubectl describe deployment kubyterlab-llm-pod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a9b485d7-ab30-4ad8-8c23-0c83a5f50f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kubectl describe pod frontend-75669c8cd8-5kkmk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "3d2ac1bd-3d25-4fa8-b2dd-fb7fa10e42d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                        READY   STATUS    RESTARTS   AGE\n",
      "fastapi-677965765c-z5kc6    1/1     Running   0          77m\n",
      "frontend-74b7bcf65b-lknld   1/1     Running   0          33m\n"
     ]
    }
   ],
   "source": [
    "!kubectl get pods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0a2da721-c219-436c-8ba5-11d905c507d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME      STATUS   VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   VOLUMEATTRIBUTESCLASS   AGE\n",
      "pvc-llm   Bound    pv-llm   500Gi      RWO            manual         <unset>                 98m\n"
     ]
    }
   ],
   "source": [
    "!kubectl get pvc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "23b517a1-9d50-47a0-bcc8-e9a0f787c216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME     CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM             STORAGECLASS   VOLUMEATTRIBUTESCLASS   REASON   AGE\n",
      "pv-llm   500Gi      RWO            Retain           Bound    default/pvc-llm   manual         <unset>                          98m\n"
     ]
    }
   ],
   "source": [
    "!kubectl get pv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "6a28c8a8-575e-4182-b448-4c5498d74666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========\n",
      "== CUDA ==\n",
      "==========\n",
      "\n",
      "CUDA Version 12.6.1\n",
      "\n",
      "Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n",
      "\n",
      "This container image and its contents are governed by the NVIDIA Deep Learning Container License.\n",
      "By pulling and using the container, you accept the terms and conditions of this license:\n",
      "https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license\n",
      "\n",
      "A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.\n",
      "\n",
      "2025-02-22 19:57:40.261589: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-22 19:57:40.277019: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1740254260.296566       1 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1740254260.302588       1 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-22 19:57:40.322043: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/app/models.py:41: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=embedding_model_path,\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [33:47<00:00, 675.85s/it]\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "INFO:     Started server process [1]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n",
      "INFO:     10.0.1.58:45566 - \"POST / HTTP/1.1\" 405 Method Not Allowed\n",
      "INFO:     10.0.1.58:40412 - \"POST / HTTP/1.1\" 405 Method Not Allowed\n",
      "INFO:     10.0.1.58:40840 - \"POST / HTTP/1.1\" 405 Method Not Allowed\n",
      "INFO:     10.0.1.58:40840 - \"POST / HTTP/1.1\" 405 Method Not Allowed\n",
      "INFO:     10.0.1.58:35784 - \"POST / HTTP/1.1\" 405 Method Not Allowed\n",
      "INFO:     10.0.1.161:47489 - \"GET / HTTP/1.1\" 200 OK\n",
      "INFO:     10.0.1.161:53110 - \"GET / HTTP/1.1\" 200 OK\n",
      "INFO:     10.0.1.161:8409 - \"GET / HTTP/1.1\" 200 OK\n",
      "INFO:     10.0.1.58:35266 - \"GET / HTTP/1.1\" 200 OK\n",
      "INFO:     10.0.1.58:53128 - \"POST / HTTP/1.1\" 405 Method Not Allowed\n",
      "INFO:     10.0.1.58:51222 - \"GET / HTTP/1.1\" 200 OK\n",
      "INFO:     10.0.1.161:7359 - \"GET / HTTP/1.1\" 200 OK\n",
      "INFO:     10.0.1.58:51596 - \"GET / HTTP/1.1\" 200 OK\n",
      "INFO:     10.0.1.161:63156 - \"GET / HTTP/1.1\" 200 OK\n",
      "INFO:     10.0.1.58:55940 - \"GET / HTTP/1.1\" 200 OK\n",
      "INFO:     10.0.1.58:40008 - \"GET / HTTP/1.1\" 200 OK\n",
      "INFO:     10.0.1.58:46336 - \"GET / HTTP/1.1\" 200 OK\n",
      "INFO:     10.0.1.58:48050 - \"GET / HTTP/1.1\" 200 OK\n",
      "INFO:     10.0.1.161:11262 - \"GET / HTTP/1.1\" 200 OK\n",
      "INFO:     10.0.1.161:30664 - \"POST /respond HTTP/1.1\" 422 Unprocessable Entity\n",
      "INFO:     10.0.1.58:60784 - \"GET / HTTP/1.1\" 200 OK\n",
      "INFO:     10.0.1.58:47764 - \"GET / HTTP/1.1\" 200 OK\n",
      "INFO:     10.0.1.58:48042 - \"POST /respond HTTP/1.1\" 422 Unprocessable Entity\n",
      "INFO:     10.0.1.58:37844 - \"POST /respond HTTP/1.1\" 422 Unprocessable Entity\n",
      "INFO:     10.0.1.58:47738 - \"POST / HTTP/1.1\" 405 Method Not Allowed\n",
      "INFO:     10.0.1.58:45666 - \"POST / HTTP/1.1\" 405 Method Not Allowed\n",
      "INFO:     10.0.1.58:54812 - \"POST / HTTP/1.1\" 405 Method Not Allowed\n",
      "INFO:     10.0.1.58:33394 - \"POST / HTTP/1.1\" 405 Method Not Allowed\n",
      "INFO:     10.0.1.58:42086 - \"POST / HTTP/1.1\" 405 Method Not Allowed\n",
      "INFO:     10.0.1.58:42098 - \"POST / HTTP/1.1\" 405 Method Not Allowed\n",
      "INFO:     10.0.1.58:47320 - \"GET / HTTP/1.1\" 200 OK\n",
      "INFO:     10.0.1.58:35926 - \"POST /respond HTTP/1.1\" 422 Unprocessable Entity\n",
      "INFO:     10.0.1.58:55868 - \"GET / HTTP/1.1\" 200 OK\n",
      "INFO:     10.0.1.58:55868 - \"GET /favicon.ico HTTP/1.1\" 404 Not Found\n",
      "INFO:     10.0.1.58:34700 - \"POST / HTTP/1.1\" 405 Method Not Allowed\n",
      "INFO:     10.0.1.58:49156 - \"POST / HTTP/1.1\" 405 Method Not Allowed\n",
      "INFO:     10.0.1.58:49164 - \"POST / HTTP/1.1\" 405 Method Not Allowed\n",
      "INFO:     10.0.1.58:34556 - \"POST / HTTP/1.1\" 405 Method Not Allowed\n",
      "INFO:     10.0.1.58:34556 - \"POST / HTTP/1.1\" 405 Method Not Allowed\n",
      "INFO:     10.0.1.58:52612 - \"POST / HTTP/1.1\" 405 Method Not Allowed\n"
     ]
    }
   ],
   "source": [
    "!kubectl logs $pod_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6c6f80b2-a78a-45e6-bd1e-0fc3bd307b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:             fastapi-698c4dc749-l5qsk\n",
      "Namespace:        default\n",
      "Priority:         0\n",
      "Service Account:  default\n",
      "Node:             ip-10-0-1-228.ca-central-1.compute.internal/10.0.1.228\n",
      "Start Time:       Sun, 09 Feb 2025 15:09:55 +0000\n",
      "Labels:           app=fastapi\n",
      "                  pod-template-hash=698c4dc749\n",
      "Annotations:      <none>\n",
      "Status:           Running\n",
      "IP:               10.0.1.238\n",
      "IPs:\n",
      "  IP:           10.0.1.238\n",
      "Controlled By:  ReplicaSet/fastapi-698c4dc749\n",
      "Containers:\n",
      "  fastapi:\n",
      "    Container ID:   containerd://a15615861206a39f38823254d93970ca29766b92277c793c2856fb57bc0cfd11\n",
      "    Image:          275678099358.dkr.ecr.ca-central-1.amazonaws.com/multi-agent/eberron-agent-server:25.02\n",
      "    Image ID:       275678099358.dkr.ecr.ca-central-1.amazonaws.com/multi-agent/eberron-agent-server@sha256:0a7ec392240e2f7a4ba9ce05b279df68209e085074bbe4fb8f465e047156b2ff\n",
      "    Port:           80/TCP\n",
      "    Host Port:      0/TCP\n",
      "    State:          Running\n",
      "      Started:      Sun, 09 Feb 2025 15:14:49 +0000\n",
      "    Ready:          True\n",
      "    Restart Count:  0\n",
      "    Environment:\n",
      "      ARTEFACT_VERSION:        04\n",
      "      ARTEFACT_ROOT_FOLDER:    /jupyterlab/artefacts\n",
      "      HF_HOME:                 /jupyterlab/models/hf\n",
      "      MODEL_NAME:              Mistral-7B-Instruct-v0.3\n",
      "      MODEL_ORG:               mistralai\n",
      "      COMMIT_HASH:             e0bc86c23ce5aae1db576c8cca6f06f1f73af2db\n",
      "      TOKENIZERS_PARALLELISM:  true\n",
      "      TRANSFORMERS_OFFLINE:    1\n",
      "      HF_DATASETS_OFFLINE:     1\n",
      "    Mounts:\n",
      "      /jupyterlab from pvc-llm (rw)\n",
      "      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-klc6j (ro)\n",
      "Conditions:\n",
      "  Type                        Status\n",
      "  PodReadyToStartContainers   True \n",
      "  Initialized                 True \n",
      "  Ready                       True \n",
      "  ContainersReady             True \n",
      "  PodScheduled                True \n",
      "Volumes:\n",
      "  pvc-llm:\n",
      "    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)\n",
      "    ClaimName:  pvc-llm\n",
      "    ReadOnly:   false\n",
      "  kube-api-access-klc6j:\n",
      "    Type:                    Projected (a volume that contains injected data from multiple sources)\n",
      "    TokenExpirationSeconds:  3607\n",
      "    ConfigMapName:           kube-root-ca.crt\n",
      "    ConfigMapOptional:       <nil>\n",
      "    DownwardAPI:             true\n",
      "QoS Class:                   BestEffort\n",
      "Node-Selectors:              <none>\n",
      "Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n",
      "                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\n",
      "                             nvidia.com/gpu:NoSchedule op=Exists\n",
      "Events:\n",
      "  Type    Reason                  Age   From                     Message\n",
      "  ----    ------                  ----  ----                     -------\n",
      "  Normal  Scheduled               21m   default-scheduler        Successfully assigned default/fastapi-698c4dc749-l5qsk to ip-10-0-1-228.ca-central-1.compute.internal\n",
      "  Normal  SuccessfulAttachVolume  21m   attachdetach-controller  AttachVolume.Attach succeeded for volume \"pv-llm\"\n",
      "  Normal  Pulling                 21m   kubelet                  Pulling image \"275678099358.dkr.ecr.ca-central-1.amazonaws.com/multi-agent/eberron-agent-server:25.02\"\n",
      "  Normal  Pulled                  16m   kubelet                  Successfully pulled image \"275678099358.dkr.ecr.ca-central-1.amazonaws.com/multi-agent/eberron-agent-server:25.02\" in 4m28.297s (4m28.297s including waiting). Image size: 14071664304 bytes.\n",
      "  Normal  Created                 16m   kubelet                  Created container fastapi\n",
      "  Normal  Started                 16m   kubelet                  Started container fastapi\n"
     ]
    }
   ],
   "source": [
    "!kubectl describe pod $pod_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad2f7fa-c856-4866-96bb-54eefb245b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "attachdetach-controller  AttachVolume.Attach failed for volume \"pv-llm\" : rpc error: code = Internal desc = Could not attach volume \"volume_id\" to node \"i-055aa2853ecdeac3b\": could not attach volume \"volume_id\" to node \"i-055aa2853ecdeac3b\": operation error EC2: AttachVolume, https response error StatusCode: 400, RequestID: a7250de3-c285-4963-84eb-5c46b28c2b96, api error InvalidParameterValue: The volume ID 'volume_id' is malformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6fbb74-4125-4ad3-96b4-f7117b82e1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl exec $pod_name bash -- nvidia-smi --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e4308b32-7f70-4861-81f6-7bc0702cbc87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulted container \"kubyterlab-llm\" out of: kubyterlab-llm, init-container (init)\n",
      "Mon Jan 13 22:15:38 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       On  |   00000000:00:1E.0 Off |                    0 |\n",
      "| N/A   25C    P8             11W /   70W |       1MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!kubectl exec $pod_name bash -- nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5a68c2d5-dc22-4529-9e8f-a7935f360340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulted container \"kubyterlab-llm\" out of: kubyterlab-llm, init-container (init)\n",
      "GPU 0: Tesla T4 (UUID: GPU-fe1acad6-41f9-c441-44ed-d4ead6db6dc2)\n"
     ]
    }
   ],
   "source": [
    "!kubectl exec $pod_name bash -- nvidia-smi -L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8d229310-97eb-4788-bf0e-22203dffe078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulted container \"kubyterlab-llm\" out of: kubyterlab-llm, init-container (init)\n",
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2024 NVIDIA Corporation\n",
      "Built on Wed_Aug_14_10:10:22_PDT_2024\n",
      "Cuda compilation tools, release 12.6, V12.6.68\n",
      "Build cuda_12.6.r12.6/compiler.34714021_0\n"
     ]
    }
   ],
   "source": [
    "!kubectl exec $pod_name bash -- nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722a4aa7-a0c3-462e-9e37-60230261fcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl describe pod $pod_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "116fd849-a569-472e-93d6-23624e93c7e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                   READY   STATUS    RESTARTS   AGE\n",
      "aws-node-cvzp2                         2/2     Running   0          4m59s\n",
      "aws-node-kmnjj                         2/2     Running   0          4m57s\n",
      "aws-node-zlmn8                         2/2     Running   0          6m35s\n",
      "coredns-749d5dbdd9-cdd8v               1/1     Running   0          8m29s\n",
      "coredns-749d5dbdd9-xtrc8               1/1     Running   0          8m29s\n",
      "ebs-csi-controller-59b6797bf-fhcph     5/5     Running   0          4m6s\n",
      "ebs-csi-controller-59b6797bf-w9mns     5/5     Running   0          4m6s\n",
      "ebs-csi-node-mp9sr                     3/3     Running   0          4m6s\n",
      "ebs-csi-node-mqt55                     3/3     Running   0          4m6s\n",
      "ebs-csi-node-q8dfl                     3/3     Running   0          4m6s\n",
      "kube-proxy-ctjcd                       1/1     Running   0          6m35s\n",
      "kube-proxy-nv8tx                       1/1     Running   0          4m57s\n",
      "kube-proxy-qmwmh                       1/1     Running   0          4m59s\n",
      "nvidia-device-plugin-daemonset-4b76p   1/1     Running   0          4m4s\n",
      "nvidia-device-plugin-daemonset-ltcdg   1/1     Running   0          4m4s\n",
      "nvidia-device-plugin-daemonset-wv2rd   1/1     Running   0          4m4s\n"
     ]
    }
   ],
   "source": [
    "!kubectl get pod -n kube-system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "90495808-55bb-4d5b-84e8-c00ca25250b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:          pvc-llm\n",
      "Namespace:     default\n",
      "StorageClass:  manual\n",
      "Status:        Bound\n",
      "Volume:        pv-llm\n",
      "Labels:        <none>\n",
      "Annotations:   pv.kubernetes.io/bind-completed: yes\n",
      "               pv.kubernetes.io/bound-by-controller: yes\n",
      "Finalizers:    [kubernetes.io/pvc-protection]\n",
      "Capacity:      10Gi\n",
      "Access Modes:  RWO\n",
      "VolumeMode:    Filesystem\n",
      "Used By:       kubyterlab-llm-pod-67f5cf95dc-s2mrn\n",
      "Events:        <none>\n"
     ]
    }
   ],
   "source": [
    "!kubectl describe pvc pvc-llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "da047557-f797-4492-b7f6-1cdcb16a3283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:            pv-llm\n",
      "Labels:          <none>\n",
      "Annotations:     pv.kubernetes.io/bound-by-controller: yes\n",
      "Finalizers:      [kubernetes.io/pv-protection]\n",
      "StorageClass:    manual\n",
      "Status:          Bound\n",
      "Claim:           default/pvc-llm\n",
      "Reclaim Policy:  Retain\n",
      "Access Modes:    RWO\n",
      "VolumeMode:      Filesystem\n",
      "Capacity:        10Gi\n",
      "Node Affinity:   <none>\n",
      "Message:         \n",
      "Source:\n",
      "    Type:       AWSElasticBlockStore (a Persistent Disk resource in AWS)\n",
      "    VolumeID:   vol-04ce08a2c05b8e1da\n",
      "    FSType:     ext4\n",
      "    Partition:  0\n",
      "    ReadOnly:   false\n",
      "Events:         <none>\n"
     ]
    }
   ],
   "source": [
    "!kubectl describe pv pv-llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "80827487-9836-450d-806d-5f56b00ab821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:                   kubyterlab-llm-pod\n",
      "Namespace:              default\n",
      "CreationTimestamp:      Mon, 02 Dec 2024 20:05:56 +0000\n",
      "Labels:                 <none>\n",
      "Annotations:            deployment.kubernetes.io/revision: 1\n",
      "Selector:               app=kubyterlab-llm\n",
      "Replicas:               1 desired | 1 updated | 1 total | 0 available | 1 unavailable\n",
      "StrategyType:           RollingUpdate\n",
      "MinReadySeconds:        0\n",
      "RollingUpdateStrategy:  25% max unavailable, 25% max surge\n",
      "Pod Template:\n",
      "  Labels:  app=kubyterlab-llm\n",
      "  Containers:\n",
      "   kubyterlab-llm:\n",
      "    Image:      kubyterlab:24.10\n",
      "    Port:       8888/TCP\n",
      "    Host Port:  0/TCP\n",
      "    Limits:\n",
      "      cpu:             1\n",
      "      memory:          8Gi\n",
      "      nvidia.com/gpu:  1\n",
      "    Requests:\n",
      "      cpu:             1\n",
      "      memory:          8Gi\n",
      "      nvidia.com/gpu:  1\n",
      "    Environment:\n",
      "      JUPYTERLAB_SETTINGS_DIR:  /jupyterlab/config\n",
      "      MISTRAL_MODEL:            /models/mistral\n",
      "    Mounts:\n",
      "      /corpus from pv-llm (rw)\n",
      "      /jupyterlab from pv-llm (rw)\n",
      "      /models from pv-llm (rw)\n",
      "  Volumes:\n",
      "   pv-llm:\n",
      "    Type:          PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)\n",
      "    ClaimName:     pvc-llm\n",
      "    ReadOnly:      false\n",
      "  Node-Selectors:  <none>\n",
      "  Tolerations:     nvidia.com/gpu:NoSchedule op=Exists\n",
      "Conditions:\n",
      "  Type           Status  Reason\n",
      "  ----           ------  ------\n",
      "  Available      False   MinimumReplicasUnavailable\n",
      "  Progressing    True    ReplicaSetUpdated\n",
      "OldReplicaSets:  <none>\n",
      "NewReplicaSet:   kubyterlab-llm-pod-67f5cf95dc (1/1 replicas created)\n",
      "Events:\n",
      "  Type    Reason             Age    From                   Message\n",
      "  ----    ------             ----   ----                   -------\n",
      "  Normal  ScalingReplicaSet  5m46s  deployment-controller  Scaled up replica set kubyterlab-llm-pod-67f5cf95dc to 1\n"
     ]
    }
   ],
   "source": [
    "!kubectl describe deployment kubyterlab-llm-pod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ce402031-be81-4578-8877-15bcd40ad8e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                          STATUS   ROLES    AGE   VERSION\n",
      "ip-10-0-1-100.ca-central-1.compute.internal   Ready    <none>   47m   v1.30.7-eks-59bf375\n",
      "ip-10-0-1-25.ca-central-1.compute.internal    Ready    <none>   47m   v1.30.7-eks-59bf375\n",
      "ip-10-0-1-50.ca-central-1.compute.internal    Ready    <none>   48m   v1.30.7-eks-59bf375\n"
     ]
    }
   ],
   "source": [
    "!kubectl get nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d0bb4314-1002-45c7-b0c2-4d706b5d533f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME               DATA   AGE\n",
      "kube-root-ca.crt   1      50m\n"
     ]
    }
   ],
   "source": [
    "!kubectl get configmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6d65a766-6f17-413c-a7f5-9f8e07fd0a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                     READY   STATUS    RESTARTS   AGE\n",
      "pod/ebs-csi-controller-59b6797bf-vfvs2   5/5     Running   0          47m\n",
      "pod/ebs-csi-controller-59b6797bf-xt7fv   5/5     Running   0          47m\n",
      "pod/ebs-csi-node-dwzz7                   3/3     Running   0          47m\n",
      "pod/ebs-csi-node-gplqp                   3/3     Running   0          47m\n",
      "pod/ebs-csi-node-l5nh6                   3/3     Running   0          47m\n",
      "\n",
      "NAME                          DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE\n",
      "daemonset.apps/ebs-csi-node   3         3         3       3            3           kubernetes.io/os=linux   47m\n",
      "\n",
      "NAME                                 READY   UP-TO-DATE   AVAILABLE   AGE\n",
      "deployment.apps/ebs-csi-controller   2/2     2            2           47m\n",
      "\n",
      "NAME                                           DESIRED   CURRENT   READY   AGE\n",
      "replicaset.apps/ebs-csi-controller-59b6797bf   2         2         2       47m\n"
     ]
    }
   ],
   "source": [
    "!kubectl get all -l app.kubernetes.io/name=aws-ebs-csi-driver -n kube-system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3bf4b6bf-4a67-48e4-ba16-8207e90b7fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"OpenIDConnectProviderList\": [\n",
      "        {\n",
      "            \"Arn\": \"arn:aws:iam::275678099358:oidc-provider/oidc.eks.ca-central-1.amazonaws.com/id/8D7619520212428BD59C46B20BF19338\"\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!aws iam list-open-id-connect-providers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "65c00bf0-9326-417c-964f-530ab62c85da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 pods, using pod/ebs-csi-controller-59b6797bf-vfvs2\n",
      "I0113 21:28:06.936951       1 main.go:153] \"Initializing metadata\"\n",
      "I0113 21:28:06.943481       1 metadata.go:48] \"Retrieved metadata from IMDS\"\n",
      "I0113 21:28:06.944585       1 driver.go:69] \"Driver Information\" Driver=\"ebs.csi.aws.com\" Version=\"v1.38.1\"\n",
      "I0113 22:01:16.236299       1 controller.go:410] \"ControllerPublishVolume: attaching\" volumeID=\"vol-0ed5d3cc8cb5a989e\" nodeID=\"i-0fd8ede93d6ac59a4\"\n",
      "I0113 22:01:17.882498       1 controller.go:419] \"ControllerPublishVolume: attached\" volumeID=\"vol-0ed5d3cc8cb5a989e\" nodeID=\"i-0fd8ede93d6ac59a4\" devicePath=\"/dev/xvdaa\"\n"
     ]
    }
   ],
   "source": [
    "!kubectl logs deployment/ebs-csi-controller -n kube-system -c ebs-plugin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8ab7f84b-398b-41fb-9c35-30f40c179f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 pods, using pod/ebs-csi-node-l5nh6\n",
      "I0113 21:27:55.828513       1 main.go:153] \"Initializing metadata\"\n",
      "I0113 21:27:55.832956       1 metadata.go:48] \"Retrieved metadata from IMDS\"\n",
      "I0113 21:27:55.833450       1 driver.go:69] \"Driver Information\" Driver=\"ebs.csi.aws.com\" Version=\"v1.38.1\"\n",
      "E0113 21:27:56.896438       1 node.go:856] \"Unexpected failure when attempting to remove node taint(s)\" err=\"isAllocatableSet: driver not found on node ip-10-0-1-50.ca-central-1.compute.internal\"\n",
      "I0113 21:27:57.410789       1 node.go:936] \"CSINode Allocatable value is set\" nodeName=\"ip-10-0-1-50.ca-central-1.compute.internal\" count=24\n",
      "I0113 22:01:18.817732       1 node.go:204] \"NodeStageVolume: invalid partition config, will ignore.\" partition=\"0\"\n",
      "I0113 22:01:18.980008       1 mount_linux.go:295] Detected OS without systemd\n"
     ]
    }
   ],
   "source": [
    "!kubectl logs daemonset/ebs-csi-node -n kube-system -c ebs-plugin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f2fd1fcf-f18f-4b19-abeb-1c207ddd58de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 pods, using pod/ebs-csi-controller-59b6797bf-vfvs2\n",
      "W0113 21:28:09.566495       1 feature_gate.go:354] Setting GA feature gate Topology=true. It will be removed in a future release.\n",
      "I0113 21:28:09.566789       1 feature_gate.go:387] feature gates: {map[Topology:true]}\n",
      "I0113 21:28:09.566840       1 csi-provisioner.go:154] Version: v5.1.0\n",
      "I0113 21:28:09.566851       1 csi-provisioner.go:177] Building kube configs for running in cluster...\n",
      "I0113 21:28:09.568886       1 common.go:143] \"Probing CSI driver for readiness\"\n",
      "I0113 21:28:09.598446       1 csi-provisioner.go:230] Detected CSI driver ebs.csi.aws.com\n",
      "I0113 21:28:09.598619       1 csi-provisioner.go:240] Supports migration from in-tree plugin: kubernetes.io/aws-ebs\n",
      "I0113 21:28:09.600672       1 common.go:143] \"Probing CSI driver for readiness\"\n",
      "I0113 21:28:09.603385       1 csi-provisioner.go:299] CSI driver supports PUBLISH_UNPUBLISH_VOLUME, watching VolumeAttachments\n",
      "I0113 21:28:09.604750       1 controller.go:744] \"Using saving PVs to API server in background\"\n",
      "I0113 21:28:09.605162       1 leaderelection.go:254] attempting to acquire leader lease kube-system/ebs-csi-aws-com...\n",
      "I0113 21:28:09.627768       1 leaderelection.go:268] successfully acquired lease kube-system/ebs-csi-aws-com\n",
      "I0113 21:28:09.629736       1 leader_election.go:184] \"became leader, starting\"\n",
      "I0113 21:28:09.629845       1 envvar.go:172] \"Feature gate default state\" feature=\"WatchListClient\" enabled=false\n",
      "I0113 21:28:09.629863       1 envvar.go:172] \"Feature gate default state\" feature=\"InformerResourceVersion\" enabled=false\n",
      "I0113 21:28:09.640053       1 reflector.go:368] Caches populated for *v1.StorageClass from k8s.io/client-go@v0.31.0/tools/cache/reflector.go:243\n",
      "I0113 21:28:09.651560       1 reflector.go:368] Caches populated for *v1.PersistentVolumeClaim from k8s.io/client-go@v0.31.0/tools/cache/reflector.go:243\n",
      "I0113 21:28:09.656122       1 reflector.go:368] Caches populated for *v1.Node from k8s.io/client-go@v0.31.0/tools/cache/reflector.go:243\n",
      "I0113 21:28:09.656502       1 reflector.go:368] Caches populated for *v1.VolumeAttachment from k8s.io/client-go@v0.31.0/tools/cache/reflector.go:243\n",
      "I0113 21:28:09.657442       1 reflector.go:368] Caches populated for *v1.CSINode from k8s.io/client-go@v0.31.0/tools/cache/reflector.go:243\n",
      "I0113 21:28:09.729875       1 controller.go:824] \"Starting provisioner controller\" component=\"ebs.csi.aws.com_ebs-csi-controller-59b6797bf-vfvs2_1c292ac0-a9d9-48ff-8621-9ba1f75e01b8\"\n",
      "I0113 21:28:09.729957       1 volume_store.go:98] \"Starting save volume queue\"\n",
      "I0113 21:28:09.734566       1 reflector.go:368] Caches populated for *v1.StorageClass from k8s.io/client-go@v0.31.0/tools/cache/reflector.go:243\n",
      "I0113 21:28:09.734567       1 reflector.go:368] Caches populated for *v1.PersistentVolume from k8s.io/client-go@v0.31.0/tools/cache/reflector.go:243\n",
      "I0113 21:28:09.831005       1 controller.go:873] \"Started provisioner controller\" component=\"ebs.csi.aws.com_ebs-csi-controller-59b6797bf-vfvs2_1c292ac0-a9d9-48ff-8621-9ba1f75e01b8\"\n"
     ]
    }
   ],
   "source": [
    "!kubectl logs deployment/ebs-csi-controller -n kube-system -c csi-provisioner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "49134a19-5818-4030-8a7a-6c9e04d6157c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 pods, using pod/ebs-csi-controller-59b6797bf-vfvs2\n",
      "I0113 21:28:12.211572       1 main.go:109] \"Version\" version=\"v4.7.0\"\n",
      "I0113 21:28:12.215865       1 common.go:143] \"Probing CSI driver for readiness\"\n",
      "I0113 21:28:12.238251       1 main.go:169] \"CSI driver name\" driver=\"ebs.csi.aws.com\"\n",
      "I0113 21:28:12.240448       1 common.go:143] \"Probing CSI driver for readiness\"\n",
      "I0113 21:28:12.242734       1 main.go:249] \"CSI driver supports ControllerPublishUnpublish, using real CSI handler\" driver=\"ebs.csi.aws.com\"\n",
      "I0113 21:28:12.243624       1 leaderelection.go:254] attempting to acquire leader lease kube-system/external-attacher-leader-ebs-csi-aws-com...\n",
      "I0113 21:28:12.265555       1 leaderelection.go:268] successfully acquired lease kube-system/external-attacher-leader-ebs-csi-aws-com\n",
      "I0113 21:28:12.266787       1 leader_election.go:184] \"became leader, starting\"\n",
      "I0113 21:28:12.266811       1 controller.go:129] \"Starting CSI attacher\"\n",
      "I0113 21:28:12.267257       1 envvar.go:172] \"Feature gate default state\" feature=\"InformerResourceVersion\" enabled=false\n",
      "I0113 21:28:12.267536       1 envvar.go:172] \"Feature gate default state\" feature=\"WatchListClient\" enabled=false\n",
      "I0113 21:28:12.271734       1 reflector.go:368] Caches populated for *v1.PersistentVolume from k8s.io/client-go@v0.31.0/tools/cache/reflector.go:243\n",
      "I0113 21:28:12.272191       1 reflector.go:368] Caches populated for *v1.CSINode from k8s.io/client-go@v0.31.0/tools/cache/reflector.go:243\n",
      "I0113 21:28:12.272546       1 reflector.go:368] Caches populated for *v1.VolumeAttachment from k8s.io/client-go@v0.31.0/tools/cache/reflector.go:243\n",
      "I0113 22:01:16.209140       1 csi_handler.go:261] \"Attaching\" VolumeAttachment=\"csi-ed0e5d922afcde1b934ffafc0f0d3a9543d0760d7dfa4ce99585f3fc2c2922b0\"\n",
      "I0113 22:01:17.882946       1 csi_handler.go:273] \"Attached\" VolumeAttachment=\"csi-ed0e5d922afcde1b934ffafc0f0d3a9543d0760d7dfa4ce99585f3fc2c2922b0\"\n"
     ]
    }
   ],
   "source": [
    "!kubectl logs deployment/ebs-csi-controller -n kube-system -c csi-attacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6fdb0f08-aba7-4d63-8e01-2b25d1b06b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"eks.amazonaws.com/role-arn\":\"\",\"meta.helm.sh/release-name\":\"aws-ebs-csi-driver\",\"meta.helm.sh/release-namespace\":\"kube-system\"}"
     ]
    }
   ],
   "source": [
    "!kubectl get sa ebs-csi-controller-sa -n kube-system -o jsonpath='{.metadata.annotations}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a85d88b-946c-40e7-8b59-0e69a4567d5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
